1:"$Sreact.fragment"
2:I[67315,["785","static/chunks/785-bf8eaad2e6817186.js","451","static/chunks/451-ff68bb1f8be23e5c.js","98","static/chunks/98-ceb6bd4957f64cef.js","108","static/chunks/108-145d964849b57510.js","177","static/chunks/app/layout-1f31a56c71888f2f.js"],"Provider"]
3:I[87555,[],""]
4:I[31295,[],""]
5:I[32176,["785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","335","static/chunks/app/%5Bslug%5D/error-ed750438367dde98.js"],"default"]
6:I[6874,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],""]
7:I[38567,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Button"]
9:I[59665,[],"OutletBoundary"]
c:I[59665,[],"ViewportBoundary"]
e:I[59665,[],"MetadataBoundary"]
10:I[26614,[],""]
:HL["/_next/static/css/9aa33c77ef4c0fa8.css","style"]
0:{"P":null,"b":"Sv6NKEQSc43phvbpdWP0z","p":"","c":["","4d387cf0-fe6a-4cb8-ba37-86f48f96315b",""],"i":false,"f":[[["",{"children":[["slug","4d387cf0-fe6a-4cb8-ba37-86f48f96315b","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/9aa33c77ef4c0fa8.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"suppressHydrationWarning":true,"lang":"ja","children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/meta/icon.png","sizes":"any"}],false]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}]]}],{"children":[["slug","4d387cf0-fe6a-4cb8-ba37-86f48f96315b","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$5","errorStyles":[],"errorScripts":[],"template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","p",null,{"children":"ページが見つかりませんでした"}],["$","$L6",null,{"href":"/","children":["$","$L7",null,{"children":"トップに戻る"}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L8","$undefined",null,["$","$L9",null,{"children":["$La","$Lb",null]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","vaC2dX9P_CrGHK-NJw43r",{"children":[["$","$Lc",null,{"children":"$Ld"}],null]}],["$","$Le",null,{"children":"$Lf"}]]}],false]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:null
11:I[90754,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Header"]
12:I[81068,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Box"]
13:I[17921,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Heading"]
14:I[90310,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Text"]
15:I[7684,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Icon"]
16:I[68264,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"ClientContainer"]
22:I[91925,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Analysis"]
23:I[91548,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Separator"]
24:I[12498,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"About"]
25:I[38639,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Stack"]
26:I[97377,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"HStack"]
27:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerRoot"]
28:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerBackdrop"]
29:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerTrigger"]
2a:I[70318,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Portal"]
2b:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerPositioner"]
2c:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerContent"]
2d:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerHeader"]
2e:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerTitle"]
2f:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerBody"]
30:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-9d7a8e0bb5feb89a.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerActionTrigger"]
17:T13f1,import concurrent.futures
import json
import logging
import re

import pandas as pd
from tqdm import tqdm

from services.category_classification import classify_args
from services.llm import request_to_chat_openai
from services.parse_json_list import parse_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(batch_inputs, prompt, model, workers)

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": arg,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    classification_categories = config["extraction"]["categories"]
    if classification_categories:
        results = classify_args(results, config, workers)

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.ERROR)


def extract_batch(batch, prompt, model, workers):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model)) for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []
        return results


def extract_by_llm(input, prompt, model):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    response = request_to_chat_openai(messages=messages, model=model)
    return response


def extract_arguments(input, prompt, model, retries=1):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response = request_to_chat_openai(messages=messages, model=model, is_json=False)
        items = parse_response(response)
        items = filter(None, items)  # omit empty strings
        return items
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
18:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
19:T142a,import json
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd

from services.llm import request_to_chat_openai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response = request_to_chat_openai(messages=messages, model=model, is_json=True)
        response_json = json.loads(response)
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
1a:T2fa5,import json
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from tqdm import tqdm

from services.llm import request_to_chat_openai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response = request_to_chat_openai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            is_json=True,
        )
        response_json = json.loads(response)
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
1b:T4c0,"""Create summaries for the clusters."""

import pandas as pd

from services.llm import request_to_chat_openai


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input = ""
    for i, _ in enumerate(ids):
        input += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input += descriptions[i] + "\n\n"

    messages = [{"role": "user", "content": prompt}, {"role": "user", "content": input}]
    response = request_to_chat_openai(messages=messages, model=model)

    with open(path, "w") as file:
        file.write(response)
1c:T22d3,"""Generate a convenient JSON output file."""

import json
from collections import defaultdict
from pathlib import Path
from typing import TypedDict

import pandas as pd

ROOT_DIR = Path(__file__).parent.parent.parent.parent
CONFIG_DIR = ROOT_DIR / "scatter" / "pipeline" / "configs"


class Argument(TypedDict):
    arg_id: str
    argument: str
    comment_id: str
    x: float
    y: float
    p: float
    cluster_ids: list[str]


class Cluster(TypedDict):
    level: int
    id: str
    label: str
    takeaway: str
    value: int
    parent: str
    density_rank_percentile: float | None


def hierarchical_aggregation(config):
    path = f"outputs/{config['output_dir']}/hierarchical_result.json"
    results = {
        "arguments": [],
        "clusters": [],
        "comments": {},
        "propertyMap": {},
        "translations": {},
        "overview": "",
        "config": config,
    }

    arguments = pd.read_csv(f"outputs/{config['output_dir']}/args.csv")
    arguments.set_index("arg-id", inplace=True)
    arg_num = len(arguments)
    relation_df = pd.read_csv(f"outputs/{config['output_dir']}/relations.csv")
    comments = pd.read_csv(f"inputs/{config['input']}.csv")
    clusters = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")
    labels = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_merge_labels.csv")

    hidden_properties_map: dict[str, list[str]] = config["hierarchical_aggregation"]["hidden_properties"]

    results["arguments"] = _build_arguments(clusters)
    results["clusters"] = _build_cluster_value(labels, arg_num)
    # NOTE: 属性に応じたコメントフィルタ機能が実装されておらず、全てのコメントが含まれてしまうので、コメントアウト
    # results["comments"] = _build_comments_value(
    #     comments, arguments, hidden_properties_map
    # )
    results["comment_num"] = len(comments)
    results["translations"] = _build_translations(config)
    # 属性情報のカラムは、元データに対して指定したカラムとclassificationするカテゴリを合わせたもの
    results["propertyMap"] = _build_property_map(arguments, hidden_properties_map, config)

    with open(f"outputs/{config['output_dir']}/hierarchical_overview.txt") as f:
        overview = f.read()
    print("overview")
    print(overview)
    results["overview"] = overview

    with open(path, "w") as file:
        json.dump(results, file, indent=2, ensure_ascii=False)
    # TODO: サンプリングロジックを実装したいが、現状は全件抽出
    create_custom_intro(config)
    if config["is_pubcom"]:
        add_original_comments(labels, arguments, relation_df, clusters, config)


def create_custom_intro(config):
    dataset = config["output_dir"]
    args_path = f"outputs/{dataset}/args.csv"
    comments = pd.read_csv(f"inputs/{config['input']}.csv")
    result_path = f"outputs/{dataset}/hierarchical_result.json"

    input_count = len(comments)
    args_count = len(pd.read_csv(args_path))
    processed_num = min(input_count, config["extraction"]["limit"])

    print(f"Input count: {input_count}")
    print(f"Args count: {args_count}")

    base_custom_intro = """{intro}
分析対象となったデータの件数は{processed_num}件で、これらのデータに対してOpenAI APIを用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。
"""

    intro = config["intro"]
    custom_intro = base_custom_intro.format(intro=intro, processed_num=processed_num, args_count=args_count)

    with open(result_path) as f:
        result = json.load(f)
    result["config"]["intro"] = custom_intro
    with open(result_path, "w") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)


def add_original_comments(labels, arguments, relation_df, clusters, config):
    # 大カテゴリ（cluster-level-1）に該当するラベルだけ抽出
    labels_lv1 = labels[labels["level"] == 1][["id", "label"]].rename(
        columns={"id": "cluster-level-1-id", "label": "category_label"}
    )

    # arguments と clusters をマージ（カテゴリ情報付与）
    merged = arguments.merge(clusters[["arg-id", "cluster-level-1-id"]], on="arg-id").merge(
        labels_lv1, on="cluster-level-1-id", how="left"
    )

    # relation_df と結合
    merged = merged.merge(relation_df, on="arg-id", how="left")

    # 元コメント取得
    comments = pd.read_csv(f"inputs/{config['input']}.csv")
    comments["comment-id"] = comments["comment-id"].astype(str)
    merged["comment-id"] = merged["comment-id"].astype(str)

    # 元コメント本文などとマージ
    final_df = merged.merge(comments, on="comment-id", how="left")

    # 必要カラムのみ整形
    final_cols = ["comment-id", "comment-body", "arg-id", "argument", "cluster-level-1-id", "category_label"]
    for col in ["source", "url"]:
        if col in comments.columns:
            final_cols.append(col)

    final_df = final_df[final_cols]
    final_df = final_df.rename(
        columns={
            "cluster-level-1-id": "category_id",
            "category_label": "category",
            "arg-id": "arg_id",
            "argument": "argument",
            "comment-body": "original-comment",
        }
    )

    # 保存
    final_df.to_csv(f"outputs/{config['output_dir']}/final_result_with_comments.csv", index=False)


def _build_arguments(clusters: pd.DataFrame) -> list[Argument]:
    cluster_columns = [col for col in clusters.columns if col.startswith("cluster-level-") and "id" in col]

    arguments: list[Argument] = []
    for _, row in clusters.iterrows():
        cluster_ids = ["0"]
        for cluster_column in cluster_columns:
            cluster_ids.append(row[cluster_column])
        argument: Argument = {
            "arg_id": row["arg-id"],
            "argument": row["argument"],
            "x": row["x"],
            "y": row["y"],
            "p": 0,  # NOTE: 一旦全部0でいれる
            "cluster_ids": cluster_ids,
        }
        arguments.append(argument)
    return arguments


def _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:
    results: list[Cluster] = [
        Cluster(
            level=0,
            id="0",
            label="全体",
            takeaway="",
            value=total_num,
            parent="",
            density_rank_percentile=0,
        )
    ]

    for _, melted_label in melted_labels.iterrows():
        cluster_value = Cluster(
            level=melted_label["level"],
            id=melted_label["id"],
            label=melted_label["label"],
            takeaway=melted_label["description"],
            value=melted_label["value"],
            parent=melted_label.get("parent", "全体"),
            density_rank_percentile=melted_label.get("density_rank_percentile"),
        )
        results.append(cluster_value)
    return results


def _build_comments_value(
    comments: pd.DataFrame,
    arguments: pd.DataFrame,
    hidden_properties_map: dict[str, list[str]],
):
    comment_dict: dict[str, dict[str, str]] = {}
    useful_comment_ids = set(arguments["comment-id"].values)
    for _, row in comments.iterrows():
        id = row["comment-id"]
        if id in useful_comment_ids:
            res = {"comment": row["comment-body"]}
            should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())
            if should_skip:
                continue
            comment_dict[str(id)] = res

    return comment_dict


def _build_translations(config):
    languages = list(config.get("translation", {}).get("languages", []))
    if len(languages) > 0:
        with open(f"outputs/{config['output_dir']}/translations.json") as f:
            translations = f.read()
        return json.loads(translations)
    return {}


def _build_property_map(
    arguments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict
) -> dict[str, dict[str, str]]:
    property_columns = list(hidden_properties_map.keys()) + list(config["extraction"]["categories"].keys())
    property_map = defaultdict(dict)

    # 指定された property_columns が arguments に存在するかチェック
    missing_cols = [col for col in property_columns if col not in arguments.columns]
    if missing_cols:
        raise ValueError(
            f"指定されたカラム {missing_cols} が args.csv に存在しません。"
            "設定ファイルaggregation / hidden_propertiesから該当カラムを取り除いてください。"
        )

    for prop in property_columns:
        for arg_id, row in arguments.iterrows():
            # LLMによるcategory classificationがうまく行かず、NaNの場合はNoneにする
            property_map[prop][arg_id] = row[prop] if not pd.isna(row[prop]) else None
    return property_map
1d:T13f1,import concurrent.futures
import json
import logging
import re

import pandas as pd
from tqdm import tqdm

from services.category_classification import classify_args
from services.llm import request_to_chat_openai
from services.parse_json_list import parse_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(batch_inputs, prompt, model, workers)

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": arg,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    classification_categories = config["extraction"]["categories"]
    if classification_categories:
        results = classify_args(results, config, workers)

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.ERROR)


def extract_batch(batch, prompt, model, workers):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model)) for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []
        return results


def extract_by_llm(input, prompt, model):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    response = request_to_chat_openai(messages=messages, model=model)
    return response


def extract_arguments(input, prompt, model, retries=1):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response = request_to_chat_openai(messages=messages, model=model, is_json=False)
        items = parse_response(response)
        items = filter(None, items)  # omit empty strings
        return items
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
1e:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
1f:T142a,import json
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd

from services.llm import request_to_chat_openai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response = request_to_chat_openai(messages=messages, model=model, is_json=True)
        response_json = json.loads(response)
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
20:T2fa5,import json
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from tqdm import tqdm

from services.llm import request_to_chat_openai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response = request_to_chat_openai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            is_json=True,
        )
        response_json = json.loads(response)
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
21:T4c0,"""Create summaries for the clusters."""

import pandas as pd

from services.llm import request_to_chat_openai


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input = ""
    for i, _ in enumerate(ids):
        input += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input += descriptions[i] + "\n\n"

    messages = [{"role": "user", "content": prompt}, {"role": "user", "content": input}]
    response = request_to_chat_openai(messages=messages, model=model)

    with open(path, "w") as file:
        file.write(response)
8:[["$","div",null,{"className":"container","children":[["$","$L11",null,{"meta":{"reporter":"テスト環境","message":"これは動作確認のためのテスト用のメタデータです。レポートの作成者に関する情報等を、public/meta/custom/metadata.jsonに記載することで、レポート上で情報を表示することができます。","webLink":null,"privacyLink":null,"termsLink":null,"brandColor":"#2577B1","isDefault":true}}],["$","$L12",null,{"mx":"auto","maxW":"750px","mb":10,"children":[["$","$L13",null,{"textAlign":"center","fontSize":"xl","mb":5,"children":"Report"}],["$","$L13",null,{"as":"h2","size":"4xl","mb":2,"className":"headingColor","children":"人々はロッテ佐々木朗希選手の完全試合達成にどのように反応したか"}],["$","$L14",null,{"fontWeight":"bold","fontSize":"xl","mb":2,"children":[["$","$L15",null,{"mr":1,"children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":20,"height":20,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-messages-square","children":[["$","path","p1xzt8",{"d":"M14 9a2 2 0 0 1-2 2H6l-4 4V4a2 2 0 0 1 2-2h8a2 2 0 0 1 2 2z"}],["$","path","1cx29u",{"d":"M18 9h2a2 2 0 0 1 2 2v11l-4-4h-6a2 2 0 0 1-2-2v-1"}],"$undefined"]}]}],"250","件"]}],["$","p",null,{"children":"各クラスタは、若手選手の成長や地域のスポーツ文化、特別な記録に対する称賛を中心に構成されています。若手選手の活躍が地域の誇りや未来への希望を象徴し、特に岩手県やメジャーリーグへの期待が強調されています。また、投手の記録更新や選手の健康に対する懸念も見られ、ファンは選手の成長を見守りつつ、成功を願っています。全体として、選手の成長と記録達成に対する期待が共通のテーマとなっています。"}]]}],["$","$L16",null,{"result":{"arguments":[{"arg_id":"Acsv-1_0","argument":"高校時代に監督が投げさせなかった選手が甲子園に行けなかったことが良かったのかもしれない","x":-4.2275743,"y":8.078887,"p":0,"cluster_ids":["0","1_1","2_8"]},{"arg_id":"Acsv-1_1","argument":"済美高校の安楽選手は連投させて問題になったが、その後どうなったのか気になる","x":-4.302046,"y":8.2677965,"p":0,"cluster_ids":["0","1_1","2_8"]},{"arg_id":"Acsv-2_0","argument":"岩手出身の優れた野球選手が登場した","x":-1.9793978,"y":7.3327684,"p":0,"cluster_ids":["0","1_3","2_6"]},{"arg_id":"Acsv-3_0","argument":"野田の名が残ったことを含めて完全な試合である","x":-2.223109,"y":9.517035,"p":0,"cluster_ids":["0","1_6","2_17"]},{"arg_id":"Acsv-3_1","argument":"中学生に投手としてのピークを強いることは虐待である","x":-4.067638,"y":8.449102,"p":0,"cluster_ids":["0","1_1","2_8"]},{"arg_id":"Acsv-4_0","argument":"13連続奪三振は4イニング中に野手の仕事が一切ないことを意味する","x":-3.8533025,"y":12.177228,"p":0,"cluster_ids":["0","1_2","2_31"]},{"arg_id":"Acsv-4_1","argument":"暑い中でボッ立ちしている野手に対する皮肉","x":-2.497813,"y":8.458743,"p":0,"cluster_ids":["0","1_3","2_19"]},{"arg_id":"Acsv-5_0","argument":"オリックス打線の状態は良くないが、本来の内容でも影響はなかったかもしれない","x":-3.3847053,"y":7.431242,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-6_0","argument":"江川並みのインパクトがある","x":-3.4980109,"y":8.186771,"p":0,"cluster_ids":["0","1_3","2_1"]},{"arg_id":"Acsv-7_0","argument":"野球に詳しくないが、閉塞感の中での明るいニュースは嬉しい","x":-2.1033504,"y":8.325776,"p":0,"cluster_ids":["0","1_3","2_19"]},{"arg_id":"Acsv-8_0","argument":"千葉テレビは放送を行わなかった","x":-2.6569219,"y":7.821555,"p":0,"cluster_ids":["0","1_3","2_29"]},{"arg_id":"Acsv-9_0","argument":"13者連続の奪三振で64年ぶりにプロ野球記録を更新した","x":-4.390353,"y":11.990458,"p":0,"cluster_ids":["0","1_2","2_9"]},{"arg_id":"Acsv-9_1","argument":"1試合19個の奪三振で27年ぶりにプロ野球記録に並んだ","x":-4.1948457,"y":11.963589,"p":0,"cluster_ids":["0","1_2","2_9"]},{"arg_id":"Acsv-9_2","argument":"記録ずくめの完全試合となった","x":-2.2261167,"y":10.001419,"p":0,"cluster_ids":["0","1_6","2_27"]},{"arg_id":"Acsv-10_0","argument":"中日ファンだったので近藤選手を思い出した。完全試合のことは忘れて、好きなように投げてほしい。","x":-2.9211671,"y":9.85226,"p":0,"cluster_ids":["0","1_6","2_11"]},{"arg_id":"Acsv-11_0","argument":"吉田正尚が3三振するのは2018年5月6日以来で、相手投手は中田賢一だった","x":-2.960014,"y":11.237516,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-11_1","argument":"三重殺と完全試合の記録があることは驚き","x":-2.3993654,"y":10.566337,"p":0,"cluster_ids":["0","1_6","2_27"]},{"arg_id":"Acsv-12_0","argument":"すごすぎ。早くメジャーに行ってほしい","x":-1.0539414,"y":10.384672,"p":0,"cluster_ids":["0","1_4","2_10"]},{"arg_id":"Acsv-13_0","argument":"吉田が三振するような投手に対抗できるチームは存在しない。彼の投球は素晴らしかった。","x":-2.9624152,"y":10.763544,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-14_0","argument":"AIは非常に優れた技術である","x":-1.687383,"y":9.4002,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-15_0","argument":"大船渡高校の監督は甲子園の予選決勝で選手を無理に投げさせなかったことで、選手の今の活躍を支えた英断だった","x":-4.4060307,"y":8.109231,"p":0,"cluster_ids":["0","1_1","2_8"]},{"arg_id":"Acsv-16_0","argument":"震災で傷つけられた少年が立派に成長したことに感動した","x":-1.4621891,"y":8.143598,"p":0,"cluster_ids":["0","1_3","2_30"]},{"arg_id":"Acsv-17_0","argument":"28年前は槇原だったのか","x":-4.2945204,"y":10.559849,"p":0,"cluster_ids":["0","1_2","2_16"]},{"arg_id":"Acsv-18_0","argument":"最終打者も三振したことに驚き","x":-3.0435228,"y":11.03655,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-19_0","argument":"夏の県決勝で投げさせなかった監督が偉い。結果が出て悪く言われることはないだろう。","x":-4.290572,"y":8.124913,"p":0,"cluster_ids":["0","1_1","2_8"]},{"arg_id":"Acsv-20_0","argument":"初完投初完封で完全試合とは何か","x":-2.0948093,"y":9.856219,"p":0,"cluster_ids":["0","1_6","2_17"]},{"arg_id":"Acsv-21_0","argument":"ピッチャーのフォークと剛速球のストレートは腕への負担が大きく、若いうちに怪我することが多いので心配である。彼の輝きを長く見たい。","x":-2.129312,"y":9.262723,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-22_0","argument":"三振を取りすぎて球が飛んできた時、野手がわちゃわちゃしていたのが凄かった","x":-3.1374002,"y":10.26117,"p":0,"cluster_ids":["0","1_2","2_7"]},{"arg_id":"Acsv-23_0","argument":"前人未到の大記録を達成した。完全試合（28年ぶり）、13者連続奪三振（64年ぶり更新）、19奪三振（27年ぶり）。早くメジャーに行くべき。","x":-4.1387215,"y":11.761324,"p":0,"cluster_ids":["0","1_2","2_9"]},{"arg_id":"Acsv-24_0","argument":"生きているうちにもう一度観られて良かった","x":-0.9506424,"y":8.385033,"p":0,"cluster_ids":["0","1_3","2_22"]},{"arg_id":"Acsv-24_1","argument":"槙原も成仏できる","x":-2.9392493,"y":9.879006,"p":0,"cluster_ids":["0","1_6","2_27"]},{"arg_id":"Acsv-25_0","argument":"槇原が28年前に達成したことを思い出し、年を取ったと感じる","x":-4.3560696,"y":10.558843,"p":0,"cluster_ids":["0","1_2","2_16"]},{"arg_id":"Acsv-26_0","argument":"高卒で長身の160超ピッチャーが奪三振記録を達成したことは、漫画を超えた圧巻の出来事である","x":-0.5193813,"y":10.170764,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-27_0","argument":"観たかった映画や番組についての感想","x":-0.81618637,"y":8.2873955,"p":0,"cluster_ids":["0","1_3","2_22"]},{"arg_id":"Acsv-28_0","argument":"9回投げて105球は少ない。ファールや遊び玉が24球もあった。槙原でリアルタイム感動したおっさんだけど、若い選手が記録を作るのは嬉しい。今後も怪我少なく活躍してほしい。","x":-3.894495,"y":10.587891,"p":0,"cluster_ids":["0","1_2","2_7"]},{"arg_id":"Acsv-29_0","argument":"すごい","x":-0.18820156,"y":8.902571,"p":0,"cluster_ids":["0","1_5","2_18"]},{"arg_id":"Acsv-30_0","argument":"母がスマホの通知を見て、ロウキの完全試合を知らせた","x":-2.363784,"y":10.08752,"p":0,"cluster_ids":["0","1_6","2_27"]},{"arg_id":"Acsv-31_0","argument":"松川がここぞでしっかり捕球したのは大きかった。18歳で素晴らしい活躍を見せたことにおめでとうと言いたい","x":-4.4484754,"y":9.126882,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-32_0","argument":"ポスティングにおけるMLB各球団の出資額についての疑問","x":-2.2068336,"y":8.455109,"p":0,"cluster_ids":["0","1_3","2_19"]},{"arg_id":"Acsv-33_0","argument":"次は一試合27奪三振を目指すべき","x":-3.404899,"y":12.179805,"p":0,"cluster_ids":["0","1_2","2_28"]},{"arg_id":"Acsv-34_0","argument":"本当にすごい投球だった","x":-2.2031891,"y":9.170462,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-35_0","argument":"漫画「ブ●ゴ」のような試合だ","x":-1.5691915,"y":8.968844,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-36_0","argument":"90年代のロッテは人材不足で選手を酷使していたが、今はじっくり育てているのが素晴らしい。千葉ロッテが球界の至宝を育てるとは思わなかった。","x":-3.1479042,"y":8.404136,"p":0,"cluster_ids":["0","1_3","2_1"]},{"arg_id":"Acsv-37_0","argument":"試合内容がMAJORの茂野吾郎のような感じである","x":-1.9994674,"y":9.208993,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-38_0","argument":"動画の内容が無茶苦茶で、ストレートを打者がほとんど着払いしていた","x":-3.3240206,"y":7.892617,"p":0,"cluster_ids":["0","1_3","2_1"]},{"arg_id":"Acsv-39_0","argument":"キャッチャーの松川を信じて投げた","x":-4.0747943,"y":9.1240635,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-40_0","argument":"横浜に現れた大魔神のように、佐々木さんは令和の怪物として大活躍するのか","x":-2.2531838,"y":11.464947,"p":0,"cluster_ids":["0","1_4","2_34"]},{"arg_id":"Acsv-41_0","argument":"岩手県から3人目の刺客がMLBに挑戦する","x":-1.9344162,"y":7.327617,"p":0,"cluster_ids":["0","1_3","2_6"]},{"arg_id":"Acsv-42_0","argument":"ヒーローインタビューに松川を呼ばない理由についての疑問","x":-4.1767097,"y":8.830131,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-43_0","argument":"今すぐにメジャーに行くべきであり、素晴らしい生涯成績を残す可能性がある","x":-0.97053075,"y":10.31794,"p":0,"cluster_ids":["0","1_4","2_10"]},{"arg_id":"Acsv-45_0","argument":"投球内容が優れた完全試合は素晴らしい","x":-2.3774276,"y":9.383357,"p":0,"cluster_ids":["0","1_6","2_32"]},{"arg_id":"Acsv-46_0","argument":"ダイジェストを見る限り、あと9イニングやってもランナーを出さない余裕があったように見える。末恐ろしい。","x":-3.3215384,"y":11.479312,"p":0,"cluster_ids":["0","1_2","2_2"]},{"arg_id":"Acsv-47_0","argument":"岩手は人が少ない状況にある","x":-1.5700577,"y":7.092914,"p":0,"cluster_ids":["0","1_3","2_6"]},{"arg_id":"Acsv-48_0","argument":"メジャーに行くの？","x":-1.1526644,"y":10.43212,"p":0,"cluster_ids":["0","1_4","2_10"]},{"arg_id":"Acsv-49_0","argument":"まーくんや大谷くんのような選手は二度と出ないと思ったが、また新たな選手が現れた","x":-3.6562412,"y":9.543291,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-50_0","argument":"334か月ぶりというのは驚きである","x":-0.36924967,"y":10.496984,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-51_0","argument":"オリックス打線が打てないのは相手投手がすごすぎるからだ","x":-3.2521517,"y":7.5200377,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-52_0","argument":"19奪三振は野手に飛んだのが8つで、サードとショートは9回に初めてゴロが飛んできたことから緊張が伝わる","x":-3.6241918,"y":11.249335,"p":0,"cluster_ids":["0","1_2","2_23"]},{"arg_id":"Acsv-52_1","argument":"レフトは守備機会がなかった","x":-2.5995307,"y":8.886421,"p":0,"cluster_ids":["0","1_6","2_32"]},{"arg_id":"Acsv-53_0","argument":"プロ初完投初完封がこれで末恐ろしい。松川捕手も素晴らしい。","x":-4.633362,"y":9.460222,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-54_0","argument":"槙原の完全試合を観戦していたことを覚えている。そこから誰も達成していなかったのはすごい。観戦していた人は運が良い。","x":-2.5339842,"y":10.021181,"p":0,"cluster_ids":["0","1_6","2_27"]},{"arg_id":"Acsv-55_0","argument":"岩手では化け物が量産されすぎているのではないか？少子化の時代において問題である","x":-1.4903448,"y":7.156254,"p":0,"cluster_ids":["0","1_3","2_6"]},{"arg_id":"Acsv-56_0","argument":"槇原の完全視界から28年が経過した","x":-4.360751,"y":10.657523,"p":0,"cluster_ids":["0","1_2","2_16"]},{"arg_id":"Acsv-57_0","argument":"本人のコメントが良い、努力を続けてきたことが伝わる","x":-0.87445706,"y":9.488203,"p":0,"cluster_ids":["0","1_5","2_5"]},{"arg_id":"Acsv-58_0","argument":"連続13奪三振は完全試合より難しい","x":-3.4157207,"y":12.115851,"p":0,"cluster_ids":["0","1_2","2_28"]},{"arg_id":"Acsv-58_1","argument":"完全試合で奪三振記録タイの選手がいる","x":-3.6296172,"y":12.093899,"p":0,"cluster_ids":["0","1_2","2_31"]},{"arg_id":"Acsv-58_2","argument":"パリーグTVがこの話題を取り上げている","x":-2.461346,"y":7.7182574,"p":0,"cluster_ids":["0","1_3","2_29"]},{"arg_id":"Acsv-59_0","argument":"ピッチャーが誰も塁に出さないことを「完全試合」と呼ぶことにモヤる","x":-2.6570544,"y":9.476679,"p":0,"cluster_ids":["0","1_6","2_11"]},{"arg_id":"Acsv-59_1","argument":"野球のパーフェクトはそういうことでいいのか疑問","x":-2.3172715,"y":8.580329,"p":0,"cluster_ids":["0","1_3","2_19"]},{"arg_id":"Acsv-59_2","argument":"佐々木選手おめでとうございます","x":-5.6476398,"y":9.529192,"p":0,"cluster_ids":["0","1_1","2_4"]},{"arg_id":"Acsv-60_0","argument":"岩手には特別なDNAがあるのだろうか？","x":-1.6457732,"y":7.004059,"p":0,"cluster_ids":["0","1_3","2_6"]},{"arg_id":"Acsv-61_0","argument":"ヒーローインタビューで捕手の松川を褒めるのは良いが、もっと自分のことも語ってほしい","x":-4.370256,"y":8.715041,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-62_0","argument":"ロッテの未来は明るい。若いバッテリー組んだ捕手の活躍が嬉しい。","x":-3.1204875,"y":8.52307,"p":0,"cluster_ids":["0","1_3","2_1"]},{"arg_id":"Acsv-63_0","argument":"速いフォークと速すぎるストレートの2種類でやっているのか？素人なので今後の解説が楽しみ","x":-1.3861895,"y":9.872064,"p":0,"cluster_ids":["0","1_6","2_17"]},{"arg_id":"Acsv-64_0","argument":"若者の現実離れが凄い","x":-1.3802555,"y":7.7405543,"p":0,"cluster_ids":["0","1_3","2_30"]},{"arg_id":"Acsv-64_1","argument":"高卒1年目の捕手が凄い","x":-4.606525,"y":9.213562,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-65_0","argument":"マンガを超える記録が出たことを祝うべき","x":-0.31378457,"y":10.034096,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-65_1","argument":"西武がトリプルプレイを決めたことに言及","x":-2.6616507,"y":10.356218,"p":0,"cluster_ids":["0","1_6","2_27"]},{"arg_id":"Acsv-65_2","argument":"天気が良くなると野球の質が上がる可能性についての考察","x":-2.0832264,"y":8.717796,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-66_0","argument":"ロッテの育成が成功し、捕手を含めて制球が向上した","x":-3.1928074,"y":8.506674,"p":0,"cluster_ids":["0","1_3","2_1"]},{"arg_id":"Acsv-67_0","argument":"令和の怪物は13連続奪三振や19奪三振、9回105球などの記録を達成し、吉田正尚から3三振を奪うなど、完全試合以外の偉業も素晴らしい","x":-3.129768,"y":11.659178,"p":0,"cluster_ids":["0","1_2","2_2"]},{"arg_id":"Acsv-68_0","argument":"松川くんは高卒1年目の捕手として開幕スタメンで完全試合を達成したことがすごい","x":-4.6946692,"y":9.191819,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-68_1","argument":"松川くんのような選手は2度と現れないレベルであり、フレッシュバッテリーが最高である","x":-4.57013,"y":9.539794,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-69_0","argument":"つよすぎ","x":-0.22655594,"y":8.685966,"p":0,"cluster_ids":["0","1_5","2_18"]},{"arg_id":"Acsv-70_0","argument":"佐々木郎希は岩手県でここ10年で3人いるレベルのピッチャーであり、岩手県の野球のレベルが高いことを示している","x":-5.413616,"y":9.274935,"p":0,"cluster_ids":["0","1_1","2_4"]},{"arg_id":"Acsv-71_0","argument":"佐々木投手は13者連続奪三振で64年ぶりにプロ野球記録を更新した","x":-4.616127,"y":11.949024,"p":0,"cluster_ids":["0","1_2","2_9"]},{"arg_id":"Acsv-72_0","argument":"マリーンズの編成と育成の集大成が素晴らしい","x":-1.9547876,"y":9.848275,"p":0,"cluster_ids":["0","1_6","2_17"]},{"arg_id":"Acsv-73_0","argument":"陸前高田を襲った震災を生き延びたことに感謝し、感動している","x":-1.5813352,"y":8.122498,"p":0,"cluster_ids":["0","1_3","2_30"]},{"arg_id":"Acsv-74_0","argument":"あっぱれ","x":-0.2111115,"y":8.679736,"p":0,"cluster_ids":["0","1_5","2_18"]},{"arg_id":"Acsv-75_0","argument":"捕手の松川は高卒1年目で完全試合を達成し、打撃でも走者を一掃する活躍を見せている","x":-4.779515,"y":9.135937,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-76_0","argument":"身体の線が華奢な印象でも、努力して大記録を達成したことを称賛する","x":-0.5500485,"y":9.685434,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-76_1","argument":"今後の影響が悪くならないことを祈り、質の悪い故障を避けて自然体で成長してほしい","x":-0.9254595,"y":9.308521,"p":0,"cluster_ids":["0","1_5","2_5"]},{"arg_id":"Acsv-77_0","argument":"28年ぶりということは、山井と岩瀬の件は考慮されないということだ。当時は合理的な判断だと思っていたが、今になってみると代えなければよかったと思う。","x":-4.4076257,"y":10.339832,"p":0,"cluster_ids":["0","1_2","2_16"]},{"arg_id":"Acsv-78_0","argument":"狼鬼は今年、食玩が発売予定","x":-1.7896196,"y":11.41465,"p":0,"cluster_ids":["0","1_4","2_0"]},{"arg_id":"Acsv-78_1","argument":"完全試合達成者の3/16が佐々木姓","x":-5.2995396,"y":9.984031,"p":0,"cluster_ids":["0","1_1","2_25"]},{"arg_id":"Acsv-79_0","argument":"捕手と合わせて38歳では今後の更新がほぼ不可能ではないか","x":-4.309264,"y":9.995603,"p":0,"cluster_ids":["0","1_2","2_21"]},{"arg_id":"Acsv-80_0","argument":"ブコメを読むだけでわくわく感が伝わってきて楽しい","x":-1.3999821,"y":8.615197,"p":0,"cluster_ids":["0","1_3","2_22"]},{"arg_id":"Acsv-81_0","argument":"大谷翔平が霞んで見えるほど無茶苦茶だ","x":-3.3381507,"y":9.463561,"p":0,"cluster_ids":["0","1_6","2_11"]},{"arg_id":"Acsv-82_0","argument":"ソフトバンクユーザー特典でネット中継を視聴できた","x":-2.519749,"y":7.5029516,"p":0,"cluster_ids":["0","1_3","2_29"]},{"arg_id":"Acsv-82_1","argument":"槙原氏のパーフェクトゲームを見た記憶がある","x":-2.6814022,"y":9.941452,"p":0,"cluster_ids":["0","1_6","2_27"]},{"arg_id":"Acsv-82_2","argument":"長嶋一茂がフライを取るのをビクビクしながら見ていた","x":-3.026934,"y":10.148744,"p":0,"cluster_ids":["0","1_6","2_27"]},{"arg_id":"Acsv-82_3","argument":"今日のショートプレイも危なかった","x":-2.6973972,"y":9.107595,"p":0,"cluster_ids":["0","1_6","2_32"]},{"arg_id":"Acsv-83_0","argument":"玉山鉄二が十数年ぶりに主演映画を持つようだ","x":-4.7832346,"y":11.105602,"p":0,"cluster_ids":["0","1_2","2_26"]},{"arg_id":"Acsv-84_0","argument":"これはすごい","x":-0.24900594,"y":9.074381,"p":0,"cluster_ids":["0","1_5","2_18"]},{"arg_id":"Acsv-85_0","argument":"1試合のパフォーマンスとして日本プロ野球史上最高ではないか","x":-3.988033,"y":11.7567005,"p":0,"cluster_ids":["0","1_2","2_9"]},{"arg_id":"Acsv-86_0","argument":"やきう漫画で9回19奪三振105球の完全試合を描いたら編集に叱責されるかもしれない","x":-3.6157017,"y":11.148641,"p":0,"cluster_ids":["0","1_2","2_23"]},{"arg_id":"Acsv-87_0","argument":"吉田の打席が印象的で、あの三振で勝負が決まった感じがある","x":-2.7993376,"y":10.945383,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-88_0","argument":"日本ですることがないと感じる","x":-0.96461755,"y":8.266883,"p":0,"cluster_ids":["0","1_3","2_22"]},{"arg_id":"Acsv-89_0","argument":"岩手県は素晴らしい","x":-1.780066,"y":7.00628,"p":0,"cluster_ids":["0","1_3","2_6"]},{"arg_id":"Acsv-90_0","argument":"佐々木希に名前が似ている人としか認識していなかったことをお詫びします。おめでとうございます。","x":-5.769078,"y":9.426382,"p":0,"cluster_ids":["0","1_1","2_4"]},{"arg_id":"Acsv-91_0","argument":"完全試合は素晴らしい","x":-1.9388478,"y":9.780087,"p":0,"cluster_ids":["0","1_6","2_17"]},{"arg_id":"Acsv-92_0","argument":"近藤真一投手は初登板以降ぱっとしなかったが、偉大な記録を持っている。","x":-4.8345137,"y":10.810314,"p":0,"cluster_ids":["0","1_2","2_26"]},{"arg_id":"Acsv-92_1","argument":"佐々木投手はこの一試合だけでも伝説に残るに十分であり、今後も実績を積み重ねてほしい。","x":-5.0012364,"y":10.002824,"p":0,"cluster_ids":["0","1_1","2_25"]},{"arg_id":"Acsv-93_0","argument":"大船渡の監督とロッテは佐々木朗希を適切に育てたことを評価されるべき","x":-4.714427,"y":8.13128,"p":0,"cluster_ids":["0","1_1","2_8"]},{"arg_id":"Acsv-94_0","argument":"フォークが150kmで草生える","x":-1.4623041,"y":9.96353,"p":0,"cluster_ids":["0","1_6","2_17"]},{"arg_id":"Acsv-95_0","argument":"これはまだ伝説の始まりにすぎないという凄みがある","x":-0.5464617,"y":9.952144,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-96_0","argument":"完全試合は槇原以来で、1試合19奪三振はフォークお化けの野田以来。連続奪三振は50年代の記録で、これに追いつける選手は当分出なさそう","x":-4.043695,"y":11.691002,"p":0,"cluster_ids":["0","1_2","2_9"]},{"arg_id":"Acsv-97_0","argument":"佐々木朗希の成長が理想通りで驚いている","x":-5.483157,"y":9.273489,"p":0,"cluster_ids":["0","1_1","2_4"]},{"arg_id":"Acsv-97_1","argument":"佐々木世代の高卒ドライチ投手の順調な成長が素晴らしい","x":-5.2424893,"y":9.365864,"p":0,"cluster_ids":["0","1_1","2_4"]},{"arg_id":"Acsv-97_2","argument":"西純は焦らず頑張ってほしい","x":-0.8752369,"y":9.467535,"p":0,"cluster_ids":["0","1_5","2_5"]},{"arg_id":"Acsv-98_0","argument":"野球については詳しくないが、素晴らしいスポーツだと思う","x":-2.3248842,"y":8.527539,"p":0,"cluster_ids":["0","1_3","2_19"]},{"arg_id":"Acsv-99_0","argument":"佐々木希さんと朝倉未来選手が混同されることがある","x":-5.732086,"y":9.503196,"p":0,"cluster_ids":["0","1_1","2_4"]},{"arg_id":"Acsv-100_0","argument":"心拍数上がらず涼しい顔で160kmを投げ、19三振を105球で達成した。ほぼストライクしか投げていない。","x":-3.7305326,"y":10.609892,"p":0,"cluster_ids":["0","1_2","2_7"]},{"arg_id":"Acsv-101_0","argument":"メジャーで通用するストレートとフォークを持っている","x":-1.3884892,"y":10.071845,"p":0,"cluster_ids":["0","1_4","2_10"]},{"arg_id":"Acsv-101_1","argument":"体作りと耐久力が必要","x":-1.2522352,"y":9.550189,"p":0,"cluster_ids":["0","1_5","2_5"]},{"arg_id":"Acsv-102_0","argument":"大船渡の監督は彼の肩と将来を大事にしたことを評価すべき","x":-4.577999,"y":8.099566,"p":0,"cluster_ids":["0","1_1","2_8"]},{"arg_id":"Acsv-103_0","argument":"本物であることを示すべき","x":-0.56522304,"y":8.899241,"p":0,"cluster_ids":["0","1_5","2_18"]},{"arg_id":"Acsv-104_0","argument":"大谷選手の活躍に驚き、岩手の素晴らしさを感じた","x":-2.3484561,"y":8.026046,"p":0,"cluster_ids":["0","1_3","2_19"]},{"arg_id":"Acsv-105_0","argument":"サファテの先発完投についての書き込みがあり、WHIP0.39の驚異的な数字に驚いた","x":-3.5886116,"y":11.350161,"p":0,"cluster_ids":["0","1_2","2_23"]},{"arg_id":"Acsv-106_0","argument":"20歳で160kmを投げるのはすごい。これからさらに成長して、25歳で160後半から170kmに達する可能性がある。","x":-1.1362907,"y":10.798967,"p":0,"cluster_ids":["0","1_4","2_35"]},{"arg_id":"Acsv-107_0","argument":"オリックスはコロナで一軍の主力を欠いて朗希に当たるのは気の毒","x":-3.4560208,"y":7.1856394,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-107_1","argument":"横浜はコロナ感染者が出て中止になったのにオリックスは中止にならなかった理由が疑問","x":-3.4326074,"y":7.1236777,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-108_0","argument":"今シーズン中にあと何回できるか予想する必要がある","x":-1.4993298,"y":10.906495,"p":0,"cluster_ids":["0","1_4","2_35"]},{"arg_id":"Acsv-109_0","argument":"彼の野球人生が長く続くことを願う","x":-1.8734285,"y":8.607785,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-110_0","argument":"野球についてよく分からないが、凄いスポーツであることは理解しているので記念にブックマークする","x":-2.215227,"y":8.219581,"p":0,"cluster_ids":["0","1_3","2_19"]},{"arg_id":"Acsv-111_0","argument":"松川捕手は2週間前まで高校生だったとは思えない貫禄がある","x":-4.626293,"y":8.932872,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-111_1","argument":"ロッテに関連する話題である","x":-2.701837,"y":7.7870035,"p":0,"cluster_ids":["0","1_3","2_29"]},{"arg_id":"Acsv-112_0","argument":"三振数に対して球数が不適切である","x":-3.546423,"y":10.294612,"p":0,"cluster_ids":["0","1_2","2_7"]},{"arg_id":"Acsv-112_1","argument":"遊び球という概念が考慮されていない","x":-3.474205,"y":10.016397,"p":0,"cluster_ids":["0","1_2","2_7"]},{"arg_id":"Acsv-112_2","argument":"球数に関して他にも問題がある","x":-3.46387,"y":10.18161,"p":0,"cluster_ids":["0","1_2","2_7"]},{"arg_id":"Acsv-113_0","argument":"今日のスポーツニュースはこの人が主役である","x":-2.1990626,"y":7.7922235,"p":0,"cluster_ids":["0","1_3","2_29"]},{"arg_id":"Acsv-114_0","argument":"バッテリー2人の年齢を足すと38歳になる","x":-4.4183135,"y":9.895208,"p":0,"cluster_ids":["0","1_2","2_21"]},{"arg_id":"Acsv-115_0","argument":"9回19奪三振105球は非常に素晴らしい成績である","x":-3.7180371,"y":11.347567,"p":0,"cluster_ids":["0","1_2","2_23"]},{"arg_id":"Acsv-116_0","argument":"チートについての議論が必要","x":-0.6945016,"y":7.501646,"p":0,"cluster_ids":["0","1_3","2_13"]},{"arg_id":"Acsv-117_0","argument":"現実の出来事が漫画のように信じられないことがある","x":-1.1564599,"y":8.843716,"p":0,"cluster_ids":["0","1_3","2_22"]},{"arg_id":"Acsv-118_0","argument":"高卒ルーキーの松川がキャッチャーとしてスタメンで試合に出ているのがすごい","x":-4.497601,"y":9.1980715,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-119_0","argument":"昨日好調だった佐野を含めて8人がコロナで出られなかった","x":-3.573042,"y":7.026383,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-120_0","argument":"岩手で号外が出るレベルの出来事がある","x":-1.6827468,"y":7.0487933,"p":0,"cluster_ids":["0","1_3","2_6"]},{"arg_id":"Acsv-121_0","argument":"無理に甲子園を目指さずにいた監督の判断は正しかった","x":-4.2321396,"y":8.003286,"p":0,"cluster_ids":["0","1_1","2_8"]},{"arg_id":"Acsv-122_0","argument":"ノーヒットノーランや完全試合は凄いが、それだけのこと","x":-1.9125063,"y":9.670113,"p":0,"cluster_ids":["0","1_6","2_17"]},{"arg_id":"Acsv-123_0","argument":"年齢とキャリアからして、あと3回くらい完全試合を達成するポテンシャルがある選手が出てきた。大谷よりも凄い選手は出ないと思っていたが、投手として本当にとんでもない選手が現れた。","x":-3.922784,"y":9.761672,"p":0,"cluster_ids":["0","1_2","2_21"]},{"arg_id":"Acsv-124_0","argument":"めでた","x":-0.15438776,"y":8.824716,"p":0,"cluster_ids":["0","1_5","2_18"]},{"arg_id":"Acsv-125_0","argument":"完全試合だけでもすごいが、さらに多くの記録があることが素晴らしい","x":-1.6243457,"y":10.191027,"p":0,"cluster_ids":["0","1_6","2_17"]},{"arg_id":"Acsv-127_0","argument":"素晴らしい。","x":-0.32733086,"y":9.111081,"p":0,"cluster_ids":["0","1_5","2_18"]},{"arg_id":"Acsv-128_0","argument":"Youtubeで見た試合は強烈だった。最初は全然バットが当たらなかったが、後半は当ててきた。最後のバッターでまた三振を取った2001年世代はすごい。","x":-3.8013742,"y":10.650518,"p":0,"cluster_ids":["0","1_2","2_7"]},{"arg_id":"Acsv-129_0","argument":"27年ぶりの奪三振タイ記録に驚いている","x":-3.7040894,"y":12.056669,"p":0,"cluster_ids":["0","1_2","2_31"]},{"arg_id":"Acsv-130_0","argument":"去年首位打者の吉田は今季ここまで58打席で1三振だが、今日は3打数3三振を記録した。去年は455打席で26三振をしており、4試合に1度しか三振しないバッターに3三振を奪ったバッテリーが注目される。","x":-3.0460582,"y":11.166418,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-131_0","argument":"ダルビッシュ、大谷翔平、佐々木朗希を育てたコーチはメジャーから声がかかる可能性がある","x":-3.7428613,"y":8.673519,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-132_0","argument":"オリックスのピッチャー山本は試合を30分くらいで終わらせることができそう","x":-3.1948404,"y":7.4540973,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-133_0","argument":"歴史に関する議論が必要","x":-0.6824942,"y":7.4978075,"p":0,"cluster_ids":["0","1_3","2_13"]},{"arg_id":"Acsv-134_0","argument":"この時代に大記録が出るとは驚きである","x":-0.34001967,"y":10.15206,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-135_0","argument":"ちょっとどうかしている記録が生まれてしまった","x":-0.15776859,"y":9.827197,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-136_0","argument":"パTVがまとめ動画をYouTubeにアップする予定なので、ぜひ見てほしい。おめでとう！最近の若い子たちは本当にすごいね。","x":-1.8650395,"y":7.8126645,"p":0,"cluster_ids":["0","1_3","2_30"]},{"arg_id":"Acsv-137_0","argument":"やはり怪物だった","x":-1.9819759,"y":11.583958,"p":0,"cluster_ids":["0","1_4","2_0"]},{"arg_id":"Acsv-138_0","argument":"令和の怪物が5回まで13者連続奪三振を記録し、完全試合を達成した。じっくり育成され、初の開幕ローテーション入りを果たした。","x":-2.768176,"y":11.726001,"p":0,"cluster_ids":["0","1_4","2_34"]},{"arg_id":"Acsv-139_0","argument":"昭和初の完全試合は藤本英雄(背番号17)、平成初の完全試合は槙原寛己(背番号17)、21世紀初・令和初の完全試合は佐々木朗希(背番号17)が達成した","x":-4.87611,"y":11.165465,"p":0,"cluster_ids":["0","1_2","2_26"]},{"arg_id":"Acsv-140_0","argument":"プロ3年目の選手は体は細いが、キレがあって素晴らしい","x":-4.092214,"y":9.748694,"p":0,"cluster_ids":["0","1_2","2_21"]},{"arg_id":"Acsv-141_0","argument":"千葉マリンについての話題があり、古田、俊介、星野、下柳がマリン風の話をする動画を見た","x":-2.5302126,"y":8.061179,"p":0,"cluster_ids":["0","1_3","2_19"]},{"arg_id":"Acsv-141_1","argument":"野田の19Kも千葉マリンで、ランナーを出さなければ盗塁もされない","x":-3.0062017,"y":8.880769,"p":0,"cluster_ids":["0","1_6","2_32"]},{"arg_id":"Acsv-142_0","argument":"この試合を見逃したことは次の完全試合が達成するまで晴らせない。前回の槇原氏の時と同じ6-0","x":-2.6142125,"y":10.124363,"p":0,"cluster_ids":["0","1_6","2_27"]},{"arg_id":"Acsv-143_0","argument":"13者連続奪三振はNPBの歴史で過去最高記録の9を一気に更新する凄い記録だと思う","x":-4.0064406,"y":12.308563,"p":0,"cluster_ids":["0","1_2","2_31"]},{"arg_id":"Acsv-144_0","argument":"令和の怪物と呼ばれる選手は、捕手の子も含めて非常に優れた能力を持っている","x":-2.4912074,"y":11.602454,"p":0,"cluster_ids":["0","1_4","2_34"]},{"arg_id":"Acsv-145_0","argument":"13連続奪三振、計19奪三振は異次元の記録","x":-3.8994544,"y":12.261062,"p":0,"cluster_ids":["0","1_2","2_31"]},{"arg_id":"Acsv-146_0","argument":"平成6年に巨人の槙原寛己投手が達成した完全試合は、28年ぶりで16人目の記録である","x":-4.6707983,"y":11.372608,"p":0,"cluster_ids":["0","1_2","2_33"]},{"arg_id":"Acsv-147_0","argument":"マイライフスレについての議論","x":-0.7274321,"y":7.463402,"p":0,"cluster_ids":["0","1_3","2_13"]},{"arg_id":"Acsv-148_0","argument":"160キロが当たり前の時代が来るのかもしれない","x":-0.8072517,"y":10.644538,"p":0,"cluster_ids":["0","1_4","2_10"]},{"arg_id":"Acsv-148_1","argument":"来年には167キロを投げる選手が出てくるかもしれない","x":-1.3523245,"y":11.062534,"p":0,"cluster_ids":["0","1_4","2_35"]},{"arg_id":"Acsv-149_0","argument":"すごいと感じる","x":-0.52360594,"y":8.652066,"p":0,"cluster_ids":["0","1_5","2_18"]},{"arg_id":"Acsv-150_0","argument":"高校野球（甲子園）での投球内容が漫画のように没にならなかったことが重要","x":-3.4931624,"y":8.057604,"p":0,"cluster_ids":["0","1_3","2_1"]},{"arg_id":"Acsv-150_1","argument":"三木谷王国や吉村帝国を回避して吉井のいる海浜幕張へ行けたことが大きい","x":-2.4703753,"y":11.15091,"p":0,"cluster_ids":["0","1_4","2_34"]},{"arg_id":"Acsv-151_0","argument":"キャッチャーの若さには物語性がある","x":-3.50366,"y":9.159942,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-152_0","argument":"27奪三振の完全試合への夢が膨らむ","x":-3.2736957,"y":12.122864,"p":0,"cluster_ids":["0","1_2","2_28"]},{"arg_id":"Acsv-153_0","argument":"三振を多く取りながら球数が少ないのは素晴らしい。20歳と18歳のバッテリーが成し遂げたことは特筆すべきで、佐々木朗希は今後もノーノーや完全試合を達成する可能性がある","x":-4.64176,"y":9.848817,"p":0,"cluster_ids":["0","1_1","2_25"]},{"arg_id":"Acsv-154_0","argument":"過保護と批判されていたが、結果的に正しかった","x":-4.5593204,"y":7.8520045,"p":0,"cluster_ids":["0","1_1","2_8"]},{"arg_id":"Acsv-155_0","argument":"完全試合達成にはエラーによる出塁が許されないため、後半は守っている野手陣も緊張しただろう","x":-2.5591795,"y":8.997479,"p":0,"cluster_ids":["0","1_6","2_32"]},{"arg_id":"Acsv-156_0","argument":"槙原寛己投手の記録の凄さを理解した","x":-4.506827,"y":11.276737,"p":0,"cluster_ids":["0","1_2","2_33"]},{"arg_id":"Acsv-157_0","argument":"まさに令和の怪物だ","x":-1.8991411,"y":11.504837,"p":0,"cluster_ids":["0","1_4","2_0"]},{"arg_id":"Acsv-158_0","argument":"大谷以上の素材が出てくるとは思わなかったが、早熟ではない","x":-3.2299392,"y":9.406376,"p":0,"cluster_ids":["0","1_6","2_11"]},{"arg_id":"Acsv-158_1","argument":"大リーグで活躍してほしいが、1年目は出場しなかったため、大リーグ行きは遅れる可能性がある","x":-1.334645,"y":10.522204,"p":0,"cluster_ids":["0","1_4","2_10"]},{"arg_id":"Acsv-158_2","argument":"早めに高く売れることを期待している","x":-0.8164403,"y":10.519022,"p":0,"cluster_ids":["0","1_4","2_10"]},{"arg_id":"Acsv-159_0","argument":"プロ野球史上最高のピッチングで、今後50年や100年はこれ以上の投球内容は出てこないかもしれない","x":-2.9120088,"y":9.448149,"p":0,"cluster_ids":["0","1_6","2_11"]},{"arg_id":"Acsv-160_0","argument":"相手チームファンでも楽しめるほどの素晴らしいパフォーマンス","x":-1.3596561,"y":9.008042,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-161_0","argument":"感嘆の表現","x":-0.7802315,"y":8.612105,"p":0,"cluster_ids":["0","1_3","2_22"]},{"arg_id":"Acsv-162_0","argument":"野手は暇だったのだろう","x":-2.6393728,"y":8.651701,"p":0,"cluster_ids":["0","1_6","2_32"]},{"arg_id":"Acsv-163_0","argument":"AIの概念をガンダムに例えると、AIはパイロットが操るモビルスーツのようなもので、パイロットの能力や判断力によってその性能が大きく変わる。","x":-1.6407503,"y":9.162433,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-165_0","argument":"故障せずにプロでピークを迎えられて良かった。これからも故障に気をつけて頑張ってほしい","x":-0.8677023,"y":9.376361,"p":0,"cluster_ids":["0","1_5","2_5"]},{"arg_id":"Acsv-166_0","argument":"キャッチャーがルーキーであることを称えることは玄人っぽい印象を与える","x":-3.6307633,"y":9.038848,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-167_0","argument":"20歳と18歳の若い人の活躍は嬉しい","x":-4.313532,"y":9.316324,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-168_0","argument":"マリーンズとバファローズのユニフォームは上が黒、下が白なので注意が必要","x":-2.034755,"y":9.775535,"p":0,"cluster_ids":["0","1_6","2_17"]},{"arg_id":"Acsv-169_0","argument":"藤岡が9回ショートゴロを下がって捕り、かなりガチガチに見えた","x":-3.4406474,"y":11.000038,"p":0,"cluster_ids":["0","1_2","2_23"]},{"arg_id":"Acsv-170_0","argument":"アンガールズの佐々木希は素晴らしい","x":-5.6280313,"y":9.338034,"p":0,"cluster_ids":["0","1_1","2_4"]},{"arg_id":"Acsv-171_0","argument":"味方が強すぎてやることがないという状況はリアルスポーツでも見られる","x":-1.8162557,"y":8.9034605,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-172_0","argument":"ストレートが10km早い大魔神が9イニング続けて出てくるとこうなる","x":-3.1932132,"y":11.661625,"p":0,"cluster_ids":["0","1_2","2_2"]},{"arg_id":"Acsv-173_0","argument":"ロッテの佐々木朗希投手が28年ぶり16人目の完全試合を達成した","x":-4.7860126,"y":11.552877,"p":0,"cluster_ids":["0","1_2","2_33"]},{"arg_id":"Acsv-173_1","argument":"佐々木投手は13者連続三振を奪い、プロ野球記録を64年ぶりに更新した","x":-4.6663837,"y":11.929931,"p":0,"cluster_ids":["0","1_2","2_9"]},{"arg_id":"Acsv-173_2","argument":"この試合はロッテのホームゲームで行われた","x":-2.8593874,"y":7.64022,"p":0,"cluster_ids":["0","1_3","2_29"]},{"arg_id":"Acsv-174_0","argument":"クレバーがセンターフライで連続三振を止めた時、オリファンは「実質勝ち」と言っていた","x":-3.047619,"y":10.640802,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-175_0","argument":"オリックスは貧打だが、吉田正尚は例外であり、彼を完全に抑えるのは素晴らしい","x":-3.4632592,"y":7.4246693,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-176_0","argument":"メジャーに行こうという意気込みを表現している","x":-1.1518211,"y":10.377459,"p":0,"cluster_ids":["0","1_4","2_10"]},{"arg_id":"Acsv-177_0","argument":"佐々木希さんにお祝いのメッセージを送るべき","x":-5.8839526,"y":9.419287,"p":0,"cluster_ids":["0","1_1","2_4"]},{"arg_id":"Acsv-178_0","argument":"13連続奪三振を経て完全試合を達成したことは素晴らしい","x":-3.592651,"y":12.264585,"p":0,"cluster_ids":["0","1_2","2_28"]},{"arg_id":"Acsv-179_0","argument":"おめでとう。素晴らしい記録です。","x":-0.26093444,"y":9.720554,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-180_0","argument":"令和版の怪物はエグい","x":-2.0559723,"y":11.651657,"p":0,"cluster_ids":["0","1_4","2_0"]},{"arg_id":"Acsv-181_0","argument":"若者の人間離れについての懸念","x":-1.2721616,"y":7.8012323,"p":0,"cluster_ids":["0","1_3","2_30"]},{"arg_id":"Acsv-182_0","argument":"プロ野球ファンとして嬉しいしおめでたい。最高のピッチングだった！","x":-1.9803259,"y":8.597372,"p":0,"cluster_ids":["0","1_6","2_3"]},{"arg_id":"Acsv-183_0","argument":"ロッテのじっくり育成が成功し、素晴らしいピッチャーが登場した","x":-3.0799742,"y":8.674433,"p":0,"cluster_ids":["0","1_3","2_1"]},{"arg_id":"Acsv-183_1","argument":"S-PARKで全アウトを見たが、ピッチャーの投球がえげつなかった","x":-2.737622,"y":8.939178,"p":0,"cluster_ids":["0","1_6","2_32"]},{"arg_id":"Acsv-183_2","argument":"2年連続首位打者の吉田がいるにもかかわらず、みんな三振していた","x":-2.9343762,"y":11.080002,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-184_0","argument":"高卒ドライチ3年目と1年目のバッテリーによる13連続奪三振と19奪三振での完全試合は、贔屓チームにいたら興奮しすぎて夜しか眠れなくなる","x":-3.2560353,"y":11.866037,"p":0,"cluster_ids":["0","1_2","2_2"]},{"arg_id":"Acsv-185_0","argument":"佐々木投手が64年ぶりに連続奪三振のプロ野球記録を更新した","x":-4.706656,"y":11.838969,"p":0,"cluster_ids":["0","1_2","2_33"]},{"arg_id":"Acsv-186_0","argument":"大谷翔平のような存在になるかどうかを考える","x":-3.4390905,"y":9.562401,"p":0,"cluster_ids":["0","1_6","2_11"]},{"arg_id":"Acsv-187_0","argument":"槇原以降出ていなかったことに驚き。奪三振19を取っているのに、9回投げて105球というのも素晴らしい","x":-3.5377924,"y":11.1832285,"p":0,"cluster_ids":["0","1_2","2_23"]},{"arg_id":"Acsv-189_0","argument":"三振を奪いながら球数が少ないのが彼の凄さ","x":-3.6276498,"y":10.466959,"p":0,"cluster_ids":["0","1_2","2_7"]},{"arg_id":"Acsv-189_1","argument":"オープン戦での制球の修正が素晴らしい","x":-2.4310696,"y":9.606102,"p":0,"cluster_ids":["0","1_6","2_27"]},{"arg_id":"Acsv-189_2","argument":"投げるたびに良くなっている","x":-3.9249098,"y":9.246465,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-190_0","argument":"クローザーのような投球で完全試合を達成したが、まだ素材の段階だと思う","x":-2.552642,"y":9.435341,"p":0,"cluster_ids":["0","1_6","2_32"]},{"arg_id":"Acsv-190_1","argument":"完投はプロで初めてであり、高校生だった捕手も評価すべき","x":-4.7184706,"y":9.045979,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-191_0","argument":"本人の努力と吉井コーチの手柄があると思う","x":-3.5322275,"y":8.413823,"p":0,"cluster_ids":["0","1_3","2_1"]},{"arg_id":"Acsv-191_1","argument":"相手のオリックスは昨年の優勝チームである","x":-3.4414039,"y":7.188127,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-192_0","argument":"佐々木希が誤爆でTwitterでトレンド入りした","x":-5.7832704,"y":9.460303,"p":0,"cluster_ids":["0","1_1","2_4"]},{"arg_id":"Acsv-193_0","argument":"高卒3年目の佐々木と高卒1年目の松川バッテリーで完全試合を達成。佐々木は松川を信じて投げたことや、松川のリードを称賛した。","x":-4.9821,"y":9.67737,"p":0,"cluster_ids":["0","1_1","2_25"]},{"arg_id":"Acsv-194_0","argument":"ピッチャーとキャッチャーの2人だけで2回分の試合を行うことが可能だったのか？","x":-3.1724932,"y":9.300582,"p":0,"cluster_ids":["0","1_6","2_11"]},{"arg_id":"Acsv-196_0","argument":"昨年の首位チームに対して19奪三振で完全試合を達成した。20歳にして平成～令和にかけての最高のピッチングである。","x":-3.7871358,"y":11.881278,"p":0,"cluster_ids":["0","1_2","2_31"]},{"arg_id":"Acsv-197_0","argument":"パ・リーグTVで見た投球はストレート160kmオーバー、フォーク140kmオーバーで非常に鋭い。奪三振は記録タイの19、連続奪三振は9から13に更新し、完全試合を達成した。3年目でついに完成した感がある。","x":-3.7913373,"y":11.48776,"p":0,"cluster_ids":["0","1_2","2_23"]},{"arg_id":"Acsv-198_0","argument":"松川選手が高卒ルーキー出場7試合目で28年ぶりの大記録を達成したことは素晴らしい","x":-4.832388,"y":9.303853,"p":0,"cluster_ids":["0","1_1","2_12"]},{"arg_id":"Acsv-199_0","argument":"吉田正尚が3打席連続三振は驚きである","x":-2.8784413,"y":11.281789,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-199_1","argument":"松川のリードも普通にヤバい","x":-3.9067562,"y":8.870748,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-200_0","argument":"宇宙の法則が乱れることにワクワクしていたが、ここまでとは思わなかった","x":-0.9525626,"y":9.788395,"p":0,"cluster_ids":["0","1_5","2_5"]},{"arg_id":"Acsv-201_0","argument":"佐々木選手と高卒ルーキー松川選手の胆力が素晴らしい","x":-5.214693,"y":9.414341,"p":0,"cluster_ids":["0","1_1","2_4"]},{"arg_id":"Acsv-202_0","argument":"デグロムが日本に来たら105球投げるのかな。交代の目安は100球前後。","x":-3.6430006,"y":10.492135,"p":0,"cluster_ids":["0","1_2","2_7"]},{"arg_id":"Acsv-203_0","argument":"千葉にロッテマリーンズという球団があることを覚えて帰るべき","x":-2.8956687,"y":8.293027,"p":0,"cluster_ids":["0","1_3","2_1"]},{"arg_id":"Acsv-204_0","argument":"物凄いポテンシャルがあったが、早く素晴らしい記録がついてくるとは思っていなかった。おめでとう🎉","x":-0.26919445,"y":9.858119,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-205_0","argument":"今どきの野球では信じられない結果が出たことに驚き","x":-2.2338767,"y":8.346608,"p":0,"cluster_ids":["0","1_3","2_19"]},{"arg_id":"Acsv-206_0","argument":"完全なるんほーについての議論は必要","x":-0.6056428,"y":7.549502,"p":0,"cluster_ids":["0","1_3","2_13"]},{"arg_id":"Acsv-207_0","argument":"19奪三振の日本タイ記録と13連続奪三振の日本新記録を達成した完全試合は、他の記録が霞むほどのヤバさがある。","x":-3.5628898,"y":12.335346,"p":0,"cluster_ids":["0","1_2","2_28"]},{"arg_id":"Acsv-208_0","argument":"ロッテ×オリックスの試合が地上波やBS/CSで中継されていないのはもったいない","x":-2.7999544,"y":7.4914117,"p":0,"cluster_ids":["0","1_3","2_29"]},{"arg_id":"Acsv-208_1","argument":"今夜のプロ野球ニュースは録画しておくべき","x":-2.1777875,"y":7.8524785,"p":0,"cluster_ids":["0","1_3","2_19"]},{"arg_id":"Acsv-209_0","argument":"去年より体重が5キロ増えたことを筋骨が5キロ増えたと表現すべき","x":-0.60837525,"y":9.876564,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-210_0","argument":"13連続三振を含むトータル19三振を記録した投手が164km/hの速球を投げた。これは平成6年に巨人の槙原寛己投手が達成して以来、28年ぶりの快挙であり、16人目の記録である。","x":-4.5290813,"y":11.5381975,"p":0,"cluster_ids":["0","1_2","2_33"]},{"arg_id":"Acsv-211_0","argument":"更新のしかたが問題である。9から一気に13にアップデートされた","x":-3.8467503,"y":12.085991,"p":0,"cluster_ids":["0","1_2","2_31"]}],"clusters":[{"level":0,"id":"0","label":"全体","takeaway":"","value":250,"parent":"","density_rank_percentile":0},{"level":1,"id":"1_1","label":"若手選手の成長と未来への期待","takeaway":"このクラスタは、高校野球における若手選手の成長と将来に対する期待を集約しています。特に、松川選手と佐々木選手の活躍が際立っており、彼らのパフォーマンスや監督の育成方針が選手の健康と将来にどのように寄与しているかが強調されています。また、若手選手の成功が地域の野球のレベルを示すものであり、今後の成長に対する希望が多くの意見に表れています。","value":43,"parent":"0","density_rank_percentile":0.6666666666666666},{"level":1,"id":"1_3","label":"岩手県のスポーツ文化と選手育成の期待","takeaway":"このクラスタは、岩手県における特異な文化や人材の素晴らしさ、特に野球選手の育成と活躍に対する期待を集約しています。少子化の影響を受ける中で、岩手出身の選手がMLBに挑戦するなど、地域のスポーツ界における注目度が高まっており、また千葉ロッテマリーンズの選手育成の成功や、オリックスの試合に対する分析も含まれています。地域の誇りや未来への希望が感じられ、スポーツを通じた感動や成長の物語が描かれています。","value":56,"parent":"0","density_rank_percentile":1},{"level":1,"id":"1_6","label":"野球の完全試合と選手の偉業に対する称賛","takeaway":"このクラスタは、野球における完全試合やノーヒットノーランといった特別な記録に対する称賛の声を集約しています。選手の優れた投球内容やチーム全体の努力、さらには過去の偉大な投手との比較や、試合のドラマを共有する喜びが表現されています。また、AI技術との関連性についても言及されており、スポーツと技術の融合に対する前向きな視点が感じられます。","value":45,"parent":"0","density_rank_percentile":0.3333333333333333},{"level":1,"id":"1_2","label":"日本プロ野球における投手の記録とその影響","takeaway":"このクラスタは、日本プロ野球における投手の記録更新やその影響に関する意見を集約しています。特に、奪三振や完全試合に関する記録の重要性、若手投手の圧倒的なパフォーマンス、過去の偉業との比較が強調されており、選手たちの成長や将来の可能性に対する期待が表れています。また、記録達成の難易度やその影響についての議論も含まれ、野球ファンの関心が高まっています。","value":63,"parent":"0","density_rank_percentile":0.8333333333333334},{"level":1,"id":"1_4","label":"若手選手の成長とメジャーリーグへの期待","takeaway":"このクラスタは、若手選手の成長過程やメジャーリーグでの活躍に対する期待を集約しています。特に、選手の投球速度や能力に関する意見が多く、令和の怪物と称される選手の影響力や、メジャー昇格への期待が強調されています。また、選手の育成や出場機会に対する懸念も含まれており、ファンは彼らの成長を見守りつつ、将来の成功を願っています。","value":19,"parent":"0","density_rank_percentile":0.5},{"level":1,"id":"1_5","label":"記録達成への称賛と選手の成長への期待","takeaway":"このクラスタは、特異な記録の達成に対する驚きと称賛の声、選手の成長に対する期待、そして故障への懸念を集約しています。特に高卒の若手選手が成し遂げた偉業に対する祝福の意が強く、身体的な特徴や努力の過程に触れつつ、選手の健康と成長を願う前向きな意見が多く見られます。記録達成の凄さを称賛しつつ、選手が自然体で成長できることを願う声が反映されています。","value":24,"parent":"0","density_rank_percentile":0.1666666666666666},{"level":2,"id":"2_8","label":"選手の将来を考えた監督の英断と育成の重要性","takeaway":"このクラスタには、高校野球における選手の健康と将来を重視した監督の判断に関する意見が集まっています。特に、大船渡高校の監督が選手を無理に投げさせなかったことが、選手の今後の活躍に繋がったという評価が強調されています。また、過保護と批判される中でも、選手の肩を守ることが重要であるという視点が示されており、選手育成における適切な判断の重要性が語られています。これに対し、他校の選手の連投問題も取り上げられ、選手の健康を守ることがいかに大切かを再認識させる内容となっています。","value":9,"parent":"1_1","density_rank_percentile":0.5},{"level":2,"id":"2_6","label":"岩手県の特異性とスポーツ界の期待","takeaway":"このクラスタには、岩手県に関連する特異な文化や人材の素晴らしさ、特に野球選手の活躍に対する期待が集まっています。少子化の影響を受ける中で、岩手出身の選手がMLBに挑戦するなど、地域のスポーツ界における注目度が高まっていることが示されています。また、岩手の特別なDNAや優れた才能の存在についての言及があり、地域の誇りや未来への希望が感じられます。","value":7,"parent":"1_3","density_rank_percentile":0.3333333333333333},{"level":2,"id":"2_17","label":"完全試合と野球の記録に対する称賛","takeaway":"このクラスタには、野球における完全試合やノーヒットノーランといった特別な記録に対する称賛の声が集まっています。特に、野田選手の名が残ることや、速いフォークとストレートの投球スタイルに対する興味が示されています。また、マリーンズの編成や育成の成果についても言及されており、チーム全体の努力が評価されています。これらの意見は、野球の魅力や選手の成長、チームの戦略に対する深い理解と期待を反映しています。","value":9,"parent":"1_6","density_rank_percentile":0.8888888888888888},{"level":2,"id":"2_31","label":"奪三振記録の更新とその影響","takeaway":"このクラスタには、奪三振に関する記録更新やその影響についての意見が集まっています。特に、9から13への一気のアップデートや、13連続奪三振、19奪三振という異次元の記録が強調されており、これらの成果が過去の記録を更新したことに対する驚きや称賛が表現されています。また、完全試合を達成した選手の偉業や、若い選手による歴史的なピッチングが、NPBの記録において重要な意味を持つことが示されています。","value":7,"parent":"1_2","density_rank_percentile":0.2222222222222222},{"level":2,"id":"2_19","label":"野球に対する驚きと期待","takeaway":"このクラスタには、野球に対する興味や驚き、特に大谷選手の活躍に対する感動が集まっています。野球について詳しくない人々が、スポーツとしての魅力や選手の素晴らしさを感じ、記録やニュースに対する関心を示しています。また、野球の現状や選手のパフォーマンスに対する疑問や皮肉も含まれており、全体として野球の多様な側面に対する感情が表れています。","value":10,"parent":"1_3","density_rank_percentile":0.6111111111111112},{"level":2,"id":"2_15","label":"オリックスの試合に関する疑問と分析","takeaway":"このクラスタには、オリックスの試合に関する疑問や分析が集まっています。特に、コロナ感染者の影響で試合が中止になった横浜と異なり、オリックスが試合を続行した理由についての疑問が表れています。また、オリックスのピッチャーや打線の状態、特定の選手のパフォーマンスに関する意見も含まれており、相手チームの投手の実力やコロナの影響を受けた選手の状況についても言及されています。全体として、オリックスの試合に対する関心と分析が強く表れています。","value":8,"parent":"1_3","density_rank_percentile":0.3055555555555556},{"level":2,"id":"2_1","label":"千葉ロッテマリーンズの育成成功と未来への期待","takeaway":"このクラスタには、千葉ロッテマリーンズの選手育成に関するポジティブな意見が集まっています。90年代の人材不足から脱却し、じっくりと選手を育てる姿勢が評価されており、特にピッチャーや捕手の成長が注目されています。高校野球での投球内容やコーチの指導が成功に寄与していることが強調され、ロッテの未来に対する明るい展望が示されています。","value":9,"parent":"1_3","density_rank_percentile":0.9166666666666666},{"level":2,"id":"2_29","label":"ロッテに関連するスポーツニュースの重要性","takeaway":"このクラスタには、ロッテに関連する試合やニュースに対する関心が集まっています。特に、パリーグTVやソフトバンクのユーザー特典を通じての視聴機会に言及されており、ロッテのホームゲームが地上波やBS/CSで中継されないことへの残念な思いが表現されています。これにより、ロッテの試合がより多くのファンに届けられるべきであるという期待が示されています。","value":7,"parent":"1_3","density_rank_percentile":0.5555555555555556},{"level":2,"id":"2_9","label":"佐々木投手の記録的なパフォーマンスとその影響","takeaway":"このクラスタには、佐々木投手が達成した13者連続奪三振や完全試合、1試合19奪三振といった記録に関する意見が集まっています。これらの記録は、プロ野球史において非常に重要なものであり、64年ぶりや28年ぶりといった長い期間を経て更新されたことから、彼のパフォーマンスがいかに特異であるかを示しています。また、メジャーリーグへの移籍を期待する声もあり、彼の才能が今後どのように発展していくのかに対する関心が高まっています。","value":7,"parent":"1_2","density_rank_percentile":0.7222222222222222},{"level":2,"id":"2_27","label":"野球における完全試合の記録とその影響","takeaway":"このクラスタには、野球の完全試合に関する様々な意見や感想が集まっています。特に、槙原氏の完全試合や三重殺、トリプルプレイなど、試合の記録や出来事に対する驚きや感動が表現されています。また、過去の試合を振り返りながら、観戦したことの幸運や、次の完全試合への期待感が感じられます。これらの意見は、野球ファンにとっての記録の重要性や、試合のドラマを共有する喜びを示しています。","value":10,"parent":"1_6","density_rank_percentile":0.9444444444444444},{"level":2,"id":"2_11","label":"プロ野球におけるピッチングの偉業と選手の比較","takeaway":"このクラスタには、プロ野球におけるピッチングの偉業や選手の比較に関する意見が集まっています。特に、過去の偉大な投手や現在のスター選手である大谷翔平との比較が行われており、今後のピッチングの可能性や選手の成長に対する期待が表れています。また、完全試合に対する考え方や、ピッチャーとキャッチャーの役割についての疑問も含まれており、野球に対する深い愛情と分析が感じられます。","value":7,"parent":"1_6","density_rank_percentile":0.8611111111111112},{"level":2,"id":"2_14","label":"吉田正尚の三振と投手の圧倒的なパフォーマンス","takeaway":"このクラスタには、吉田正尚選手の三振に関する驚きや、彼が直面した投手の素晴らしい投球に対する評価が集まっています。特に、吉田選手が過去に比べて三振を多く記録したことや、彼の打席が試合の勝敗に影響を与えたという意見が強調されています。また、他の選手も三振を喫している中で、吉田選手のパフォーマンスが際立っていることが示されています。全体として、投手の力強さと吉田選手の意外な結果に対する驚きが共通のテーマとなっています。","value":8,"parent":"1_2","density_rank_percentile":0.3888888888888889},{"level":2,"id":"2_10","label":"メジャーリーグへの期待と選手の成長","takeaway":"このクラスタには、選手がメジャーリーグで活躍することへの期待や、成長過程に関する意見が集まっています。特に、選手が持つ能力や将来の可能性に対する期待が強く、早期のメジャー昇格を望む声が多く見られます。また、選手の出場機会が限られていることに対する懸念も含まれており、今後の成長と活躍を見守る姿勢が表れています。","value":8,"parent":"1_4","density_rank_percentile":0.4722222222222222},{"level":2,"id":"2_3","label":"プロ野球とAI技術に対する期待と感謝","takeaway":"このクラスタには、プロ野球の試合や選手のパフォーマンスに対する感動や期待、さらにはAI技術に対する評価が集まっています。ファンとしての喜びや選手の健康を願う気持ちが表現されており、特にピッチャーの投球に対する称賛や、彼のキャリアが長く続くことへの願望が強調されています。また、AI技術をモビルスーツに例えることで、その重要性やパフォーマンスの変化に対する理解が示されています。全体として、スポーツと技術の融合に対する前向きな視点が感じられます。","value":11,"parent":"1_6","density_rank_percentile":1},{"level":2,"id":"2_30","label":"若者の成長と現実離れに対する感慨","takeaway":"このクラスタには、若者の成長や震災を乗り越えた経験に対する感謝と感動の意見が集まっています。一方で、若者の現実離れや人間離れに対する懸念も表明されており、若者たちが直面する社会的な課題や変化に対する複雑な感情が反映されています。特に震災を経験した地域の若者たちの成長に感動しつつも、彼らが抱える現実とのギャップについて考える声が見受けられます。","value":5,"parent":"1_3","density_rank_percentile":0.6944444444444444},{"level":2,"id":"2_16","label":"槇原の28年の変遷と過去の選択への再評価","takeaway":"このクラスタには、槇原に関連する28年前の出来事やその影響についての考察が集まっています。槇原が達成したことを振り返ることで、時間の経過と共に感じる変化や成長、そして過去の選択に対する再評価が表現されています。特に、当時の合理的な判断が今の視点からどう見えるかという内面的な葛藤が強調されており、時間の流れと共に変わる価値観や感情が反映されています。","value":4,"parent":"1_2","density_rank_percentile":0.0555555555555555},{"level":2,"id":"2_7","label":"投球数と三振の効率に関する議論","takeaway":"このクラスタには、投球数と三振数の関係、特に球数が少ない中での三振奪取の効率性についての意見が集まっています。参加者は、特定の選手のパフォーマンスに感動しつつも、球数やファール球の影響、さらには遊び球の概念についても言及しています。選手の記録や怪我の少なさを願う声もあり、今後の選手たちの活躍に期待を寄せています。","value":9,"parent":"1_2","density_rank_percentile":0.7777777777777778},{"level":2,"id":"2_22","label":"現実とエンターテインメントの交差点における感動","takeaway":"このクラスタには、現実の出来事やエンターテインメントに対する感動や興奮が集まっています。特に、漫画や映画、番組に対する感想や期待感が強調されており、観ることの喜びや感嘆の表現が見受けられます。また、日本での活動に対する物足りなさも感じられますが、それを補うようにエンターテインメントが心を豊かにしている様子が伺えます。","value":6,"parent":"1_3","density_rank_percentile":0.8333333333333334},{"level":2,"id":"2_20","label":"驚異的な記録達成とその称賛","takeaway":"このクラスタには、特異な記録の達成に対する驚きと称賛の声が集まっています。記録が生まれたこと自体が伝説の始まりであり、特にマンガを超えるような偉業に対して祝福の意が表されています。身体的な特徴や努力の過程に触れつつ、記録達成の凄さを強調する意見が多く、特に高卒の若手選手が成し遂げた偉業に対する期待と称賛が感じられます。","value":10,"parent":"1_5","density_rank_percentile":0.6388888888888888},{"level":2,"id":"2_18","label":"称賛と感嘆の表現","takeaway":"このグループには、何かに対する強い称賛や感嘆の気持ちを表す言葉が集まっています。言葉の選び方から、対象に対する高い評価や感動が伝わってきます。特に「すごい」や「素晴らしい」といった表現が繰り返されており、対象の素晴らしさや価値を強調する意図が感じられます。","value":8,"parent":"1_5","density_rank_percentile":0.4444444444444444},{"level":2,"id":"2_12","label":"高卒ルーキー捕手松川の驚異的な活躍","takeaway":"このクラスタには、高卒1年目の捕手松川選手の素晴らしいパフォーマンスに関する意見が集まっています。完全試合の達成や、打撃での活躍、さらにはプロ初完投や完封といった記録的な成果が称賛されています。松川選手の若さと才能が際立っており、彼の成長と将来への期待が強く表現されています。また、彼の貫禄やヒーローインタビューでの姿勢についても言及されており、松川選手が特別な存在であることが強調されています。","value":12,"parent":"1_1","density_rank_percentile":0.5833333333333334},{"level":2,"id":"2_28","label":"完全試合と奪三振記録への挑戦","takeaway":"このクラスタには、完全試合や奪三振に関する記録達成の重要性やその難易度についての意見が集まっています。特に、27奪三振を目指すという目標や、13連続奪三振の達成が完全試合よりも難しいという見解が示されており、これらの記録が持つ特別な価値や挑戦の意義が強調されています。選手の偉業に対する称賛と、さらなる記録への期待が感じられます。","value":5,"parent":"1_2","density_rank_percentile":0.1111111111111111},{"level":2,"id":"2_24","label":"若手選手とキャッチャー松川への期待と評価","takeaway":"このクラスタには、若手選手やキャッチャー松川に対する期待や評価が集まっています。特に、松川選手の成長やリードに対する称賛が目立ち、彼がチームに与える影響についての意見が多く見受けられます。また、過去の偉大な選手たちと比較しつつも、新たな才能の登場に対する期待感が表現されています。キャッチャーの若さや物語性が強調され、彼の存在がチームにとって重要であることが示されています。","value":8,"parent":"1_1","density_rank_percentile":0.9722222222222222},{"level":2,"id":"2_34","label":"令和の怪物とその影響力","takeaway":"このクラスタには、令和の怪物と称される選手に関する話題が集まっています。特に、彼の奪三振記録や完全試合の達成、さらには育成過程やチームへの影響についての言及が見られます。選手の優れた能力や活躍が、ファンやメディアの注目を集め、今後の成長やチームの成功に対する期待感を高めています。","value":4,"parent":"1_4","density_rank_percentile":0.8055555555555556},{"level":2,"id":"2_32","label":"完全試合の達成とその影響","takeaway":"このクラスタには、完全試合の達成に関するさまざまな視点が集まっています。投手の優れた投球内容やクローザーの役割、守備陣の緊張感、さらには試合中の野手の状況についての考察が含まれています。完全試合はエラーが許されないため、守備陣も緊張感を持ってプレーする必要があり、試合の流れや選手のパフォーマンスがどのように影響し合うかについての意見が交わされています。","value":8,"parent":"1_6","density_rank_percentile":0.75},{"level":2,"id":"2_2","label":"令和の怪物による圧巻の投球パフォーマンス","takeaway":"このクラスタには、高卒ドライチ選手の驚異的な投球成績や記録に対する興奮と期待が集まっています。特に、13連続奪三振や19奪三振での完全試合といった圧倒的なパフォーマンスは、ファンにとって特別な感動をもたらし、贔屓チームへの愛情をさらに深める要因となっています。また、ストレートの速さや他の選手からの三振奪取など、今後の成長や活躍に対する期待感も強く表れています。","value":4,"parent":"1_2","density_rank_percentile":0.1388888888888889},{"level":2,"id":"2_23","label":"野球における投手の圧倒的なパフォーマンスとその影響","takeaway":"このクラスタには、投手の奪三振数や完投、さらには試合の緊張感に関する意見が集まっています。特に、19奪三振という記録や9回105球という投球数は、投手の圧倒的なパフォーマンスを示しており、野手の緊張感や試合の流れに対する考察も含まれています。また、過去の名投手との比較や、漫画での描写に対する意見も見られ、野球の魅力や投手の成績がどのように評価されるかについての関心が伺えます。","value":7,"parent":"1_2","density_rank_percentile":0.1666666666666666},{"level":2,"id":"2_5","label":"選手の成長と故障への懸念","takeaway":"このクラスタには、選手の成長やパフォーマンスに対する期待と、故障に対する懸念が表現されています。特に、西純選手に対しては焦らず自然体での成長を願う声があり、体作りや耐久力の重要性が強調されています。また、選手の努力やコメントに対する評価も見られ、選手がプロとしてのピークを迎えられたことへの安堵感が共有されています。全体として、選手の健康と成長を願う前向きな意見が集まっています。","value":6,"parent":"1_5","density_rank_percentile":0.2777777777777778},{"level":2,"id":"2_4","label":"佐々木選手と佐々木希に関する注目と祝福","takeaway":"このクラスタには、佐々木朗希選手と佐々木希さんに関する意見が集まっています。佐々木朗希選手の成長や活躍に対する驚きや祝福の声が多く、特に彼の才能が岩手県の野球のレベルを示すものであることが強調されています。また、佐々木希さんに対する称賛や、彼女と他の有名人との混同についての言及も見られ、両者の影響力や人気が伺えます。全体として、佐々木姓を持つ二人の人物に対する関心と祝福が一つのグループとして形成されています。","value":10,"parent":"1_1","density_rank_percentile":0.5277777777777778},{"level":2,"id":"2_0","label":"令和の怪物としての狼鬼の食玩","takeaway":"このクラスタには、狼鬼というキャラクターが令和の時代において怪物としての存在感を持ち、食玩としての発売が期待されていることに関する意見が集まっています。参加者は、狼鬼が持つ独特の魅力やエグさを強調し、現代の文化やトレンドにおける怪物の位置づけについて考察しています。食玩としての展開は、キャラクターの人気をさらに高める要素として捉えられています。","value":4,"parent":"1_4","density_rank_percentile":0.0833333333333333},{"level":2,"id":"2_25","label":"若き才能が生み出した伝説的な試合","takeaway":"このクラスタには、佐々木投手と松川捕手の若いバッテリーが成し遂げた完全試合に関する称賛の声が集まっています。特に、彼らの年齢や経験に対する期待感、そして今後の成長に対する希望が強調されています。佐々木投手の素晴らしいパフォーマンスや、松川捕手との信頼関係が、今後のさらなる成功につながることを願う意見が多く見られます。","value":4,"parent":"1_1","density_rank_percentile":0.6666666666666666},{"level":2,"id":"2_21","label":"若手投手の成長と将来の可能性","takeaway":"このクラスタには、若手投手の年齢やキャリアに基づく成長の期待が集まっています。特に、バッテリーの年齢やプロ3年目の選手のパフォーマンスに注目が集まり、将来的な完全試合の達成可能性についての前向きな意見が見受けられます。また、大谷選手に匹敵する才能の出現に対する驚きと期待が表現されており、若手選手の成長が今後のプロ野球に与える影響についての関心が高まっています。","value":4,"parent":"1_2","density_rank_percentile":0.4166666666666667},{"level":2,"id":"2_26","label":"日本の野球における完全試合の歴史と選手の功績","takeaway":"このクラスタには、日本の野球における完全試合の達成者やその記録に関する情報が集まっています。近藤真一投手の初登板以降の成績に対する言及と、藤本英雄、槙原寛己、佐々木朗希といった選手たちが背番号17を持ちながら達成した完全試合の歴史的意義が強調されています。また、玉山鉄二の主演映画に関する情報も含まれており、野球とエンターテインメントの交差点における日本の文化的な側面が示されています。","value":3,"parent":"1_2","density_rank_percentile":0.1944444444444444},{"level":2,"id":"2_35","label":"若手選手の成長と将来の投球速度への期待","takeaway":"このクラスタには、若手選手の投球速度に関する期待や予測が集まっています。特に、20歳で160kmを投げる選手の成長に注目し、将来的にはさらに速い投球が可能になるという前向きな見解が示されています。また、来年には167kmを投げる選手が現れる可能性についても言及されており、選手の成長過程やシーズン中のパフォーマンスに対する期待が感じられます。","value":3,"parent":"1_4","density_rank_percentile":0.25},{"level":2,"id":"2_13","label":"多様な議論の必要性","takeaway":"このクラスタには、さまざまなテーマに関する議論の重要性が集まっています。完全なるんほー、歴史、マイライフスレ、チートといった異なるトピックが含まれており、それぞれのテーマに対して深い考察や意見交換が求められています。参加者は、これらの議論を通じて知識を深めたり、新たな視点を得たりすることを目指しており、各テーマの理解を深めることが重要であると認識しています。","value":4,"parent":"1_3","density_rank_percentile":0.0277777777777777},{"level":2,"id":"2_33","label":"日本プロ野球における歴史的な投手の快挙","takeaway":"このクラスタには、日本プロ野球における投手の記録や快挙に関する情報が集まっています。特に、槙原寛己投手や佐々木朗希投手のような歴史的な投手が達成した記録に焦点を当てており、28年ぶりの快挙や連続三振の記録更新など、過去の偉業と現在の成果が交差しています。これにより、投手の技術や成績がどのように進化しているか、またその影響がどれほど大きいかを理解することができます。","value":5,"parent":"1_2","density_rank_percentile":0.3611111111111111}],"comments":{},"propertyMap":{},"translations":{},"overview":"各クラスタは、若手選手の成長や地域のスポーツ文化、特別な記録に対する称賛を中心に構成されています。若手選手の活躍が地域の誇りや未来への希望を象徴し、特に岩手県やメジャーリーグへの期待が強調されています。また、投手の記録更新や選手の健康に対する懸念も見られ、ファンは選手の成長を見守りつつ、成功を願っています。全体として、選手の成長と記録達成に対する期待が共通のテーマとなっています。","config":{"name":"4d387cf0-fe6a-4cb8-ba37-86f48f96315b","input":"4d387cf0-fe6a-4cb8-ba37-86f48f96315b","question":"人々はロッテ佐々木朗希選手の完全試合達成にどのように反応したか","intro":"このAIが作成したレポートは、はてなブックマークのブックマークコメントのデータに依拠している。\n分析対象となったデータの件数は211件で、これらのデータに対してOpenAI APIを用いて250件の意見（議論）を抽出し、クラスタリングを行った。\n","model":"gpt-4o-mini","is_pubcom":true,"extraction":{"prompt":"/system\nあなたは専門的なリサーチアシスタントで、整理された議論のデータセットを作成するお手伝いをする役割です。\n人工知能に関する公開協議を実施した状況を想定しています。一般市民から寄せられた議論の例を提示しますので、それらをより簡潔で読みやすい形に整理するお手伝いをお願いします。必要な場合は2つの別個の議論に分割することもできますが、多くの場合は1つの議論にまとめる方が望ましいでしょう。\n結果は整形されたJSON形式の文字列リストとして返してください。\n要約は必ず日本語で作成してください。\n\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n[\n\"AIテクノロジーの環境負荷削減に焦点を当てるべき\"\n]\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、一般市民を教育する協調的な取り組みが必要です。\n\n/ai\n\n[\n\"AIの能力について一般市民を教育すべき\",\n\"AIの限界と倫理的考慮事項について一般市民を教育すべき\"\n]\n\n/human\n\nAIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できます。\n\n/ai\n\n[\n\"AIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できる\"\n]\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n[\n\"AIはエネルギーグリッドを最適化して無駄と炭素排出を削減できる\"\n]\n","workers":30,"limit":211,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$17","model":"gpt-4o-mini"},"hierarchical_clustering":{"cluster_nums":[6,36],"source_code":"$18"},"hierarchical_initial_labelling":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説して、それから表札をつけてください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n# サンプルの入出力\n## 入力例\n最近、AIアシスタントが医療診断サポートの実用化に向けた具体的な成果を発表し、医療現場の未来に明るい希望が見えてきました。患者として、この技術進歩に感謝しています。\nAI医療支援システムが、学会でもしっかり議論されるようになり、開発者が医療従事者の本当のニーズに応える姿勢に期待しています。\n技術カンファレンスを通じて、AIエンジニアが医療分野に全力で取り組む姿勢が伝わってきます。具体的な応用事例を目にするたび、医療の未来への希望が膨らみます。\n\n## 出力例\n{{\n   \"label\": \"医療現場を支えるAI技術の具体的応用への期待\",\n   \"description\": \"このクラスタには、医療診断や患者支援など、実際の医療課題に対してAI技術が具体的かつ効果的に貢献する可能性を支持する前向きな意見が集まっています。利用者は、学会や技術カンファレンスを通じて、現場のニーズに即したAIソリューションや支援システムが実現されることを期待し、医療の未来の発展に向けた技術革新を応援しています。\"\n}}\n","sampling_num":30,"workers":30,"source_code":"$19","model":"gpt-4o-mini"},"hierarchical_merge_labelling":{"prompt":"分割されすぎたクラスタを統合する必要があるので、統合後の名称を考えて出力して。\n\n# 指示\n* 統合前のクラスタの名称・説明および統合後のクラスタに属するデータ点のサンプルを与えるので、これらに基づいて統合後のクラスタの名称を出力してください\n   * 統合後のクラスタ名において、統合前のクラスタ名をそのまま使うことは避けてください。\n* 出力例に記載したJSONのフォーマットに従って出力してください\n\n# サンプルの入出力\n## 入力例（クラスタラベル:説明文）\n- AI技術の環境影響に関する懸念: このクラスタは、AI技術の開発・運用に伴う環境負荷に対する批判的な意見を集約したものです。市民からは、大規模モデルの学習時のエネルギー消費や、データセンターの冷却システムによる環境負荷などに対する強い懸念が表明されています。\n- AI開発の持続可能性への疑問: このクラスタは、AI技術開発全般における持続可能性に対する疑問を示す意見をまとめたものです。エネルギー消費の増大や資源利用の効率性に疑問を持つ声が多く、より環境に配慮した技術開発アプローチを求める意見が特徴です。\n- AI運用の炭素排出量: このクラスタは、AIシステムの運用時における炭素排出量が予想以上に大きい点に対する懸念や不満を反映しています。クラウドサービスの利用拡大やモデル推論時のエネルギー消費、そしてそれに伴う環境への長期的影響が強調されています。\n## 出力例\n{{\n   \"label\": \"AI技術のエネルギー効率と環境持続可能性\",\n   \"description\": \"このクラスタは、AI技術の開発・運用における環境への影響と持続可能性に対する懸念を集約しています。市民は、技術革新の議論の中で、エネルギー効率と炭素排出削減を最優先すべきだとの期待と、現行のAI開発手法に対する環境面での改善要求を強く表明しており、より持続可能で環境に配慮したAI技術の発展を求める声が反映されています。\"\n}}\n","sampling_num":30,"workers":30,"source_code":"$1a","model":"gpt-4o-mini"},"hierarchical_overview":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$1b","model":"gpt-4o-mini"},"hierarchical_aggregation":{"sampling_num":30,"hidden_properties":{},"source_code":"$1c"},"output_dir":"4d387cf0-fe6a-4cb8-ba37-86f48f96315b","skip-interaction":true,"without-html":true,"embedding":{"model":"text-embedding-3-small","source_code":"import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)\n"},"hierarchical_visualization":{"replacements":[],"source_code":"import subprocess\n\n\ndef hierarchical_visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../report\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"not trace of previous run"},{"step":"embedding","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_clustering","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_initial_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_merge_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_overview","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_aggregation","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_visualization","run":false,"reason":"skipping html output"}],"status":"running","start_time":"2025-04-10T11:05:55.113944","completed_jobs":[{"step":"extraction","completed":"2025-04-10T11:06:32.135269","duration":37.020753,"params":{"prompt":"/system\nあなたは専門的なリサーチアシスタントで、整理された議論のデータセットを作成するお手伝いをする役割です。\n人工知能に関する公開協議を実施した状況を想定しています。一般市民から寄せられた議論の例を提示しますので、それらをより簡潔で読みやすい形に整理するお手伝いをお願いします。必要な場合は2つの別個の議論に分割することもできますが、多くの場合は1つの議論にまとめる方が望ましいでしょう。\n結果は整形されたJSON形式の文字列リストとして返してください。\n要約は必ず日本語で作成してください。\n\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n[\n\"AIテクノロジーの環境負荷削減に焦点を当てるべき\"\n]\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、一般市民を教育する協調的な取り組みが必要です。\n\n/ai\n\n[\n\"AIの能力について一般市民を教育すべき\",\n\"AIの限界と倫理的考慮事項について一般市民を教育すべき\"\n]\n\n/human\n\nAIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できます。\n\n/ai\n\n[\n\"AIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できる\"\n]\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n[\n\"AIはエネルギーグリッドを最適化して無駄と炭素排出を削減できる\"\n]\n","workers":30,"limit":211,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$1d","model":"gpt-4o-mini"}},{"step":"embedding","completed":"2025-04-10T11:06:43.045110","duration":10.908524,"params":{"model":"text-embedding-3-small","source_code":"import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)\n"}},{"step":"hierarchical_clustering","completed":"2025-04-10T11:06:51.128623","duration":8.082821,"params":{"cluster_nums":[6,36],"source_code":"$1e"}},{"step":"hierarchical_initial_labelling","completed":"2025-04-10T11:07:02.330068","duration":11.200766,"params":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説して、それから表札をつけてください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n# サンプルの入出力\n## 入力例\n最近、AIアシスタントが医療診断サポートの実用化に向けた具体的な成果を発表し、医療現場の未来に明るい希望が見えてきました。患者として、この技術進歩に感謝しています。\nAI医療支援システムが、学会でもしっかり議論されるようになり、開発者が医療従事者の本当のニーズに応える姿勢に期待しています。\n技術カンファレンスを通じて、AIエンジニアが医療分野に全力で取り組む姿勢が伝わってきます。具体的な応用事例を目にするたび、医療の未来への希望が膨らみます。\n\n## 出力例\n{{\n   \"label\": \"医療現場を支えるAI技術の具体的応用への期待\",\n   \"description\": \"このクラスタには、医療診断や患者支援など、実際の医療課題に対してAI技術が具体的かつ効果的に貢献する可能性を支持する前向きな意見が集まっています。利用者は、学会や技術カンファレンスを通じて、現場のニーズに即したAIソリューションや支援システムが実現されることを期待し、医療の未来の発展に向けた技術革新を応援しています。\"\n}}\n","sampling_num":30,"workers":30,"source_code":"$1f","model":"gpt-4o-mini"}},{"step":"hierarchical_merge_labelling","completed":"2025-04-10T11:07:07.232465","duration":4.901582,"params":{"prompt":"分割されすぎたクラスタを統合する必要があるので、統合後の名称を考えて出力して。\n\n# 指示\n* 統合前のクラスタの名称・説明および統合後のクラスタに属するデータ点のサンプルを与えるので、これらに基づいて統合後のクラスタの名称を出力してください\n   * 統合後のクラスタ名において、統合前のクラスタ名をそのまま使うことは避けてください。\n* 出力例に記載したJSONのフォーマットに従って出力してください\n\n# サンプルの入出力\n## 入力例（クラスタラベル:説明文）\n- AI技術の環境影響に関する懸念: このクラスタは、AI技術の開発・運用に伴う環境負荷に対する批判的な意見を集約したものです。市民からは、大規模モデルの学習時のエネルギー消費や、データセンターの冷却システムによる環境負荷などに対する強い懸念が表明されています。\n- AI開発の持続可能性への疑問: このクラスタは、AI技術開発全般における持続可能性に対する疑問を示す意見をまとめたものです。エネルギー消費の増大や資源利用の効率性に疑問を持つ声が多く、より環境に配慮した技術開発アプローチを求める意見が特徴です。\n- AI運用の炭素排出量: このクラスタは、AIシステムの運用時における炭素排出量が予想以上に大きい点に対する懸念や不満を反映しています。クラウドサービスの利用拡大やモデル推論時のエネルギー消費、そしてそれに伴う環境への長期的影響が強調されています。\n## 出力例\n{{\n   \"label\": \"AI技術のエネルギー効率と環境持続可能性\",\n   \"description\": \"このクラスタは、AI技術の開発・運用における環境への影響と持続可能性に対する懸念を集約しています。市民は、技術革新の議論の中で、エネルギー効率と炭素排出削減を最優先すべきだとの期待と、現行のAI開発手法に対する環境面での改善要求を強く表明しており、より持続可能で環境に配慮したAI技術の発展を求める声が反映されています。\"\n}}\n","sampling_num":30,"workers":30,"source_code":"$20","model":"gpt-4o-mini"}},{"step":"hierarchical_overview","completed":"2025-04-10T11:07:10.189417","duration":2.955229,"params":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$21","model":"gpt-4o-mini"}}],"lock_until":"2025-04-10T11:12:10.193452","current_job":"hierarchical_aggregation","current_job_started":"2025-04-10T11:07:10.193434","current_job_progress":null,"current_jop_tasks":null},"comment_num":211}}],["$","$L22",null,{"result":"$8:0:props:children:2:props:result"}],["$","$L12",null,{"w":"fit-content","mx":"auto","children":["$","$L6",null,{"href":"/","children":["$","$L7",null,{"variant":"outline","size":"md","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-left","children":[["$","path","1wnfg3",{"d":"m15 18-6-6 6-6"}],"$undefined"]}],"一覧へ戻る"]}]}]}],["$","$L23",null,{"my":12,"maxW":"750px","mx":"auto"}],["$","$L24",null,{"meta":"$8:0:props:children:0:props:meta"}]]}],["$","footer",null,{"children":["$","$L25",null,{"direction":{"base":"column","lg":"row"},"justify":"space-between","maxW":"800px","mx":"auto","children":[["$","$L26",null,{"gap":5,"justify":"center","align":"center","children":[["$","$L14",null,{"fontWeight":"bold","fontSize":"lg","children":"テスト環境"}],null,null]}],["$","$L26",null,{"justify":"center","children":["$","$L27",null,{"placement":"bottom","children":[["$","$L28",null,{}],["$","$L29",null,{"children":["$","$L14",null,{"className":"textLink","cursor":"pointer","children":"デジタル民主主義2030プロジェクトについて"}]}],["$","$L2a",null,{"disabled":false,"container":"$undefined","children":["$","$L2b",null,{"padding":"$undefined","children":["$","$L2c",null,{"ref":"$undefined","roundedTop":"md","p":5,"asChild":false,"children":[["$","$L2d",null,{"children":["$","$L2e",null,{"fontSize":"2xl","fontWeight":"bold","textAlign":"center","className":"gradientColor","children":"デジタル民主主義2030プロジェクト"}]}],["$","$L2f",null,{"textAlign":"center","children":[["$","$L12",null,{"mb":8,"maxW":"700px","mx":"auto","children":[["$","$L13",null,{"size":"lg","mb":2,"textAlign":"center","children":"プロジェクトについて"}],["$","$L14",null,{"children":"2030年には、情報技術により民主主義のあり方はアップデートされており、一人ひとりの声が政治・行政に届き、適切に合意形成・政策反映されていくような社会が当たり前になる──そんな未来を目指して立ち上げられたのがデジタル民主主義2030プロジェクトです。 AIやデジタル技術の進化により、これまで不可能だった新しい形の市民参加や政策運営が可能になるはずだという信念に基づいています。"}],["$","$L6",null,{"href":"https://dd2030.org","target":"_blank","rel":"noreferrer noopener","children":["$","$L26",null,{"justify":"center","mt":2,"children":[["$","$L14",null,{"className":"textLink","children":"プロジェクトについての詳細はこちら"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-external-link","children":[["$","path","1q9fwt",{"d":"M15 3h6v6"}],["$","path","gplh6r",{"d":"M10 14 21 3"}],["$","path","a6xqqp",{"d":"M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"}],"$undefined"]}]]}]}]]}],["$","$L12",null,{"mb":8,"maxW":"700px","mx":"auto","children":[["$","$L13",null,{"size":"lg","mb":2,"textAlign":"center","children":"免責"}],["$","$L14",null,{"mb":2,"children":"このレポート内容に関する質問や意見はレポート発行責任者へお問い合わせください。"}],["$","$L14",null,{"children":"大規模言語モデル（LLM）にはバイアスがあり、信頼性の低い結果を生成することが知られています。私たちはこれらの問題を軽減する方法に積極的に取り組んでいますが、現段階ではいかなる保証も提供することはできません。特に重要な決定を下す際は、本アプリの出力結果のみに依存せず、必ず内容を検証してください。"}]]}],["$","$L12",null,{"mb":8,"maxW":"700px","mx":"auto","children":[["$","$L13",null,{"size":"lg","mb":2,"textAlign":"center","children":"謝辞"}],["$","$L14",null,{"children":["このプロジェクトは"," ",["$","a",null,{"className":"textLink","href":"https://ai.objectives.institute/","target":"_blank","rel":"noreferrer","children":"AI Objectives Institute"}]," ","が開発した"," ",["$","a",null,{"className":"textLink","href":"https://github.com/AIObjectives/talk-to-the-city-reports","target":"_blank","rel":"noreferrer","children":"Talk to the City"}]," ","を参考に開発されています。",["$","br",null,{}],"ライセンスに基づいてソースコードを一部活用し、機能追加や改善を実施しています。"]}]]}],["$","$L30",null,{"children":["$","$L7",null,{"variant":"outline","children":"閉じる"}]}]]}]]}]}]}]]}]}]]}]}]]
b:null
f:[["$","title","0",{"children":"人々はロッテ佐々木朗希選手の完全試合達成にどのように反応したか - テスト環境"}],["$","meta","1",{"name":"description","content":"各クラスタは、若手選手の成長や地域のスポーツ文化、特別な記録に対する称賛を中心に構成されています。若手選手の活躍が地域の誇りや未来への希望を象徴し、特に岩手県やメジャーリーグへの期待が強調されています。また、投手の記録更新や選手の健康に対する懸念も見られ、ファンは選手の成長を見守りつつ、成功を願っています。全体として、選手の成長と記録達成に対する期待が共通のテーマとなっています。"}],["$","meta","2",{"property":"og:title","content":"人々はロッテ佐々木朗希選手の完全試合達成にどのように反応したか - テスト環境"}],["$","meta","3",{"property":"og:description","content":"各クラスタは、若手選手の成長や地域のスポーツ文化、特別な記録に対する称賛を中心に構成されています。若手選手の活躍が地域の誇りや未来への希望を象徴し、特に岩手県やメジャーリーグへの期待が強調されています。また、投手の記録更新や選手の健康に対する懸念も見られ、ファンは選手の成長を見守りつつ、成功を願っています。全体として、選手の成長と記録達成に対する期待が共通のテーマとなっています。"}],["$","meta","4",{"property":"og:image","content":"http://localhost:3000/4d387cf0-fe6a-4cb8-ba37-86f48f96315b/opengraph-image.png"}],["$","meta","5",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","6",{"name":"twitter:title","content":"人々はロッテ佐々木朗希選手の完全試合達成にどのように反応したか - テスト環境"}],["$","meta","7",{"name":"twitter:description","content":"各クラスタは、若手選手の成長や地域のスポーツ文化、特別な記録に対する称賛を中心に構成されています。若手選手の活躍が地域の誇りや未来への希望を象徴し、特に岩手県やメジャーリーグへの期待が強調されています。また、投手の記録更新や選手の健康に対する懸念も見られ、ファンは選手の成長を見守りつつ、成功を願っています。全体として、選手の成長と記録達成に対する期待が共通のテーマとなっています。"}],["$","meta","8",{"name":"twitter:image","content":"http://localhost:3000/4d387cf0-fe6a-4cb8-ba37-86f48f96315b/opengraph-image.png"}]]
