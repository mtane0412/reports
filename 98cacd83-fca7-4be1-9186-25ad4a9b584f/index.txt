1:"$Sreact.fragment"
2:I[67315,["785","static/chunks/785-bf8eaad2e6817186.js","451","static/chunks/451-ff68bb1f8be23e5c.js","98","static/chunks/98-ceb6bd4957f64cef.js","108","static/chunks/108-145d964849b57510.js","177","static/chunks/app/layout-1f31a56c71888f2f.js"],"Provider"]
3:I[87555,[],""]
4:I[31295,[],""]
5:I[32176,["785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","335","static/chunks/app/%5Bslug%5D/error-ed750438367dde98.js"],"default"]
6:I[6874,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],""]
7:I[38567,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Button"]
9:I[59665,[],"OutletBoundary"]
c:I[59665,[],"ViewportBoundary"]
e:I[59665,[],"MetadataBoundary"]
10:I[26614,[],""]
:HL["/kouchou-ai-reports/_next/static/css/9aa33c77ef4c0fa8.css","style"]
0:{"P":null,"b":"GxkcznzSy01uft9grKZAw","p":"/kouchou-ai-reports","c":["","98cacd83-fca7-4be1-9186-25ad4a9b584f",""],"i":false,"f":[[["",{"children":[["slug","98cacd83-fca7-4be1-9186-25ad4a9b584f","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/kouchou-ai-reports/_next/static/css/9aa33c77ef4c0fa8.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"suppressHydrationWarning":true,"lang":"ja","children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/kouchou-ai-reports/meta/icon.png","sizes":"any"}],false]}],["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}]]}],{"children":[["slug","98cacd83-fca7-4be1-9186-25ad4a9b584f","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$5","errorStyles":[],"errorScripts":[],"template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","p",null,{"children":"ページが見つかりませんでした"}],["$","$L6",null,{"href":"/","children":["$","$L7",null,{"children":"トップに戻る"}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L8","$undefined",null,["$","$L9",null,{"children":["$La","$Lb",null]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","aE7yN-kiWnWpVmnehLOk2",{"children":[["$","$Lc",null,{"children":"$Ld"}],null]}],["$","$Le",null,{"children":"$Lf"}]]}],false]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:null
11:I[90754,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Header"]
12:I[81068,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Box"]
13:I[17921,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Heading"]
14:I[90310,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Text"]
15:I[7684,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Icon"]
16:I[68264,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"ClientContainer"]
22:I[91925,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Analysis"]
23:I[91548,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Separator"]
24:I[12498,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"About"]
25:I[38639,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Stack"]
26:I[97377,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"HStack"]
27:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerRoot"]
28:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerBackdrop"]
29:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerTrigger"]
2a:I[70318,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"Portal"]
2b:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerPositioner"]
2c:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerContent"]
2d:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerHeader"]
2e:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerTitle"]
2f:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerBody"]
30:I[30220,["150","static/chunks/59650de3-481cc0c44db376d7.js","785","static/chunks/785-bf8eaad2e6817186.js","567","static/chunks/567-5def3a78145d78b3.js","451","static/chunks/451-ff68bb1f8be23e5c.js","874","static/chunks/874-3fb57fb0f9cdc472.js","98","static/chunks/98-ceb6bd4957f64cef.js","299","static/chunks/299-7527e558eef9c80d.js","22","static/chunks/22-7988931263da9a67.js","3","static/chunks/3-17196377db553479.js","182","static/chunks/app/%5Bslug%5D/page-bed5a9e8f3c9b31d.js"],"DrawerActionTrigger"]
17:T13f1,import concurrent.futures
import json
import logging
import re

import pandas as pd
from tqdm import tqdm

from services.category_classification import classify_args
from services.llm import request_to_chat_openai
from services.parse_json_list import parse_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(batch_inputs, prompt, model, workers)

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": arg,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    classification_categories = config["extraction"]["categories"]
    if classification_categories:
        results = classify_args(results, config, workers)

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.ERROR)


def extract_batch(batch, prompt, model, workers):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model)) for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []
        return results


def extract_by_llm(input, prompt, model):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    response = request_to_chat_openai(messages=messages, model=model)
    return response


def extract_arguments(input, prompt, model, retries=1):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response = request_to_chat_openai(messages=messages, model=model, is_json=False)
        items = parse_response(response)
        items = filter(None, items)  # omit empty strings
        return items
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
18:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
19:T142a,import json
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd

from services.llm import request_to_chat_openai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response = request_to_chat_openai(messages=messages, model=model, is_json=True)
        response_json = json.loads(response)
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
1a:T2fa5,import json
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from tqdm import tqdm

from services.llm import request_to_chat_openai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response = request_to_chat_openai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            is_json=True,
        )
        response_json = json.loads(response)
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
1b:T4c0,"""Create summaries for the clusters."""

import pandas as pd

from services.llm import request_to_chat_openai


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input = ""
    for i, _ in enumerate(ids):
        input += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input += descriptions[i] + "\n\n"

    messages = [{"role": "user", "content": prompt}, {"role": "user", "content": input}]
    response = request_to_chat_openai(messages=messages, model=model)

    with open(path, "w") as file:
        file.write(response)
1c:T22d3,"""Generate a convenient JSON output file."""

import json
from collections import defaultdict
from pathlib import Path
from typing import TypedDict

import pandas as pd

ROOT_DIR = Path(__file__).parent.parent.parent.parent
CONFIG_DIR = ROOT_DIR / "scatter" / "pipeline" / "configs"


class Argument(TypedDict):
    arg_id: str
    argument: str
    comment_id: str
    x: float
    y: float
    p: float
    cluster_ids: list[str]


class Cluster(TypedDict):
    level: int
    id: str
    label: str
    takeaway: str
    value: int
    parent: str
    density_rank_percentile: float | None


def hierarchical_aggregation(config):
    path = f"outputs/{config['output_dir']}/hierarchical_result.json"
    results = {
        "arguments": [],
        "clusters": [],
        "comments": {},
        "propertyMap": {},
        "translations": {},
        "overview": "",
        "config": config,
    }

    arguments = pd.read_csv(f"outputs/{config['output_dir']}/args.csv")
    arguments.set_index("arg-id", inplace=True)
    arg_num = len(arguments)
    relation_df = pd.read_csv(f"outputs/{config['output_dir']}/relations.csv")
    comments = pd.read_csv(f"inputs/{config['input']}.csv")
    clusters = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")
    labels = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_merge_labels.csv")

    hidden_properties_map: dict[str, list[str]] = config["hierarchical_aggregation"]["hidden_properties"]

    results["arguments"] = _build_arguments(clusters)
    results["clusters"] = _build_cluster_value(labels, arg_num)
    # NOTE: 属性に応じたコメントフィルタ機能が実装されておらず、全てのコメントが含まれてしまうので、コメントアウト
    # results["comments"] = _build_comments_value(
    #     comments, arguments, hidden_properties_map
    # )
    results["comment_num"] = len(comments)
    results["translations"] = _build_translations(config)
    # 属性情報のカラムは、元データに対して指定したカラムとclassificationするカテゴリを合わせたもの
    results["propertyMap"] = _build_property_map(arguments, hidden_properties_map, config)

    with open(f"outputs/{config['output_dir']}/hierarchical_overview.txt") as f:
        overview = f.read()
    print("overview")
    print(overview)
    results["overview"] = overview

    with open(path, "w") as file:
        json.dump(results, file, indent=2, ensure_ascii=False)
    # TODO: サンプリングロジックを実装したいが、現状は全件抽出
    create_custom_intro(config)
    if config["is_pubcom"]:
        add_original_comments(labels, arguments, relation_df, clusters, config)


def create_custom_intro(config):
    dataset = config["output_dir"]
    args_path = f"outputs/{dataset}/args.csv"
    comments = pd.read_csv(f"inputs/{config['input']}.csv")
    result_path = f"outputs/{dataset}/hierarchical_result.json"

    input_count = len(comments)
    args_count = len(pd.read_csv(args_path))
    processed_num = min(input_count, config["extraction"]["limit"])

    print(f"Input count: {input_count}")
    print(f"Args count: {args_count}")

    base_custom_intro = """{intro}
分析対象となったデータの件数は{processed_num}件で、これらのデータに対してOpenAI APIを用いて{args_count}件の意見（議論）を抽出し、クラスタリングを行った。
"""

    intro = config["intro"]
    custom_intro = base_custom_intro.format(intro=intro, processed_num=processed_num, args_count=args_count)

    with open(result_path) as f:
        result = json.load(f)
    result["config"]["intro"] = custom_intro
    with open(result_path, "w") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)


def add_original_comments(labels, arguments, relation_df, clusters, config):
    # 大カテゴリ（cluster-level-1）に該当するラベルだけ抽出
    labels_lv1 = labels[labels["level"] == 1][["id", "label"]].rename(
        columns={"id": "cluster-level-1-id", "label": "category_label"}
    )

    # arguments と clusters をマージ（カテゴリ情報付与）
    merged = arguments.merge(clusters[["arg-id", "cluster-level-1-id"]], on="arg-id").merge(
        labels_lv1, on="cluster-level-1-id", how="left"
    )

    # relation_df と結合
    merged = merged.merge(relation_df, on="arg-id", how="left")

    # 元コメント取得
    comments = pd.read_csv(f"inputs/{config['input']}.csv")
    comments["comment-id"] = comments["comment-id"].astype(str)
    merged["comment-id"] = merged["comment-id"].astype(str)

    # 元コメント本文などとマージ
    final_df = merged.merge(comments, on="comment-id", how="left")

    # 必要カラムのみ整形
    final_cols = ["comment-id", "comment-body", "arg-id", "argument", "cluster-level-1-id", "category_label"]
    for col in ["source", "url"]:
        if col in comments.columns:
            final_cols.append(col)

    final_df = final_df[final_cols]
    final_df = final_df.rename(
        columns={
            "cluster-level-1-id": "category_id",
            "category_label": "category",
            "arg-id": "arg_id",
            "argument": "argument",
            "comment-body": "original-comment",
        }
    )

    # 保存
    final_df.to_csv(f"outputs/{config['output_dir']}/final_result_with_comments.csv", index=False)


def _build_arguments(clusters: pd.DataFrame) -> list[Argument]:
    cluster_columns = [col for col in clusters.columns if col.startswith("cluster-level-") and "id" in col]

    arguments: list[Argument] = []
    for _, row in clusters.iterrows():
        cluster_ids = ["0"]
        for cluster_column in cluster_columns:
            cluster_ids.append(row[cluster_column])
        argument: Argument = {
            "arg_id": row["arg-id"],
            "argument": row["argument"],
            "x": row["x"],
            "y": row["y"],
            "p": 0,  # NOTE: 一旦全部0でいれる
            "cluster_ids": cluster_ids,
        }
        arguments.append(argument)
    return arguments


def _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:
    results: list[Cluster] = [
        Cluster(
            level=0,
            id="0",
            label="全体",
            takeaway="",
            value=total_num,
            parent="",
            density_rank_percentile=0,
        )
    ]

    for _, melted_label in melted_labels.iterrows():
        cluster_value = Cluster(
            level=melted_label["level"],
            id=melted_label["id"],
            label=melted_label["label"],
            takeaway=melted_label["description"],
            value=melted_label["value"],
            parent=melted_label.get("parent", "全体"),
            density_rank_percentile=melted_label.get("density_rank_percentile"),
        )
        results.append(cluster_value)
    return results


def _build_comments_value(
    comments: pd.DataFrame,
    arguments: pd.DataFrame,
    hidden_properties_map: dict[str, list[str]],
):
    comment_dict: dict[str, dict[str, str]] = {}
    useful_comment_ids = set(arguments["comment-id"].values)
    for _, row in comments.iterrows():
        id = row["comment-id"]
        if id in useful_comment_ids:
            res = {"comment": row["comment-body"]}
            should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())
            if should_skip:
                continue
            comment_dict[str(id)] = res

    return comment_dict


def _build_translations(config):
    languages = list(config.get("translation", {}).get("languages", []))
    if len(languages) > 0:
        with open(f"outputs/{config['output_dir']}/translations.json") as f:
            translations = f.read()
        return json.loads(translations)
    return {}


def _build_property_map(
    arguments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict
) -> dict[str, dict[str, str]]:
    property_columns = list(hidden_properties_map.keys()) + list(config["extraction"]["categories"].keys())
    property_map = defaultdict(dict)

    # 指定された property_columns が arguments に存在するかチェック
    missing_cols = [col for col in property_columns if col not in arguments.columns]
    if missing_cols:
        raise ValueError(
            f"指定されたカラム {missing_cols} が args.csv に存在しません。"
            "設定ファイルaggregation / hidden_propertiesから該当カラムを取り除いてください。"
        )

    for prop in property_columns:
        for arg_id, row in arguments.iterrows():
            # LLMによるcategory classificationがうまく行かず、NaNの場合はNoneにする
            property_map[prop][arg_id] = row[prop] if not pd.isna(row[prop]) else None
    return property_map
1d:T13f1,import concurrent.futures
import json
import logging
import re

import pandas as pd
from tqdm import tqdm

from services.category_classification import classify_args
from services.llm import request_to_chat_openai
from services.parse_json_list import parse_response
from utils import update_progress

COMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r",\s*(\])")


def _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:
    if not all(property in comments.columns for property in property_columns):
        raise ValueError(f"Properties {property_columns} not found in comments. Columns are {comments.columns}")


def extraction(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/args.csv"
    model = config["extraction"]["model"]
    prompt = config["extraction"]["prompt"]
    workers = config["extraction"]["workers"]
    limit = config["extraction"]["limit"]
    property_columns = config["extraction"]["properties"]

    # カラム名だけを読み込み、必要なカラムが含まれているか確認する
    comments = pd.read_csv(f"inputs/{config['input']}.csv", nrows=0)
    _validate_property_columns(property_columns, comments)
    # エラーが出なかった場合、すべての行を読み込む
    comments = pd.read_csv(
        f"inputs/{config['input']}.csv", usecols=["comment-id", "comment-body"] + config["extraction"]["properties"]
    )
    comment_ids = (comments["comment-id"].values)[:limit]
    comments.set_index("comment-id", inplace=True)
    results = pd.DataFrame()
    update_progress(config, total=len(comment_ids))

    argument_map = {}
    relation_rows = []

    for i in tqdm(range(0, len(comment_ids), workers)):
        batch = comment_ids[i : i + workers]
        batch_inputs = [comments.loc[id]["comment-body"] for id in batch]
        batch_results = extract_batch(batch_inputs, prompt, model, workers)

        for comment_id, extracted_args in zip(batch, batch_results, strict=False):
            for j, arg in enumerate(extracted_args):
                if arg not in argument_map:
                    # argumentテーブルに追加
                    arg_id = f"A{comment_id}_{j}"
                    argument_map[arg] = {
                        "arg-id": arg_id,
                        "argument": arg,
                    }
                else:
                    arg_id = argument_map[arg]["arg-id"]

                # relationテーブルにcommentとargの関係を追加
                relation_row = {
                    "arg-id": arg_id,
                    "comment-id": comment_id,
                }
                relation_rows.append(relation_row)

        update_progress(config, incr=len(batch))

    # DataFrame化
    results = pd.DataFrame(argument_map.values())
    relation_df = pd.DataFrame(relation_rows)

    if results.empty:
        raise RuntimeError("result is empty, maybe bad prompt")

    classification_categories = config["extraction"]["categories"]
    if classification_categories:
        results = classify_args(results, config, workers)

    results.to_csv(path, index=False)
    # comment-idとarg-idの関係を保存
    relation_df.to_csv(f"outputs/{dataset}/relations.csv", index=False)


logging.basicConfig(level=logging.ERROR)


def extract_batch(batch, prompt, model, workers):
    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
        futures_with_index = [
            (i, executor.submit(extract_arguments, input, prompt, model)) for i, input in enumerate(batch)
        ]

        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)
        results = [[] for _ in range(len(batch))]

        for _, future in futures_with_index:
            if future in not_done and not future.cancelled():
                future.cancel()

        for i, future in futures_with_index:
            if future in done:
                try:
                    result = future.result()
                    results[i] = result
                except Exception as e:
                    logging.error(f"Task {future} failed with error: {e}")
                    results[i] = []
        return results


def extract_by_llm(input, prompt, model):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    response = request_to_chat_openai(messages=messages, model=model)
    return response


def extract_arguments(input, prompt, model, retries=1):
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response = request_to_chat_openai(messages=messages, model=model, is_json=False)
        items = parse_response(response)
        items = filter(None, items)  # omit empty strings
        return items
    except json.decoder.JSONDecodeError as e:
        print("JSON error:", e)
        print("Input was:", input)
        print("Response was:", response)
        print("Silently giving up on trying to generate valid list.")
        return []
1e:T1149,"""Cluster the arguments using UMAP + HDBSCAN and GPT-4."""

from importlib import import_module

import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans


def hierarchical_clustering(config):
    UMAP = import_module("umap").UMAP

    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_clusters.csv"
    arguments_df = pd.read_csv(f"outputs/{dataset}/args.csv", usecols=["arg-id", "argument"])
    embeddings_df = pd.read_pickle(f"outputs/{dataset}/embeddings.pkl")
    embeddings_array = np.asarray(embeddings_df["embedding"].values.tolist())
    cluster_nums = config["hierarchical_clustering"]["cluster_nums"]

    n_samples = embeddings_array.shape[0]
    # デフォルト設定は15
    default_n_neighbors = 15

    # テスト等サンプルが少なすぎる場合、n_neighborsの設定値を下げる
    if n_samples <= default_n_neighbors:
        n_neighbors = max(2, n_samples - 1)  # 最低2以上
    else:
        n_neighbors = default_n_neighbors

    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)
    # TODO 詳細エラーメッセージを加える
    # 以下のエラーの場合、おそらく元の意見件数が少なすぎることが原因
    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.
    umap_embeds = umap_model.fit_transform(embeddings_array)

    cluster_results = hierarchical_clustering_embeddings(
        umap_embeds=umap_embeds,
        cluster_nums=cluster_nums,
    )
    result_df = pd.DataFrame(
        {
            "arg-id": arguments_df["arg-id"],
            "argument": arguments_df["argument"],
            "x": umap_embeds[:, 0],
            "y": umap_embeds[:, 1],
        }
    )

    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):
        result_df[f"cluster-level-{cluster_level}-id"] = [f"{cluster_level}_{label}" for label in final_labels]

    result_df.to_csv(path, index=False)


def generate_cluster_count_list(min_clusters: int, max_clusters: int):
    cluster_counts = []
    current = min_clusters
    cluster_counts.append(current)

    if min_clusters == max_clusters:
        return cluster_counts

    while True:
        next_double = current * 2
        next_triple = current * 3

        if next_double >= max_clusters:
            if cluster_counts[-1] != max_clusters:
                cluster_counts.append(max_clusters)
            break

        # 次の倍はまだ max_clusters に収まるが、3倍だと超える
        # -> (次の倍は細かすぎるので)スキップして max_clusters に飛ぶ
        if next_triple > max_clusters:
            cluster_counts.append(max_clusters)
            break

        cluster_counts.append(next_double)
        current = next_double

    return cluster_counts


def merge_clusters_with_hierarchy(
    cluster_centers: np.ndarray,
    kmeans_labels: np.ndarray,
    umap_array: np.ndarray,
    n_cluster_cut: int,
):
    Z = sch.linkage(cluster_centers, method="ward")
    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion="maxclust")

    n_samples = umap_array.shape[0]
    final_labels = np.zeros(n_samples, dtype=int)

    for i in range(n_samples):
        original_label = kmeans_labels[i]
        final_labels[i] = cluster_labels_merged[original_label]

    return final_labels


def hierarchical_clustering_embeddings(
    umap_embeds,
    cluster_nums,
):
    # 最大分割数でクラスタリングを実施
    print("start initial clustering")
    initial_cluster_num = cluster_nums[-1]
    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)
    kmeans_model.fit(umap_embeds)
    print("end initial clustering")

    results = {}
    print("start hierarchical clustering")
    cluster_nums.sort()
    print(cluster_nums)
    for n_cluster_cut in cluster_nums[:-1]:
        print("n_cluster_cut: ", n_cluster_cut)
        final_labels = merge_clusters_with_hierarchy(
            cluster_centers=kmeans_model.cluster_centers_,
            kmeans_labels=kmeans_model.labels_,
            umap_array=umap_embeds,
            n_cluster_cut=n_cluster_cut,
        )
        results[n_cluster_cut] = final_labels

    results[initial_cluster_num] = kmeans_model.labels_
    print("end hierarchical clustering")

    return results
1f:T142a,import json
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import TypedDict

import pandas as pd

from services.llm import request_to_chat_openai


class LabellingResult(TypedDict):
    """各クラスタのラベリング結果を表す型"""

    cluster_id: str  # クラスタのID
    label: str  # クラスタのラベル名
    description: str  # クラスタの説明文


def hierarchical_initial_labelling(config: dict) -> None:
    """階層的クラスタリングの初期ラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_initial_labelling: 初期ラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
    """
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_initial_labels.csv"
    clusters_argument_df = pd.read_csv(f"outputs/{dataset}/hierarchical_clusters.csv")

    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith("cluster-level-")]
    initial_cluster_id_column = cluster_id_columns[-1]
    sampling_num = config["hierarchical_initial_labelling"]["sampling_num"]
    initial_labelling_prompt = config["hierarchical_initial_labelling"]["prompt"]
    model = config["hierarchical_initial_labelling"]["model"]
    workers = config["hierarchical_initial_labelling"]["workers"]

    initial_label_df = initial_labelling(
        initial_labelling_prompt,
        clusters_argument_df,
        sampling_num,
        model,
        workers,
    )
    print("start initial labelling")
    initial_clusters_argument_df = clusters_argument_df.merge(
        initial_label_df,
        left_on=initial_cluster_id_column,
        right_on="cluster_id",
        how="left",
    ).rename(
        columns={
            "label": f"{initial_cluster_id_column.replace('-id', '')}-label",
            "description": f"{initial_cluster_id_column.replace('-id', '')}-description",
        }
    )
    print("end initial labelling")
    initial_clusters_argument_df.to_csv(path, index=False)


def initial_labelling(
    prompt: str,
    clusters_df: pd.DataFrame,
    sampling_num: int,
    model: str,
    workers: int,
) -> pd.DataFrame:
    """各クラスタに対して初期ラベリングを実行する

    Args:
        prompt: LLMへのプロンプト
        clusters_df: クラスタリング結果のDataFrame
        sampling_num: 各クラスタからサンプリングする意見の数
        model: 使用するLLMモデル名
        workers: 並列処理のワーカー数

    Returns:
        各クラスタのラベリング結果を含むDataFrame
    """
    cluster_columns = [col for col in clusters_df.columns if col.startswith("cluster-level-")]
    initial_cluster_column = cluster_columns[-1]
    cluster_ids = clusters_df[initial_cluster_column].unique()
    process_func = partial(
        process_initial_labelling,
        df=clusters_df,
        prompt=prompt,
        sampling_num=sampling_num,
        target_column=initial_cluster_column,
        model=model,
    )
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(process_func, cluster_ids))
    return pd.DataFrame(results)


def process_initial_labelling(
    cluster_id: str,
    df: pd.DataFrame,
    prompt: str,
    sampling_num: int,
    target_column: str,
    model: str,
) -> LabellingResult:
    """個別のクラスタに対してラベリングを実行する

    Args:
        cluster_id: 処理対象のクラスタID
        df: クラスタリング結果のDataFrame
        prompt: LLMへのプロンプト
        sampling_num: サンプリングする意見の数
        target_column: クラスタIDが格納されている列名
        model: 使用するLLMモデル名

    Returns:
        クラスタのラベリング結果
    """
    cluster_data = df[df[target_column] == cluster_id]
    sampling_num = min(sampling_num, len(cluster_data))
    cluster = cluster_data.sample(sampling_num)
    input = "\n".join(cluster["argument"].values)
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": input},
    ]
    try:
        response = request_to_chat_openai(messages=messages, model=model, is_json=True)
        response_json = json.loads(response)
        return LabellingResult(
            cluster_id=cluster_id,
            label=response_json.get("label", "エラーでラベル名が取得できませんでした"),
            description=response_json.get("description", "エラーで解説が取得できませんでした"),
        )
    except Exception as e:
        print(e)
        return LabellingResult(
            cluster_id=cluster_id,
            label="エラーでラベル名が取得できませんでした",
            description="エラーで解説が取得できませんでした",
        )
20:T2fa5,import json
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from functools import partial

import numpy as np
import pandas as pd
from tqdm import tqdm

from services.llm import request_to_chat_openai


@dataclass
class ClusterColumns:
    """同一階層のクラスター関連のカラム名を管理するクラス"""

    id: str
    label: str
    description: str

    @classmethod
    def from_id_column(cls, id_column: str) -> "ClusterColumns":
        """ID列名から関連するカラム名を生成"""
        return cls(
            id=id_column,
            label=id_column.replace("-id", "-label"),
            description=id_column.replace("-id", "-description"),
        )


@dataclass
class ClusterValues:
    """対象クラスタのlabel/descriptionを管理するクラス"""

    label: str
    description: str

    def to_prompt_text(self) -> str:
        return f"- {self.label}: {self.description}"


def hierarchical_merge_labelling(config: dict) -> None:
    """階層的クラスタリングの結果に対してマージラベリングを実行する

    Args:
        config: 設定情報を含む辞書
            - output_dir: 出力ディレクトリ名
            - hierarchical_merge_labelling: マージラベリングの設定
                - sampling_num: サンプリング数
                - prompt: LLMへのプロンプト
                - model: 使用するLLMモデル名
                - workers: 並列処理のワーカー数
    """
    dataset = config["output_dir"]
    merge_path = f"outputs/{dataset}/hierarchical_merge_labels.csv"
    clusters_df = pd.read_csv(f"outputs/{dataset}/hierarchical_initial_labels.csv")

    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)
    # ボトムクラスタのラベル・説明とクラスタid付きの各argumentを入力し、各階層のクラスタラベル・説明を生成し、argumentに付けたdfを作成
    merge_result_df = merge_labelling(
        clusters_df=clusters_df,
        cluster_id_columns=sorted(cluster_id_columns, reverse=True),
        config=config,
    )
    # 上記のdfから各クラスタのlevel, id, label, description, valueを取得してdfを作成
    melted_df = melt_cluster_data(merge_result_df)
    # 上記のdfに親子関係を追加
    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)
    melted_df = melted_df.merge(parent_child_df, on=["level", "id"], how="left")
    density_df = calculate_cluster_density(melted_df, config)
    density_df.to_csv(merge_path, index=False)


def _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):
    """クラスタ間の親子関係をマッピングする

    Args:
        df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト

    Returns:
        親子関係のマッピング情報を含むDataFrame
    """
    results = []
    top_cluster_column = cluster_id_columns[0]
    top_cluster_values = df[top_cluster_column].unique()
    for c in top_cluster_values:
        results.append(
            {
                "level": 1,
                "id": c,
                "parent": "0",  # aggregationで追加する全体クラスタのid
            }
        )

    for idx in range(len(cluster_id_columns) - 1):
        current_column = cluster_id_columns[idx]
        children_column = cluster_id_columns[idx + 1]
        current_level = current_column.replace("-id", "").replace("cluster-level-", "")
        # 現在のレベルのクラスタid
        current_cluster_values = df[current_column].unique()
        for current_id in current_cluster_values:
            children_ids = df.loc[df[current_column] == current_id, children_column].unique()
            for child_id in children_ids:
                results.append(
                    {
                        "level": int(current_level) + 1,
                        "id": child_id,
                        "parent": current_id,
                    }
                )
    return pd.DataFrame(results)


def _filter_id_columns(columns: list[str]) -> list[str]:
    """クラスタIDのカラム名をフィルタリングする

    Args:
        columns: 全カラム名のリスト

    Returns:
        クラスタIDのカラム名のリスト
    """
    return [col for col in columns if col.startswith("cluster-level-") and col.endswith("-id")]


def melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:
    """クラスタデータを行形式に変換する

    cluster-level-n-(id|label|description) を行形式 (level, id, label, description, value) にまとめる。
    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] を [level, id, label, description, value(件数)] に変換する。

    Args:
        df: クラスタリング結果のDataFrame

    Returns:
        行形式に変換されたDataFrame
    """
    id_columns: list[str] = _filter_id_columns(df.columns)
    levels: set[int] = {int(col.replace("cluster-level-", "").replace("-id", "")) for col in id_columns}
    all_rows: list[dict] = []

    # levelごとに各クラスタの出現件数を集計・縦持ちにする
    for level in levels:
        cluster_columns = ClusterColumns.from_id_column(f"cluster-level-{level}-id")
        # クラスタidごとの件数集計
        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name="value")

        level_unique_val_df = df[
            [cluster_columns.id, cluster_columns.label, cluster_columns.description]
        ].drop_duplicates()
        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how="left")
        level_unique_vals = [
            {
                "level": level,
                "id": row[cluster_columns.id],
                "label": row[cluster_columns.label],
                "description": row[cluster_columns.description],
                "value": row["value"],
            }
            for _, row in level_unique_val_df.iterrows()
        ]
        all_rows.extend(level_unique_vals)
    return pd.DataFrame(all_rows)


def merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:
    """階層的なクラスタのマージラベリングを実行する

    Args:
        clusters_df: クラスタリング結果のDataFrame
        cluster_id_columns: クラスタIDのカラム名のリスト
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含むDataFrame
    """
    for idx in tqdm(range(len(cluster_id_columns) - 1)):
        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])
        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])

        process_fn = partial(
            process_merge_labelling,
            result_df=clusters_df,
            current_columns=current_columns,
            previous_columns=previous_columns,
            config=config,
        )

        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())
        with ThreadPoolExecutor(max_workers=config["hierarchical_merge_labelling"]["workers"]) as executor:
            responses = list(
                tqdm(
                    executor.map(process_fn, current_cluster_ids),
                    total=len(current_cluster_ids),
                )
            )

        current_result_df = pd.DataFrame(responses)
        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])
    return clusters_df


def process_merge_labelling(
    target_cluster_id: str,
    result_df: pd.DataFrame,
    current_columns: ClusterColumns,
    previous_columns: ClusterColumns,
    config,
):
    """個別のクラスタに対してマージラベリングを実行する

    Args:
        target_cluster_id: 処理対象のクラスタID
        result_df: クラスタリング結果のDataFrame
        current_columns: 現在のレベルのカラム情報
        previous_columns: 前のレベルのカラム情報
        config: 設定情報を含む辞書

    Returns:
        マージラベリング結果を含む辞書
    """

    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:
        """前のレベルのクラスタ情報を取得する"""
        previous_records = df[df[current_columns.id] == target_cluster_id][
            [previous_columns.label, previous_columns.description]
        ].drop_duplicates()
        previous_values = [
            ClusterValues(
                label=row[previous_columns.label],
                description=row[previous_columns.description],
            )
            for _, row in previous_records.iterrows()
        ]
        return previous_values

    previous_values = filter_previous_values(result_df, previous_columns)
    if len(previous_values) == 1:
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: previous_values[0].label,
            current_columns.description: previous_values[0].description,
        }
    elif len(previous_values) == 0:
        raise ValueError(f"クラスタ {target_cluster_id} には前のレベルのクラスタが存在しません。")

    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]
    sampling_num = min(
        config["hierarchical_merge_labelling"]["sampling_num"],
        len(current_cluster_data),
    )
    sampled_data = current_cluster_data.sample(sampling_num)
    sampled_argument_text = "\n".join(sampled_data["argument"].values)
    cluster_text = "\n".join([value.to_prompt_text() for value in previous_values])
    messages = [
        {"role": "system", "content": config["hierarchical_merge_labelling"]["prompt"]},
        {
            "role": "user",
            "content": "クラスタラベル\n" + cluster_text + "\n" + "クラスタの意見\n" + sampled_argument_text,
        },
    ]
    try:
        response = request_to_chat_openai(
            messages=messages,
            model=config["hierarchical_merge_labelling"]["model"],
            is_json=True,
        )
        response_json = json.loads(response)
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: response_json.get("label", "エラーでラベル名が取得できませんでした"),
            current_columns.description: response_json.get("description", "エラーで解説が取得できませんでした"),
        }
    except Exception as e:
        print(f"エラーが発生しました: {e}")
        return {
            current_columns.id: target_cluster_id,
            current_columns.label: "エラーでラベル名が取得できませんでした",
            current_columns.description: "エラーで解説が取得できませんでした",
        }


def calculate_cluster_density(melted_df: pd.DataFrame, config: dict):
    """クラスタ内の密度計算"""
    hierarchical_cluster_df = pd.read_csv(f"outputs/{config['output_dir']}/hierarchical_clusters.csv")

    densities = []
    for level, c_id in zip(melted_df["level"], melted_df["id"], strict=False):
        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f"cluster-level-{level}-id"] == c_id][
            ["x", "y"]
        ].values
        density = calculate_density(cluster_embeds)
        densities.append(density)

    # 密度のランクを計算
    melted_df["density"] = densities
    melted_df["density_rank"] = melted_df.groupby("level")["density"].rank(ascending=False, method="first")
    melted_df["density_rank_percentile"] = melted_df.groupby("level")["density_rank"].transform(lambda x: x / len(x))
    return melted_df


def calculate_density(embeds: np.ndarray):
    """平均距離に基づいて密度を計算"""
    center = np.mean(embeds, axis=0)
    distances = np.linalg.norm(embeds - center, axis=1)
    avg_distance = np.mean(distances)
    density = 1 / (avg_distance + 1e-10)
    return density
21:T4c0,"""Create summaries for the clusters."""

import pandas as pd

from services.llm import request_to_chat_openai


def hierarchical_overview(config):
    dataset = config["output_dir"]
    path = f"outputs/{dataset}/hierarchical_overview.txt"

    hierarchical_label_df = pd.read_csv(f"outputs/{dataset}/hierarchical_merge_labels.csv")

    prompt = config["hierarchical_overview"]["prompt"]
    model = config["hierarchical_overview"]["model"]

    # TODO: level1で固定にしているが、設定で変えられるようにする
    target_level = 1
    target_records = hierarchical_label_df[hierarchical_label_df["level"] == target_level]
    ids = target_records["id"].to_list()
    labels = target_records["label"].to_list()
    descriptions = target_records["description"].to_list()
    target_records.set_index("id", inplace=True)

    input = ""
    for i, _ in enumerate(ids):
        input += f"# Cluster {i}/{len(ids)}: {labels[i]}\n\n"
        input += descriptions[i] + "\n\n"

    messages = [{"role": "user", "content": prompt}, {"role": "user", "content": input}]
    response = request_to_chat_openai(messages=messages, model=model)

    with open(path, "w") as file:
        file.write(response)
8:[["$","div",null,{"className":"container","children":[["$","$L11",null,{"meta":{"reporter":"テスト環境","message":"これは動作確認のためのテスト用のメタデータです。レポートの作成者に関する情報等を、public/meta/default/metadata.jsonに記載することで、レポート上で情報を表示することができます。","webLink":"https://example.com/","privacyLink":"https://example.com/privacy","termsLink":"https://example.com/terms","brandColor":"#2577B1"}}],["$","$L12",null,{"mx":"auto","maxW":"750px","mb":10,"children":[["$","$L13",null,{"textAlign":"center","fontSize":"xl","mb":5,"children":"Report"}],["$","$L13",null,{"as":"h2","size":"4xl","mb":2,"className":"headingColor","children":"さいとう三陸菓匠国分通店のGoogle Mapsレビュー"}],["$","$L14",null,{"fontWeight":"bold","fontSize":"xl","mb":2,"children":[["$","$L15",null,{"mr":1,"children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":20,"height":20,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-messages-square","children":[["$","path","p1xzt8",{"d":"M14 9a2 2 0 0 1-2 2H6l-4 4V4a2 2 0 0 1 2-2h8a2 2 0 0 1 2 2z"}],["$","path","1cx29u",{"d":"M18 9h2a2 2 0 0 1 2 2v11l-4-4h-6a2 2 0 0 1-2-2v-1"}],"$undefined"]}]}],"115","件"]}],["$","p",null,{"children":"この調査では、岩手の和菓子文化やスイーツ、パンに関する多様な意見が集約されました。特に「かもめの玉子」のような地域特産品や、スイーツの選択肢の豊富さ、イートイン体験の重要性が強調されています。また、顧客体験やサービス品質の向上がリピート訪問に寄与することが示され、店舗の改善が求められています。全体として、食文化の楽しさと顧客満足度の向上が重要なテーマとなっています。"}]]}],["$","$L16",null,{"result":{"arguments":[{"arg_id":"Acsv-1_0","argument":"毎年、県外に住む嫁っこのお母さんに誕生日のお祝いとして'カモメの玉子'を送っている","x":6.71236,"y":6.663228,"p":0,"cluster_ids":["0","1_2","2_41"]},{"arg_id":"Acsv-1_1","argument":"ケーキの販売が再開されたことを嬉しく思っている","x":8.244471,"y":4.7733397,"p":0,"cluster_ids":["0","1_4","2_18"]},{"arg_id":"Acsv-1_2","argument":"手作りパンのCOCOAが進化しており、シフォンケーキのような食感のパンがある","x":9.33662,"y":4.41996,"p":0,"cluster_ids":["0","1_3","2_35"]},{"arg_id":"Acsv-1_3","argument":"期間限定のパンに注目している","x":9.241378,"y":3.4794207,"p":0,"cluster_ids":["0","1_3","2_17"]},{"arg_id":"Acsv-1_4","argument":"お目当てのパンは電話で予約することを勧めている","x":9.150509,"y":3.2536156,"p":0,"cluster_ids":["0","1_3","2_36"]},{"arg_id":"Acsv-2_0","argument":"大船渡の本店で有名な「かもめの玉子」は、県外への贈り物やおもたせに最適なお菓子屋さんである。","x":6.092381,"y":6.6235466,"p":0,"cluster_ids":["0","1_2","2_33"]},{"arg_id":"Acsv-2_1","argument":"プレーンやプレーンミニサイズの他に、紅茶味や季節限定の味（冬はりんご、夏はブルーベリーなど）があり、バラエティ豊かで最高である。","x":8.118802,"y":6.0501914,"p":0,"cluster_ids":["0","1_4","2_6"]},{"arg_id":"Acsv-2_2","argument":"年末年始には紅白のかもめの玉子が販売される。","x":5.855,"y":7.020569,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-2_3","argument":"ラムレーズンバターどら焼きが特におすすめである。","x":10.339375,"y":4.973765,"p":0,"cluster_ids":["0","1_3","2_22"]},{"arg_id":"Acsv-2_4","argument":"店内の半分はイートイン可能なパン屋さんで、機会があれば試してみたい。","x":8.369732,"y":3.515937,"p":0,"cluster_ids":["0","1_5","2_40"]},{"arg_id":"Acsv-2_5","argument":"店員の愛想が良く、気持ちよく会計できるのが印象的で、社内教育がしっかりしていると思われる。","x":4.7105923,"y":4.3799806,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-3_0","argument":"三陸を代表する銘菓「かもめの玉子」は、季節感のあるいちご、みかん、林檎、チョコなど多彩なバリエーションがある。特に、さいとう製菓の季節限定いちご春かもめの玉子は、香り豊かで美味しい。可愛らしいピンク色のチョコレートコーティングの玉子の中には白餡と苺ピューレが入っており、一口でいちごの香りが広がる。","x":6.250732,"y":6.990256,"p":0,"cluster_ids":["0","1_2","2_29"]},{"arg_id":"Acsv-4_0","argument":"さいとう三陸菓匠は焼きたてのパンやお菓子をイートインスペースで楽しめる","x":8.047573,"y":3.7055972,"p":0,"cluster_ids":["0","1_5","2_16"]},{"arg_id":"Acsv-4_1","argument":"イートインスペースには無料のコーヒーやお茶があり、スタッフの対応が優しく丁寧","x":6.595309,"y":3.0848417,"p":0,"cluster_ids":["0","1_5","2_31"]},{"arg_id":"Acsv-4_2","argument":"塩あんパンが絶妙でリーズナブルだが、売り切れだったためあんブレットを購入","x":9.17162,"y":3.6932993,"p":0,"cluster_ids":["0","1_3","2_17"]},{"arg_id":"Acsv-4_3","argument":"本日はイートインスペースが利用できなかったが、また訪れたい","x":6.726743,"y":3.470534,"p":0,"cluster_ids":["0","1_5","2_26"]},{"arg_id":"Acsv-5_0","argument":"パン工房COCOAでいくつかのパンを試食した。","x":8.981894,"y":4.096888,"p":0,"cluster_ids":["0","1_3","2_23"]},{"arg_id":"Acsv-5_1","argument":"幻のクリームパンは滑らかなクリームが美味しく、さらっと食べられる。","x":10.4277315,"y":4.4197893,"p":0,"cluster_ids":["0","1_3","2_48"]},{"arg_id":"Acsv-5_2","argument":"サラダパンはヘルシーで美味しく、店員さんに薦められた通りの期待を裏切らない。","x":10.184109,"y":3.9229279,"p":0,"cluster_ids":["0","1_3","2_3"]},{"arg_id":"Acsv-5_3","argument":"コーヒーあんパンはコーヒーの風味と生クリームの相性が抜群で、個人的に一番美味しかった。","x":10.204817,"y":4.6829224,"p":0,"cluster_ids":["0","1_3","2_22"]},{"arg_id":"Acsv-5_4","argument":"さいとう製菓のシュークリームやケーキも販売されている。","x":7.5673404,"y":4.907777,"p":0,"cluster_ids":["0","1_4","2_18"]},{"arg_id":"Acsv-5_5","argument":"店員さんの接客は好感が持てた。","x":4.8951454,"y":4.3034267,"p":0,"cluster_ids":["0","1_1","2_44"]},{"arg_id":"Acsv-5_6","argument":"2022.2追記: 焼きみたらし団子は美味しかったが、焼きならではの特徴は感じられなかった。","x":9.910581,"y":5.4880247,"p":0,"cluster_ids":["0","1_3","2_7"]},{"arg_id":"Acsv-6_0","argument":"さいとう製菓の分店では、定番の「かもめの玉子」やケーキ、和菓子、パンを販売している。","x":5.8152485,"y":6.1295447,"p":0,"cluster_ids":["0","1_2","2_25"]},{"arg_id":"Acsv-6_1","argument":"店内にはオシャレな喫茶スペースがあり、購入したパンを食べることができる。","x":7.9355626,"y":3.2265303,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-6_2","argument":"LINE登録をするとパンの割引があり、パンはフカフカで食べやすい。","x":10.086116,"y":3.5990665,"p":0,"cluster_ids":["0","1_3","2_42"]},{"arg_id":"Acsv-6_3","argument":"「かもめの玉子」には時期によって限定品があるため、LINE登録をオススメする。","x":6.193807,"y":7.1887527,"p":0,"cluster_ids":["0","1_2","2_29"]},{"arg_id":"Acsv-7_0","argument":"三陸菓匠さいとうのかもめの玉子ミニはしっとりほくほくの黄味餡をカステラ生地とホワイトチョコで包んだお菓子で、美味しかった。","x":6.0419145,"y":7.281381,"p":0,"cluster_ids":["0","1_2","2_29"]},{"arg_id":"Acsv-8_0","argument":"最近ハマっているちぎりパンのチョコとクリームの2色パンが美味しい","x":9.832584,"y":4.694624,"p":0,"cluster_ids":["0","1_3","2_45"]},{"arg_id":"Acsv-8_1","argument":"初めて食べた明太子ポテトピザも美味しかった","x":9.938553,"y":5.5658712,"p":0,"cluster_ids":["0","1_3","2_7"]},{"arg_id":"Acsv-8_2","argument":"昔本町でよく食べたじゃがいもがゴロッと入っているパンを再販してほしい","x":9.431624,"y":3.622807,"p":0,"cluster_ids":["0","1_3","2_17"]},{"arg_id":"Acsv-8_3","argument":"店員さんが笑顔で丁寧だったので、また行きたいと思った","x":5.0168333,"y":4.6077604,"p":0,"cluster_ids":["0","1_1","2_32"]},{"arg_id":"Acsv-8_4","argument":"次回はお菓子も買ってみたい","x":7.145267,"y":5.6005573,"p":0,"cluster_ids":["0","1_4","2_12"]},{"arg_id":"Acsv-9_0","argument":"お昼時を過ぎていたため、パンの種類は少なかったが、きな粉揚げパンを購入してイートインコーナーで楽しんだ","x":8.607882,"y":3.4277508,"p":0,"cluster_ids":["0","1_5","2_4"]},{"arg_id":"Acsv-9_1","argument":"サービスで飲み物が3種類あり、コーヒーが特に好き","x":7.2086015,"y":2.98658,"p":0,"cluster_ids":["0","1_5","2_5"]},{"arg_id":"Acsv-9_2","argument":"さいとう製菓のお店で、お菓子を楽しみながら飲むのが本来の楽しみ方だが、雰囲気が良く長居しても良いかもしれない","x":6.8648367,"y":4.822172,"p":0,"cluster_ids":["0","1_4","2_8"]},{"arg_id":"Acsv-10_0","argument":"お土産用のお菓子が多種類あり、詰め替えが可能である","x":7.189279,"y":5.920074,"p":0,"cluster_ids":["0","1_4","2_37"]},{"arg_id":"Acsv-10_1","argument":"手作りのパンがあり、どれも美味しい","x":9.869692,"y":4.3677897,"p":0,"cluster_ids":["0","1_3","2_27"]},{"arg_id":"Acsv-10_2","argument":"包装待ちの間にコーヒーサーバーがあり、コーヒーを飲みながら待てる","x":7.082868,"y":3.1280167,"p":0,"cluster_ids":["0","1_5","2_5"]},{"arg_id":"Acsv-10_3","argument":"オマケのお菓子をもらえて嬉しかった","x":7.337482,"y":5.5563874,"p":0,"cluster_ids":["0","1_4","2_12"]},{"arg_id":"Acsv-10_4","argument":"至れり尽くせりのお店であった","x":6.119475,"y":4.8104534,"p":0,"cluster_ids":["0","1_1","2_21"]},{"arg_id":"Acsv-11_0","argument":"岩手銘菓かもめの玉子のお店は、かもめの玉子以外にも多くのお菓子を提供している","x":5.910399,"y":6.4121227,"p":0,"cluster_ids":["0","1_2","2_33"]},{"arg_id":"Acsv-11_1","argument":"さまざまなお菓子があり、選ぶのに迷う","x":7.413794,"y":5.8563614,"p":0,"cluster_ids":["0","1_4","2_37"]},{"arg_id":"Acsv-11_2","argument":"パンやケーキも取り揃えており、穴場的な存在","x":8.614885,"y":4.8849955,"p":0,"cluster_ids":["0","1_4","2_0"]},{"arg_id":"Acsv-11_3","argument":"好きなパンやケーキがある","x":9.705355,"y":4.4690456,"p":0,"cluster_ids":["0","1_3","2_27"]},{"arg_id":"Acsv-11_4","argument":"お菓子の価格は少々高めで、もう少し求めやすい価格になると良い","x":7.5383205,"y":5.358304,"p":0,"cluster_ids":["0","1_4","2_39"]},{"arg_id":"Acsv-12_0","argument":"さいとう製菓の直営店では「かもめの玉子」をはじめとする和菓子の種類が豊富で、品切れの心配が少ない","x":5.726637,"y":6.5187297,"p":0,"cluster_ids":["0","1_2","2_33"]},{"arg_id":"Acsv-12_1","argument":"和菓子だけでなくパンも販売しており、カレーパンを試したが味はまあまあだった","x":8.830414,"y":4.6809373,"p":0,"cluster_ids":["0","1_4","2_0"]},{"arg_id":"Acsv-12_2","argument":"駐車スペースがやや少ない印象","x":6.059942,"y":3.4612987,"p":0,"cluster_ids":["0","1_5","2_10"]},{"arg_id":"Acsv-13_0","argument":"ちょっとした差し入れに使えるお店で、かもめのたまごだけでなく洋菓子も豊富に取り揃えている","x":6.5594664,"y":5.3923965,"p":0,"cluster_ids":["0","1_2","2_19"]},{"arg_id":"Acsv-13_1","argument":"美味しいパンやケーキもある","x":9.547234,"y":4.7388163,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-14_0","argument":"飲食スペースがあるが、紙コップが備え付けられなくなり、店員に必要個数を言わなければならないため、少し行きにくくなった","x":6.4498687,"y":3.4099429,"p":0,"cluster_ids":["0","1_5","2_26"]},{"arg_id":"Acsv-14_1","argument":"タルトタタンのように試食があれば、集客が見込めると思う","x":8.523961,"y":3.9071028,"p":0,"cluster_ids":["0","1_5","2_40"]},{"arg_id":"Acsv-15_0","argument":"盛岡でかもめの玉子の色々な種類が買えるのは嬉しい","x":5.6983013,"y":6.909214,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-15_1","argument":"自分は実は隠れ光の朝派である","x":6.3987103,"y":5.5394764,"p":0,"cluster_ids":["0","1_2","2_19"]},{"arg_id":"Acsv-16_0","argument":"かもめの玉子の限定品いちごは期待どおりの美味しさだった","x":5.8179326,"y":7.5711446,"p":0,"cluster_ids":["0","1_2","2_9"]},{"arg_id":"Acsv-16_1","argument":"ケーキやパンも販売されていた","x":8.432173,"y":4.636039,"p":0,"cluster_ids":["0","1_4","2_0"]},{"arg_id":"Acsv-17_0","argument":"杏仁豆腐味のかもめの玉子が大好評だった","x":5.706426,"y":7.3384395,"p":0,"cluster_ids":["0","1_2","2_9"]},{"arg_id":"Acsv-17_1","argument":"月が丘店は美容室の並びにあり、美容室の帰りに寄るのに便利","x":5.8402824,"y":4.8923664,"p":0,"cluster_ids":["0","1_1","2_21"]},{"arg_id":"Acsv-17_2","argument":"隣接するケーキ＆パンコーナーでパンを買ってコーヒータイムを楽しむことができる","x":7.9280496,"y":3.105142,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-18_0","argument":"かもめの玉子の限定品がある","x":5.596795,"y":7.1553817,"p":0,"cluster_ids":["0","1_2","2_14"]},{"arg_id":"Acsv-18_1","argument":"揚げパンが気に入っている","x":9.7368,"y":4.2470603,"p":0,"cluster_ids":["0","1_3","2_27"]},{"arg_id":"Acsv-18_2","argument":"揚げパンを友達にあげたら絶賛された","x":9.505619,"y":4.112889,"p":0,"cluster_ids":["0","1_3","2_35"]},{"arg_id":"Acsv-19_0","argument":"お店に行けて良かった。かもめの玉子や他の種類のバラ売りが魅力的。ケーキやパンも販売していて、店員さんの対応も良かった。また行きたい。","x":5.5124474,"y":5.166196,"p":0,"cluster_ids":["0","1_1","2_47"]},{"arg_id":"Acsv-20_0","argument":"和から洋、パンまで商品が豊富である","x":8.042033,"y":5.9302244,"p":0,"cluster_ids":["0","1_4","2_6"]},{"arg_id":"Acsv-20_1","argument":"チーズスフレケーキは美味しいが数量が少なく、11時には売り切れるため早い者勝ち","x":7.877539,"y":4.4511185,"p":0,"cluster_ids":["0","1_4","2_38"]},{"arg_id":"Acsv-21_0","argument":"かもめの玉子等のお菓子をバラ売りしているので、来客用のお茶菓子として重宝する","x":6.0172467,"y":5.99583,"p":0,"cluster_ids":["0","1_2","2_25"]},{"arg_id":"Acsv-21_1","argument":"休憩スペースがありコーヒー等のサービスがあるが、近所の方の談話室状態になっており利用したことがない","x":6.745202,"y":3.1574388,"p":0,"cluster_ids":["0","1_5","2_31"]},{"arg_id":"Acsv-22_0","argument":"お菓子も美味しいが、パンも美味しい","x":9.419838,"y":5.015675,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-22_1","argument":"肉球パンは可愛く、中身はビターなチョコレートクリーム","x":10.581481,"y":4.527581,"p":0,"cluster_ids":["0","1_3","2_34"]},{"arg_id":"Acsv-23_0","argument":"モンブランは甘めだが美味しい","x":10.336161,"y":4.2836123,"p":0,"cluster_ids":["0","1_3","2_48"]},{"arg_id":"Acsv-23_1","argument":"店内のパンコーナーはお勧め","x":8.219661,"y":3.114835,"p":0,"cluster_ids":["0","1_5","2_20"]},{"arg_id":"Acsv-24_0","argument":"帰省中のお土産をまとめて買って郵送してもらう","x":6.7443132,"y":6.1909785,"p":0,"cluster_ids":["0","1_2","2_30"]},{"arg_id":"Acsv-24_1","argument":"ポイントカードを作成した","x":6.5316067,"y":6.947007,"p":0,"cluster_ids":["0","1_2","2_41"]},{"arg_id":"Acsv-24_2","argument":"カモメのたまごシリーズは鉄板","x":7.0857506,"y":6.682577,"p":0,"cluster_ids":["0","1_2","2_13"]},{"arg_id":"Acsv-25_0","argument":"イートインがあり落ち着いた良いお店で、贈答品にも親切に対応してくれる","x":5.6767945,"y":4.3203855,"p":0,"cluster_ids":["0","1_1","2_2"]},{"arg_id":"Acsv-25_1","argument":"お店で販売しているパンが美味しい","x":9.650449,"y":3.763863,"p":0,"cluster_ids":["0","1_3","2_42"]},{"arg_id":"Acsv-26_0","argument":"贈答用のお菓子の購入に便利で、パンやケーキも美味しかった","x":7.877848,"y":5.5402255,"p":0,"cluster_ids":["0","1_4","2_43"]},{"arg_id":"Acsv-27_0","argument":"パンが美味しくていつも買いに行く","x":9.7228155,"y":3.5336027,"p":0,"cluster_ids":["0","1_3","2_42"]},{"arg_id":"Acsv-27_1","argument":"揚げたゴマ団子もとても美味しい","x":10.065365,"y":5.2888875,"p":0,"cluster_ids":["0","1_3","2_7"]},{"arg_id":"Acsv-28_0","argument":"さいとう製菓の定員さんはとても親切で丁寧","x":5.270648,"y":4.6221433,"p":0,"cluster_ids":["0","1_1","2_32"]},{"arg_id":"Acsv-29_0","argument":"お遣い物に丁度よさげなお菓子がある","x":6.9349694,"y":5.522014,"p":0,"cluster_ids":["0","1_4","2_12"]},{"arg_id":"Acsv-29_1","argument":"焼きたてのパンがある","x":9.605414,"y":4.1332955,"p":0,"cluster_ids":["0","1_3","2_35"]},{"arg_id":"Acsv-30_0","argument":"和菓子から洋菓子、パンまで豊富な品揃えがあり、じっくり選ぶ楽しみがある","x":7.745719,"y":5.8476663,"p":0,"cluster_ids":["0","1_4","2_6"]},{"arg_id":"Acsv-31_0","argument":"御使い物に重宝している","x":6.3087893,"y":5.674189,"p":0,"cluster_ids":["0","1_2","2_19"]},{"arg_id":"Acsv-31_1","argument":"ケーキもパンも美味しい","x":9.430561,"y":4.801797,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-32_0","argument":"季節感のあるイチゴ、ミカン、林檎、チョコを使った多彩な『カモメの卵』がある","x":7.007805,"y":6.955423,"p":0,"cluster_ids":["0","1_2","2_13"]},{"arg_id":"Acsv-33_0","argument":"スフレチーズケーキは美味しかった","x":9.517736,"y":5.1522746,"p":0,"cluster_ids":["0","1_3","2_15"]},{"arg_id":"Acsv-34_0","argument":"ケーキ類が販売停止になり、代わりにブッセと一口サイズの限定商品が入っている","x":8.100673,"y":4.7579193,"p":0,"cluster_ids":["0","1_4","2_18"]},{"arg_id":"Acsv-35_0","argument":"種類が多く選ぶ楽しみがある","x":7.593594,"y":6.308912,"p":0,"cluster_ids":["0","1_4","2_28"]},{"arg_id":"Acsv-35_1","argument":"スタッフが親切丁寧である","x":5.082734,"y":4.1807246,"p":0,"cluster_ids":["0","1_1","2_44"]},{"arg_id":"Acsv-36_0","argument":"イートインで焼きたてパンやケーキを楽しめるカフェがある","x":8.273047,"y":3.6391108,"p":0,"cluster_ids":["0","1_5","2_40"]},{"arg_id":"Acsv-37_0","argument":"岩手の銘菓かもめの玉子はお土産に最適で、種類が豊富で選ぶのが楽しい","x":6.26514,"y":6.348367,"p":0,"cluster_ids":["0","1_2","2_1"]},{"arg_id":"Acsv-38_0","argument":"最低な店で、お使い物を買うのはおすすめしない","x":6.053618,"y":4.2428784,"p":0,"cluster_ids":["0","1_1","2_2"]},{"arg_id":"Acsv-39_0","argument":"岩手のお菓子のお土産を買いに来る","x":6.6429234,"y":6.27864,"p":0,"cluster_ids":["0","1_2","2_30"]},{"arg_id":"Acsv-40_0","argument":"鴎のたまごを発送しました","x":6.9223905,"y":6.1190557,"p":0,"cluster_ids":["0","1_2","2_30"]},{"arg_id":"Acsv-40_1","argument":"ケーキも取り扱っている支店です","x":7.982554,"y":4.900448,"p":0,"cluster_ids":["0","1_4","2_18"]},{"arg_id":"Acsv-41_0","argument":"かもめの卵フルセットが揃っている","x":6.182315,"y":7.5207715,"p":0,"cluster_ids":["0","1_2","2_29"]},{"arg_id":"Acsv-42_0","argument":"店員の態度や商品の品揃えが非常に良かった","x":4.755679,"y":4.320081,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-43_0","argument":"和菓子やパンが売っている","x":8.393693,"y":5.1629515,"p":0,"cluster_ids":["0","1_4","2_49"]},{"arg_id":"Acsv-44_0","argument":"お菓子とパンを楽しみながらコーヒーを飲む小休止が必要","x":7.342373,"y":3.1179166,"p":0,"cluster_ids":["0","1_5","2_5"]},{"arg_id":"Acsv-45_0","argument":"店員の対応が良く、商品が豊富である","x":4.887399,"y":4.002425,"p":0,"cluster_ids":["0","1_1","2_11"]},{"arg_id":"Acsv-46_0","argument":"パンが美味しい","x":10.3439045,"y":4.259018,"p":0,"cluster_ids":["0","1_3","2_48"]},{"arg_id":"Acsv-46_1","argument":"コーヒー、お茶、紅茶のセルフサービスがある","x":7.0395985,"y":2.8382483,"p":0,"cluster_ids":["0","1_5","2_5"]},{"arg_id":"Acsv-48_0","argument":"店員さんの対応に満足で、商品にも満足した","x":4.6183896,"y":4.442904,"p":0,"cluster_ids":["0","1_1","2_24"]},{"arg_id":"Acsv-49_0","argument":"パンはふかふかでおいしい","x":10.344126,"y":3.90916,"p":0,"cluster_ids":["0","1_3","2_3"]},{"arg_id":"Acsv-50_0","argument":"ケーキやパンはふわふわで美味しい","x":9.804339,"y":4.8479457,"p":0,"cluster_ids":["0","1_3","2_45"]},{"arg_id":"Acsv-51_0","argument":"たまにパンを買いに行く","x":9.37987,"y":3.2016904,"p":0,"cluster_ids":["0","1_3","2_36"]},{"arg_id":"Acsv-52_0","argument":"限定商品は出たらすぐに購入すべき","x":7.092712,"y":4.470933,"p":0,"cluster_ids":["0","1_4","2_46"]},{"arg_id":"Acsv-53_0","argument":"バリエーションが豊富である","x":7.723879,"y":6.523817,"p":0,"cluster_ids":["0","1_4","2_28"]},{"arg_id":"Acsv-54_0","argument":"岩手の土産はここで購入できる","x":6.436837,"y":6.334346,"p":0,"cluster_ids":["0","1_2","2_1"]},{"arg_id":"Acsv-55_0","argument":"塩パンは美味しい","x":10.131395,"y":4.0599275,"p":0,"cluster_ids":["0","1_3","2_3"]},{"arg_id":"Acsv-56_0","argument":"店員の態度が良くない","x":5.1495366,"y":4.1334553,"p":0,"cluster_ids":["0","1_1","2_44"]},{"arg_id":"Acsv-57_0","argument":"添加物が多く含まれている食品が多い","x":7.9222274,"y":6.155775,"p":0,"cluster_ids":["0","1_4","2_6"]},{"arg_id":"Acsv-59_0","argument":"駐車場が狭い","x":6.1400046,"y":3.2742555,"p":0,"cluster_ids":["0","1_5","2_10"]}],"clusters":[{"level":0,"id":"0","label":"全体","takeaway":"","value":115,"parent":"","density_rank_percentile":0},{"level":1,"id":"1_2","label":"岩手の和菓子と贈り物文化の魅力","takeaway":"このクラスタは、岩手県の名物「かもめの玉子」を中心に、地域の和菓子文化や贈り物としての利用に関する意見を集約しています。誕生日祝いの贈り物や来客用のお茶菓子としての多様な選択肢、季節感を楽しむ限定品の魅力が強調されており、地域の特産品としての「かもめの玉子」の存在感が際立っています。また、ポイントカード制度やお土産の購入方法に関する情報も含まれ、消費者の期待や評価が反映されています。","value":26,"parent":"0","density_rank_percentile":0.4},{"level":1,"id":"1_4","label":"スイーツとパンの多様性と楽しみ","takeaway":"このクラスタは、ケーキ、和菓子、洋菓子、パンなどの多様なスイーツが揃っていることに対する喜びや期待を集約しています。消費者は、豊富な選択肢の中から自分の好みに合った商品を選ぶ楽しさを感じており、特に限定商品や贈答用のお菓子の魅力が強調されています。また、店舗の雰囲気や居心地の良さも評価されており、スイーツを楽しむだけでなく、リラックスした時間を過ごすことができる場所としての価値が見出されています。","value":24,"parent":"0","density_rank_percentile":0.8},{"level":1,"id":"1_3","label":"パンとスイーツの魅力と体験","takeaway":"このクラスタは、パンやケーキに対する愛情や美味しさ、そしてそれらに関連する体験を集約しています。利用者は、手作りパンや焼き菓子の新しい食感や風味を楽しみ、特に揚げパンやスフレチーズケーキなどの具体的な評価を通じて、食文化の豊かさを感じています。また、予約の重要性やお得な購入体験についても言及されており、パンやスイーツを通じたポジティブな食体験が強調されています。","value":33,"parent":"0","density_rank_percentile":0.6},{"level":1,"id":"1_5","label":"イートイン体験とリラックス空間の融合","takeaway":"このクラスタは、イートインスペースを持つパン屋やカフェにおける食事体験とリラックスした時間の過ごし方に関する意見を集約しています。利用者は、焼きたてのパンやお菓子を楽しむことができる環境や、コーヒーを中心とした飲み物サービスの充実を求めており、特に試食やサービスの質に対する期待が高まっています。また、駐車スペースの不足や利用状況に対する懸念も示されており、より快適な訪問体験を実現するための改善が求められています。","value":18,"parent":"0","density_rank_percentile":1},{"level":1,"id":"1_1","label":"顧客体験とサービス品質の向上","takeaway":"このクラスタは、店舗における顧客体験とサービス品質に関する意見を集約しています。顧客は、店員の接客態度や商品の品揃えに対する評価を通じて、快適なショッピング体験を求めており、親切で丁寧なサービスがリピート訪問を促進する要因であることが示されています。また、店舗の立地や商品魅力も顧客満足度に寄与しており、全体として店舗のサービス向上が求められています。","value":14,"parent":"0","density_rank_percentile":0.2},{"level":2,"id":"2_41","label":"誕生日祝いのための贈り物とポイントシステム","takeaway":"このクラスタには、誕生日のお祝いとして特定の贈り物（カモメの玉子）を送る習慣と、それに関連するポイントカードの作成が含まれています。贈り物は、家族や親しい人との絆を深めるための重要な行為であり、ポイントカードはその行為を記録し、次回の贈り物に活用することで、より一層の感謝の気持ちを表す手段となっています。","value":2,"parent":"1_2","density_rank_percentile":0.84},{"level":2,"id":"2_18","label":"ケーキ販売再開への喜びと多様なスイーツの提供","takeaway":"このクラスタには、ケーキの販売再開に対する喜びや、さいとう製菓のシュークリームやケーキの取り扱いに関する期待が集まっています。また、ケーキ類が一時的に販売停止となった際の代替商品としてのブッセや一口サイズの限定商品についても言及されており、顧客が多様なスイーツを楽しむことができる環境が整っていることが強調されています。","value":4,"parent":"1_4","density_rank_percentile":1},{"level":2,"id":"2_35","label":"手作りパンの魅力と進化","takeaway":"このクラスタには、焼きたてのパンや手作りパンに関するポジティブな体験が集まっています。揚げパンを友達にあげた際の絶賛の声や、手作りパンのCOCOAの進化に関する言及があり、特にシフォンケーキのような食感のパンの登場が注目されています。これらの意見は、手作りパンの魅力や新しい食感の発見を通じて、パン作りの楽しさやその進化に対する期待感を表しています。","value":3,"parent":"1_3","density_rank_percentile":0.86},{"level":2,"id":"2_17","label":"懐かしのパンと新たな発見","takeaway":"このクラスタには、昔食べた懐かしいパンや期間限定の新商品に対する期待が集まっています。特に、じゃがいもがゴロッと入ったパンの再販を望む声や、塩あんパンの絶妙な味わいに感謝しつつも、売り切れのためにあんブレットを選んだという体験が共有されています。これにより、過去の思い出と新たなパンの発見が交錯し、パンに対する愛情が感じられます。","value":3,"parent":"1_3","density_rank_percentile":0.62},{"level":2,"id":"2_36","label":"パン購入における予約の重要性","takeaway":"このクラスタには、パンを購入する際の予約の利便性や重要性に関する意見が集まっています。特に、お目当てのパンを確実に手に入れるために電話予約を推奨する声があり、予約を通じてスムーズな購入体験を得ることができるという期待が表れています。利用者は、予約によってパンの取り置きが可能になり、より満足のいく買い物ができることを重視しています。","value":2,"parent":"1_3","density_rank_percentile":0.46},{"level":2,"id":"2_33","label":"岩手の和菓子文化と多様な商品展開","takeaway":"このクラスタには、岩手県の名物である「かもめの玉子」を中心に、さいとう製菓が提供する多様な和菓子の魅力が集まっています。特に、直営店での豊富な商品ラインナップや、品切れの心配が少ない点が強調されており、贈り物やおもたせとしての利用が推奨されています。これにより、地域の和菓子文化の発展と、観光客や地元の人々に愛されるお菓子屋さんとしての存在感が示されています。","value":3,"parent":"1_2","density_rank_percentile":0.8},{"level":2,"id":"2_6","label":"多様なスイーツとパンの魅力","takeaway":"このクラスタには、和菓子、洋菓子、パンなど、さまざまな種類のスイーツやベーカリーが豊富に揃っていることへの評価が集まっています。特に、プレーンや季節限定のフレーバーが楽しめる点が強調されており、選ぶ楽しみやバラエティの豊かさが支持されています。一方で、添加物の多さに対する懸念も見受けられ、消費者は品質や健康面にも注意を払っています。全体として、豊富な選択肢が提供されることが、食の楽しみを広げる要因となっています。","value":4,"parent":"1_4","density_rank_percentile":0.88},{"level":2,"id":"2_14","label":"かもめの玉子の限定品と多様性","takeaway":"このクラスタには、かもめの玉子に関する限定品や多様な種類の販売に対する喜びや期待が集まっています。特に盛岡での購入機会や、年末年始に特別な紅白のかもめの玉子が販売されることに対する関心が高まっており、地域の特産品としての魅力や季節感を楽しむ姿勢が見受けられます。","value":3,"parent":"1_2","density_rank_percentile":0.7},{"level":2,"id":"2_22","label":"おすすめの和菓子とその特徴","takeaway":"このクラスタには、特に美味しいと感じる和菓子に関する意見が集まっています。ラムレーズンバターどら焼きはその独特な風味から特におすすめされており、コーヒーあんパンはコーヒーの風味と生クリームの組み合わせが絶妙で、個人の好みとして一番美味しいと評価されています。これらの和菓子は、味のバリエーションや風味の組み合わせが楽しめる点で、食文化の魅力を伝えています。","value":2,"parent":"1_3","density_rank_percentile":0.78},{"level":2,"id":"2_40","label":"イートイン体験を重視したパン屋の集客戦略","takeaway":"このクラスタには、イートインスペースを持つパン屋やカフェに関する意見が集まっています。特に、焼きたてのパンやケーキを楽しむことができる環境や、試食を通じて集客を図るアイデアが強調されています。利用者は、イートインの魅力を感じており、試食の導入が集客に寄与する可能性を期待しています。","value":3,"parent":"1_5","density_rank_percentile":0.92},{"level":2,"id":"2_24","label":"顧客満足度の高い店舗体験","takeaway":"このクラスタには、店員の態度や商品の品揃えに対する高い評価が集まっています。顧客は、店員の対応や愛想の良さに満足し、快適なショッピング体験を享受していることが示されています。また、社内教育の充実が感じられることから、店舗全体のサービス品質が向上していることが伺えます。","value":3,"parent":"1_1","density_rank_percentile":0.26},{"level":2,"id":"2_29","label":"三陸の銘菓「かもめの玉子」の魅力と多様性","takeaway":"このクラスタには、三陸菓匠さいとうの「かもめの玉子」に関する様々な意見や感想が集まっています。しっとりほくほくの黄味餡を包んだお菓子としての美味しさや、季節ごとの限定品、バリエーションの豊富さが強調されています。特に、いちご春かもめの玉子のような季節感を感じさせる商品が、消費者にとっての魅力となっており、LINE登録を通じて新しい情報を得ることが推奨されています。","value":4,"parent":"1_2","density_rank_percentile":0.9},{"level":2,"id":"2_16","label":"焼きたてのパンとお菓子を楽しむイートインスペース","takeaway":"このクラスタには、さいとう三陸菓匠が提供する焼きたてのパンやお菓子を、イートインスペースで楽しむことに関する情報が集まっています。利用者は、焼きたての美味しさをその場で味わえる体験を重視しており、食事やおやつの時間を特別なものにするための場所としての魅力を感じています。","value":1,"parent":"1_5","density_rank_percentile":0.02},{"level":2,"id":"2_31","label":"休憩スペースの利用状況とサービスの評価","takeaway":"このクラスタには、休憩スペースの存在や提供される飲み物サービスに関する意見が集まっています。近所の方々が談話室のように利用している一方で、実際に利用したことがないという声もあり、利用状況に対する関心が伺えます。また、イートインスペースでの無料のコーヒーやお茶、スタッフの優しい対応に対する評価が高く、サービスの質に満足している様子が見受けられます。","value":2,"parent":"1_5","density_rank_percentile":0.32},{"level":2,"id":"2_26","label":"飲食スペースの利用に関する体験と期待","takeaway":"このクラスタには、飲食スペースの利用に関する体験が集まっています。紙コップが備え付けられなくなったことで、利用者は少し不便を感じているものの、イートインスペースの利用を再度希望する意向が示されています。これは、飲食体験の向上や利便性の改善に対する期待があることを示しており、今後のサービス向上に対する前向きな姿勢が伺えます。","value":2,"parent":"1_5","density_rank_percentile":0.66},{"level":2,"id":"2_23","label":"パン工房COCOAの試食体験","takeaway":"このクラスタには、パン工房COCOAでの試食に関する体験や感想が集まっています。参加者は、さまざまな種類のパンを試食し、その味や食感、香りについての意見を共有しています。試食を通じて、パンの品質や工房の特徴を感じ取り、今後の購入意欲やリピートの可能性を高めるような前向きなフィードバックが見受けられます。","value":1,"parent":"1_3","density_rank_percentile":0.04},{"level":2,"id":"2_48","label":"美味しいパンの魅力","takeaway":"このクラスタには、さまざまな種類のパンに対する美味しさの感想が集まっています。特に幻のクリームパンやモンブランのように、特定のパンの特徴や味わいについての具体的な評価が含まれており、食べる楽しさや満足感が強調されています。利用者は、パンの滑らかなクリームや甘さを楽しみながら、パンの多様性とその魅力を再認識しています。","value":3,"parent":"1_3","density_rank_percentile":0.28},{"level":2,"id":"2_3","label":"パンの美味しさと多様性","takeaway":"このクラスタには、さまざまな種類のパンに対するポジティブな感想が集まっています。ふかふかで美味しいパンや、ヘルシーなサラダパン、そして塩パンの美味しさが強調されており、利用者はそれぞれのパンの特性や味わいを楽しんでいることが伺えます。店員の推薦に対する信頼感も表れており、パンの多様性とその魅力が評価されています。","value":3,"parent":"1_3","density_rank_percentile":0.38},{"level":2,"id":"2_44","label":"接客に関する評価と期待","takeaway":"このクラスタには、店員やスタッフの接客態度に関する意見が集まっています。一部の意見では好感を持てる接客が評価されている一方で、他の意見では態度が良くないと感じられています。全体として、顧客は親切で丁寧な接客を期待しており、接客の質が店舗の印象に大きく影響することが示唆されています。","value":3,"parent":"1_1","density_rank_percentile":0.48},{"level":2,"id":"2_7","label":"美味しい食体験の共有","takeaway":"このクラスタには、さまざまな食べ物に対する感想や体験が集まっています。明太子ポテトピザや焼きみたらし団子、揚げたゴマ団子など、具体的な料理名が挙げられ、それぞれの味や特徴についての意見が述べられています。食に対するポジティブな感情や、特定の料理に対する評価が共通しており、食文化や新しい味の発見に対する興味が感じられます。","value":3,"parent":"1_3","density_rank_percentile":0.54},{"level":2,"id":"2_25","label":"来客用お茶菓子としての多様な選択肢","takeaway":"このクラスタには、さいとう製菓の分店が提供する多様なお菓子やスイーツが集まっています。特に「かもめの玉子」やケーキ、和菓子、パンなど、来客用のお茶菓子として重宝される商品が強調されています。これにより、訪問者に対して多様な選択肢を提供し、特別な場面での楽しみを増やすことが期待されています。","value":2,"parent":"1_2","density_rank_percentile":0.5},{"level":2,"id":"2_20","label":"オシャレな喫茶スペースでのパンとコーヒーの楽しみ","takeaway":"このクラスタには、店内のオシャレな喫茶スペースでパンを楽しむことができるという情報が集まっています。購入したパンを食べながら、隣接するケーキ＆パンコーナーでの選択肢を楽しむことができる点が強調されており、特にパンコーナーのお勧めが示されています。利用者は、リラックスした雰囲気の中で美味しいパンとコーヒーを楽しむことができる体験を期待しています。","value":3,"parent":"1_5","density_rank_percentile":0.64},{"level":2,"id":"2_42","label":"美味しいパンとお得な購入体験","takeaway":"このクラスタには、パンの美味しさや食べやすさ、そしてLINE登録による割引といったお得な情報が集まっています。利用者は、フカフカで美味しいパンを求めてお店に足を運び、割引を利用することでよりお得にパンを楽しむことができるという体験を共有しています。","value":3,"parent":"1_3","density_rank_percentile":0.98},{"level":2,"id":"2_45","label":"ふわふわな焼き菓子への愛情","takeaway":"このクラスタには、ケーキやパンといった焼き菓子に対する好意的な意見が集まっています。特に、ちぎりパンのような新しいスタイルのパンに対する興味や美味しさの評価が見られ、ふわふわとした食感や味わいが強調されています。利用者は、焼き菓子の魅力を楽しみながら、特にチョコとクリームの組み合わせに惹かれている様子が伺えます。","value":2,"parent":"1_3","density_rank_percentile":0.3},{"level":2,"id":"2_32","label":"顧客サービスの質に対するポジティブな評価","takeaway":"このクラスタには、店員の笑顔や丁寧な接客に対する感謝の意が集まっています。顧客は、親切で丁寧なサービスを受けることで、再訪したいという気持ちを抱いており、店舗のサービス品質が顧客の満足度やリピート意向に大きく影響していることが示されています。","value":2,"parent":"1_1","density_rank_percentile":0.56},{"level":2,"id":"2_12","label":"お菓子に関する嬉しい体験と期待","takeaway":"このクラスタには、オマケのお菓子をもらったことへの喜びや、お遣い物としての適性、お菓子を次回購入したいという期待が含まれています。利用者は、特別な体験を通じてお菓子の魅力を再認識し、今後の選択肢としてお菓子を考えるようになっています。","value":3,"parent":"1_4","density_rank_percentile":0.74},{"level":2,"id":"2_4","label":"お昼時の軽食体験","takeaway":"このクラスタには、昼食時における食事の選択や体験に関する意見が集まっています。特に、パンの種類が限られている中での特定の選択（きな粉揚げパン）や、イートインコーナーでの楽しみ方に焦点を当てています。利用者は、限られた選択肢の中でも満足感を得ることができる食事体験を重視しており、食事の時間を楽しむことに価値を見出しています。","value":1,"parent":"1_5","density_rank_percentile":0.06},{"level":2,"id":"2_5","label":"コーヒーを中心としたリラックスタイムの楽しみ","takeaway":"このクラスタには、コーヒーを中心にした飲み物の楽しみ方や、リラックスした時間を過ごすための環境に関する意見が集まっています。利用者は、お菓子やパンを楽しみながらコーヒーを飲むことで、心地よい小休止を求めており、特にコーヒーのセルフサービスや待機中のコーヒーサーバーの存在が、より快適な体験を提供することに寄与しています。","value":4,"parent":"1_5","density_rank_percentile":0.76},{"level":2,"id":"2_8","label":"さいとう製菓の楽しみ方と店舗の雰囲気","takeaway":"このクラスタには、さいとう製菓のお菓子を楽しむ際の本来の楽しみ方や、店舗の雰囲気に関する意見が集まっています。お菓子を味わいながら飲むことが推奨されている一方で、店舗の居心地の良さや雰囲気の良さから、長時間滞在することも楽しめるという意見が見受けられます。これにより、訪れる人々はお菓子を楽しむだけでなく、リラックスした時間を過ごすことができる場所としての魅力を感じています。","value":1,"parent":"1_4","density_rank_percentile":0.08},{"level":2,"id":"2_37","label":"多様なお土産菓子の選択肢と詰め替えの楽しさ","takeaway":"このクラスタには、お土産用のお菓子の多様性と詰め替え可能な楽しさに関する意見が集まっています。さまざまなお菓子が揃っていることで、選ぶ楽しみや迷う楽しみが生まれ、贈り物としての魅力が増しています。利用者は、個々の好みに合わせたカスタマイズができることを重視し、特別な体験を提供するお土産菓子の価値を感じています。","value":2,"parent":"1_4","density_rank_percentile":0.44},{"level":2,"id":"2_27","label":"パンとケーキへの愛情","takeaway":"このクラスタには、揚げパンや手作りのパン、好きなパンやケーキに対する好意的な意見が集まっています。利用者は、特に揚げパンの魅力を感じており、手作りのパンの美味しさにも感謝しています。これらの意見は、パンやケーキに対する深い愛情と、食文化への関心を示しています。","value":3,"parent":"1_3","density_rank_percentile":0.4},{"level":2,"id":"2_21","label":"美容室帰りに便利な至れり尽くせりのお店","takeaway":"このクラスタには、美容室の近くに位置するお店に対する便利さや、至れり尽くせりのサービスに対する満足感が集まっています。利用者は、美容室での施術後に手軽に立ち寄れる立地を評価し、利便性とサービスの質を重視しています。","value":2,"parent":"1_1","density_rank_percentile":0.72},{"level":2,"id":"2_0","label":"多様なスイーツとパンの提供","takeaway":"このクラスタには、和菓子だけでなく、パンやケーキなどの多様なスイーツが取り揃えられていることに関する意見が集まっています。利用者は、特に穴場的な存在としての店舗の魅力を感じており、さまざまな商品を試すことができる楽しさを表現しています。また、カレーパンの味についての具体的な感想も含まれており、商品の品質やバリエーションに対する関心が伺えます。","value":3,"parent":"1_4","density_rank_percentile":0.96},{"level":2,"id":"2_39","label":"お菓子の価格に関する消費者の期待","takeaway":"このクラスタには、お菓子の価格に対する消費者の意見が集まっています。特に、現在の価格が高めであることに対する不満や、より求めやすい価格設定を望む声が多く見られます。消費者は、手頃な価格でお菓子を楽しむことができるようになることを期待しており、価格の見直しが求められています。","value":1,"parent":"1_4","density_rank_percentile":0.1},{"level":2,"id":"2_10","label":"駐車スペースの不足と狭さに関する懸念","takeaway":"このクラスタには、駐車場のスペースが不足していることや、駐車場が狭いという具体的な不満が集まっています。利用者は、駐車のしやすさや利便性に対する懸念を表明しており、駐車場の改善が求められています。これにより、より快適な駐車体験を実現するための対策が必要であることが示唆されています。","value":2,"parent":"1_5","density_rank_percentile":0.36},{"level":2,"id":"2_19","label":"贈り物や差し入れに最適な洋菓子店の魅力","takeaway":"このクラスタには、贈り物や差し入れに適した洋菓子店に関する意見が集まっています。特に、かもめのたまごをはじめとする多様な洋菓子が取り揃えられており、利用者はその豊富な品揃えを重宝していることが伺えます。また、隠れた好みとして「光の朝派」を挙げることで、個々の嗜好や特別な選択肢に対する愛着も表現されています。贈り物としての価値や、ちょっとした差し入れに最適な点が強調されています。","value":3,"parent":"1_2","density_rank_percentile":0.6},{"level":2,"id":"2_15","label":"美味しいパンとケーキの魅力","takeaway":"このクラスタには、パンやケーキの美味しさに関する意見が集まっています。特にスフレチーズケーキの具体的な体験が共有されており、全体として多様なお菓子やパンの魅力を称賛する声が見られます。利用者は、これらのスイーツやベーカリー製品が持つ美味しさを楽しんでおり、食文化の豊かさを感じています。","value":4,"parent":"1_3","density_rank_percentile":0.82},{"level":2,"id":"2_9","label":"かもめの玉子の新しいフレーバーへの期待と評価","takeaway":"このクラスタには、かもめの玉子の新しいフレーバー、特に杏仁豆腐味や限定のいちご味に対する高い評価と期待が集まっています。利用者は、これらの新しい味が期待を上回る美味しさであることを喜び、今後のフレーバー展開に対する関心を示しています。特に、限定品に対する期待感が強く、商品開発に対するポジティブな反応が見られます。","value":2,"parent":"1_2","density_rank_percentile":0.58},{"level":2,"id":"2_47","label":"魅力的な商品と良好なサービスを提供する店舗体験","takeaway":"このクラスタには、店舗でのポジティブな体験に関する意見が集まっています。特に、かもめの玉子やバラ売りの魅力、ケーキやパンの販売、そして店員の良い対応が強調されています。利用者は、これらの要素が相まって再訪したいという気持ちを抱いており、店舗の魅力を高く評価しています。","value":1,"parent":"1_1","density_rank_percentile":0.12},{"level":2,"id":"2_38","label":"チーズスフレケーキの人気と供給の限界","takeaway":"このクラスタには、チーズスフレケーキの美味しさとその供給の少なさに関する意見が集まっています。特に、数量が限られているため、早い者勝ちであることが強調されており、消費者の間での人気の高さが伺えます。11時には売り切れてしまうという情報は、商品の希少性を示し、より多くの人々がこのスイーツを求める理由を反映しています。","value":1,"parent":"1_4","density_rank_percentile":0.14},{"level":2,"id":"2_34","label":"可愛さとビターな味わいの融合","takeaway":"このクラスタには、肉球パンという可愛らしい形状と、ビターなチョコレートクリームという味わいの対比が特徴的な食べ物に関する意見が集まっています。可愛さと味のギャップが楽しめるこのパンは、見た目の楽しさと共に、味覚的な満足感を提供することが期待されており、食べる人々に新しい体験をもたらすことを目指しています。","value":1,"parent":"1_3","density_rank_percentile":0.16},{"level":2,"id":"2_30","label":"岩手のお菓子と帰省のお土産","takeaway":"このクラスタには、岩手県のお菓子に関連するお土産の購入や発送に関する意見が集まっています。特に、帰省中に地元の特産品をまとめて購入し、郵送することに焦点を当てています。具体的には、鴎のたまごという特定のお菓子が発送されたことが言及されており、地元の味を大切にする気持ちや、帰省の際にお土産を選ぶ楽しさが表れています。","value":3,"parent":"1_2","density_rank_percentile":0.42},{"level":2,"id":"2_13","label":"季節感を楽しむカモメの卵シリーズ","takeaway":"このクラスタには、イチゴ、ミカン、林檎、チョコといった季節感のある素材を使用した『カモメの卵』に関する意見が集まっています。これらの素材は、季節ごとの味わいを楽しむことができ、特にカモメの卵シリーズが多彩であることが強調されています。利用者は、これらの鉄板商品が持つ魅力や、季節ごとの新しい味の提案に期待を寄せています。","value":2,"parent":"1_2","density_rank_percentile":0.68},{"level":2,"id":"2_2","label":"店舗の評価と贈答品の取り扱い","takeaway":"このクラスタには、店舗のサービスや雰囲気に関する意見が集まっています。一方では、最低な店として評価される店舗があり、特にお使い物の購入にはおすすめできないという否定的な意見が存在します。対照的に、イートインがあり落ち着いた良いお店では、贈答品に対して親切に対応してくれるという肯定的な意見があり、顧客のニーズに応える姿勢が評価されています。これにより、店舗の質やサービスの違いが明確に示されています。","value":2,"parent":"1_1","density_rank_percentile":0.94},{"level":2,"id":"2_43","label":"贈答用お菓子と美味しいパン・ケーキの体験","takeaway":"このクラスタには、贈答用のお菓子の購入が便利であることや、パンやケーキの美味しさに関するポジティブな意見が集まっています。利用者は、贈り物としての選択肢が豊富で、味にも満足していることを示しており、贈答用のスイーツや焼き菓子が特別な場面での選択肢として好まれていることが伺えます。","value":1,"parent":"1_4","density_rank_percentile":0.18},{"level":2,"id":"2_28","label":"多様性と選択の楽しさ","takeaway":"このクラスタには、豊富なバリエーションや多様な種類が存在することに対するポジティブな意見が集まっています。利用者は、選択肢が多いことで自分の好みに合ったものを見つける楽しさを感じており、選ぶこと自体が楽しみであると考えています。多様性は、個々のニーズや嗜好に応じた選択を可能にし、より満足度の高い体験を提供する要素として重要視されています。","value":2,"parent":"1_4","density_rank_percentile":0.52},{"level":2,"id":"2_1","label":"岩手の銘菓「かもめの玉子」の魅力とお土産としての適性","takeaway":"このクラスタには、岩手の名物である「かもめの玉子」が持つ多様な種類やその魅力が集約されています。お土産としての選びやすさや楽しさが強調されており、岩手を訪れた際にぜひ手に入れたいアイテムとして位置づけられています。また、購入場所の情報も含まれており、観光客にとって便利な情報が提供されています。","value":2,"parent":"1_2","density_rank_percentile":0.34},{"level":2,"id":"2_49","label":"和菓子とパンの販売","takeaway":"このグループには、和菓子とパンという異なる種類の食品が共に販売されていることに関連する情報が集まっています。和菓子は日本の伝統的なスイーツであり、パンは西洋の主食として広く親しまれていますが、両者が同じ場所で販売されることで、様々な食文化の融合や多様な顧客ニーズに応える姿勢が示されています。これにより、消費者は和菓子とパンの両方を楽しむことができる選択肢を持ち、食の楽しみが広がります。","value":1,"parent":"1_4","density_rank_percentile":0.2},{"level":2,"id":"2_11","label":"顧客満足度を高める店舗の特徴","takeaway":"このクラスタには、店舗のサービス品質や商品ラインナップに関するポジティブな意見が集まっています。店員の対応が良いことは、顧客に対する信頼感や満足感を高め、リピート訪問を促進します。また、商品が豊富であることは、顧客の多様なニーズに応えるための重要な要素であり、選択肢の多さが購買意欲を引き出す要因となっています。これらの要素が組み合わさることで、顧客体験が向上し、店舗の評価が高まることが期待されます。","value":1,"parent":"1_1","density_rank_percentile":0.22},{"level":2,"id":"2_46","label":"限定商品の即時購入の重要性","takeaway":"このクラスタには、限定商品に対する消費者の強い関心と、入手のタイミングに関する意見が集まっています。限定商品は数量が限られているため、出た瞬間に購入しなければ手に入らない可能性が高いという認識が共有されています。消費者は、特別な商品を逃さないために迅速な行動を促されており、限定商品の価値や希少性がその購買意欲を高めています。","value":1,"parent":"1_4","density_rank_percentile":0.24}],"comments":{},"propertyMap":{},"translations":{},"overview":"この調査では、岩手の和菓子文化やスイーツ、パンに関する多様な意見が集約されました。特に「かもめの玉子」のような地域特産品や、スイーツの選択肢の豊富さ、イートイン体験の重要性が強調されています。また、顧客体験やサービス品質の向上がリピート訪問に寄与することが示され、店舗の改善が求められています。全体として、食文化の楽しさと顧客満足度の向上が重要なテーマとなっています。","config":{"name":"98cacd83-fca7-4be1-9186-25ad4a9b584f","input":"98cacd83-fca7-4be1-9186-25ad4a9b584f","question":"さいとう三陸菓匠国分通店のGoogle Mapsレビュー","intro":"このAI生成レポートは、Google Mapsのレビューに依拠している\n分析対象となったデータの件数は60件で、これらのデータに対してOpenAI APIを用いて115件の意見（議論）を抽出し、クラスタリングを行った。\n","model":"gpt-4o-mini","is_pubcom":true,"extraction":{"prompt":"/system\nあなたは専門的なリサーチアシスタントで、整理された議論のデータセットを作成するお手伝いをする役割です。\n人工知能に関する公開協議を実施した状況を想定しています。一般市民から寄せられた議論の例を提示しますので、それらをより簡潔で読みやすい形に整理するお手伝いをお願いします。必要な場合は2つの別個の議論に分割することもできますが、多くの場合は1つの議論にまとめる方が望ましいでしょう。\n結果は整形されたJSON形式の文字列リストとして返してください。\n要約は必ず日本語で作成してください。\n\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n[\n\"AIテクノロジーの環境負荷削減に焦点を当てるべき\"\n]\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、一般市民を教育する協調的な取り組みが必要です。\n\n/ai\n\n[\n\"AIの能力について一般市民を教育すべき\",\n\"AIの限界と倫理的考慮事項について一般市民を教育すべき\"\n]\n\n/human\n\nAIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できます。\n\n/ai\n\n[\n\"AIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できる\"\n]\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n[\n\"AIはエネルギーグリッドを最適化して無駄と炭素排出を削減できる\"\n]\n","workers":30,"limit":60,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$17","model":"gpt-4o-mini"},"hierarchical_clustering":{"cluster_nums":[5,50],"source_code":"$18"},"hierarchical_initial_labelling":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説して、それから表札をつけてください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n# サンプルの入出力\n## 入力例\n最近、AIアシスタントが医療診断サポートの実用化に向けた具体的な成果を発表し、医療現場の未来に明るい希望が見えてきました。患者として、この技術進歩に感謝しています。\nAI医療支援システムが、学会でもしっかり議論されるようになり、開発者が医療従事者の本当のニーズに応える姿勢に期待しています。\n技術カンファレンスを通じて、AIエンジニアが医療分野に全力で取り組む姿勢が伝わってきます。具体的な応用事例を目にするたび、医療の未来への希望が膨らみます。\n\n## 出力例\n{{\n   \"label\": \"医療現場を支えるAI技術の具体的応用への期待\",\n   \"description\": \"このクラスタには、医療診断や患者支援など、実際の医療課題に対してAI技術が具体的かつ効果的に貢献する可能性を支持する前向きな意見が集まっています。利用者は、学会や技術カンファレンスを通じて、現場のニーズに即したAIソリューションや支援システムが実現されることを期待し、医療の未来の発展に向けた技術革新を応援しています。\"\n}}\n","sampling_num":30,"workers":30,"source_code":"$19","model":"gpt-4o-mini"},"hierarchical_merge_labelling":{"prompt":"分割されすぎたクラスタを統合する必要があるので、統合後の名称を考えて出力して。\n\n# 指示\n* 統合前のクラスタの名称・説明および統合後のクラスタに属するデータ点のサンプルを与えるので、これらに基づいて統合後のクラスタの名称を出力してください\n   * 統合後のクラスタ名において、統合前のクラスタ名をそのまま使うことは避けてください。\n* 出力例に記載したJSONのフォーマットに従って出力してください\n\n# サンプルの入出力\n## 入力例（クラスタラベル:説明文）\n- AI技術の環境影響に関する懸念: このクラスタは、AI技術の開発・運用に伴う環境負荷に対する批判的な意見を集約したものです。市民からは、大規模モデルの学習時のエネルギー消費や、データセンターの冷却システムによる環境負荷などに対する強い懸念が表明されています。\n- AI開発の持続可能性への疑問: このクラスタは、AI技術開発全般における持続可能性に対する疑問を示す意見をまとめたものです。エネルギー消費の増大や資源利用の効率性に疑問を持つ声が多く、より環境に配慮した技術開発アプローチを求める意見が特徴です。\n- AI運用の炭素排出量: このクラスタは、AIシステムの運用時における炭素排出量が予想以上に大きい点に対する懸念や不満を反映しています。クラウドサービスの利用拡大やモデル推論時のエネルギー消費、そしてそれに伴う環境への長期的影響が強調されています。\n## 出力例\n{{\n   \"label\": \"AI技術のエネルギー効率と環境持続可能性\",\n   \"description\": \"このクラスタは、AI技術の開発・運用における環境への影響と持続可能性に対する懸念を集約しています。市民は、技術革新の議論の中で、エネルギー効率と炭素排出削減を最優先すべきだとの期待と、現行のAI開発手法に対する環境面での改善要求を強く表明しており、より持続可能で環境に配慮したAI技術の発展を求める声が反映されています。\"\n}}\n","sampling_num":30,"workers":30,"source_code":"$1a","model":"gpt-4o-mini"},"hierarchical_overview":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$1b","model":"gpt-4o-mini"},"hierarchical_aggregation":{"sampling_num":30,"hidden_properties":{},"source_code":"$1c"},"output_dir":"98cacd83-fca7-4be1-9186-25ad4a9b584f","skip-interaction":true,"without-html":true,"embedding":{"model":"text-embedding-3-small","source_code":"import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)\n"},"hierarchical_visualization":{"replacements":[],"source_code":"import subprocess\n\n\ndef hierarchical_visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../report\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"not trace of previous run"},{"step":"embedding","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_clustering","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_initial_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_merge_labelling","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_overview","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_aggregation","run":true,"reason":"not trace of previous run"},{"step":"hierarchical_visualization","run":false,"reason":"skipping html output"}],"status":"running","start_time":"2025-04-17T08:15:23.251182","completed_jobs":[{"step":"extraction","completed":"2025-04-17T08:15:31.650355","duration":8.39869,"params":{"prompt":"/system\nあなたは専門的なリサーチアシスタントで、整理された議論のデータセットを作成するお手伝いをする役割です。\n人工知能に関する公開協議を実施した状況を想定しています。一般市民から寄せられた議論の例を提示しますので、それらをより簡潔で読みやすい形に整理するお手伝いをお願いします。必要な場合は2つの別個の議論に分割することもできますが、多くの場合は1つの議論にまとめる方が望ましいでしょう。\n結果は整形されたJSON形式の文字列リストとして返してください。\n要約は必ず日本語で作成してください。\n\n/human\n\nAIテクノロジーは、そのライフサイクル全体における環境負荷を削減することに焦点を当てて開発されるべきです。\n\n/ai\n\n[\n\"AIテクノロジーの環境負荷削減に焦点を当てるべき\"\n]\n\n/human\n\nAIの能力、限界、倫理的考慮事項について、一般市民を教育する協調的な取り組みが必要です。\n\n/ai\n\n[\n\"AIの能力について一般市民を教育すべき\",\n\"AIの限界と倫理的考慮事項について一般市民を教育すべき\"\n]\n\n/human\n\nAIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できます。\n\n/ai\n\n[\n\"AIはスマートホームやビルのエネルギー効率と居住者の快適性を最適化できる\"\n]\n\n/human\n\nAIはエネルギーグリッドを最適化し、無駄や炭素排出を削減できます。\n\n/ai\n\n[\n\"AIはエネルギーグリッドを最適化して無駄と炭素排出を削減できる\"\n]\n","workers":30,"limit":60,"properties":[],"categories":{},"category_batch_size":5,"source_code":"$1d","model":"gpt-4o-mini"}},{"step":"embedding","completed":"2025-04-17T08:15:36.605126","duration":4.953457,"params":{"model":"text-embedding-3-small","source_code":"import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)\n"}},{"step":"hierarchical_clustering","completed":"2025-04-17T08:15:44.211990","duration":7.605738,"params":{"cluster_nums":[5,50],"source_code":"$1e"}},{"step":"hierarchical_initial_labelling","completed":"2025-04-17T08:15:51.605917","duration":7.393233,"params":{"prompt":"あなたはKJ法が得意なデータ分析者です。userのinputはグループに集まったラベルです。なぜそのラベルが一つのグループであるか解説して、それから表札をつけてください。\n出力はJSONとし、フォーマットは以下のサンプルを参考にしてください。\n\n# サンプルの入出力\n## 入力例\n最近、AIアシスタントが医療診断サポートの実用化に向けた具体的な成果を発表し、医療現場の未来に明るい希望が見えてきました。患者として、この技術進歩に感謝しています。\nAI医療支援システムが、学会でもしっかり議論されるようになり、開発者が医療従事者の本当のニーズに応える姿勢に期待しています。\n技術カンファレンスを通じて、AIエンジニアが医療分野に全力で取り組む姿勢が伝わってきます。具体的な応用事例を目にするたび、医療の未来への希望が膨らみます。\n\n## 出力例\n{{\n   \"label\": \"医療現場を支えるAI技術の具体的応用への期待\",\n   \"description\": \"このクラスタには、医療診断や患者支援など、実際の医療課題に対してAI技術が具体的かつ効果的に貢献する可能性を支持する前向きな意見が集まっています。利用者は、学会や技術カンファレンスを通じて、現場のニーズに即したAIソリューションや支援システムが実現されることを期待し、医療の未来の発展に向けた技術革新を応援しています。\"\n}}\n","sampling_num":30,"workers":30,"source_code":"$1f","model":"gpt-4o-mini"}},{"step":"hierarchical_merge_labelling","completed":"2025-04-17T08:15:58.594227","duration":6.987099,"params":{"prompt":"分割されすぎたクラスタを統合する必要があるので、統合後の名称を考えて出力して。\n\n# 指示\n* 統合前のクラスタの名称・説明および統合後のクラスタに属するデータ点のサンプルを与えるので、これらに基づいて統合後のクラスタの名称を出力してください\n   * 統合後のクラスタ名において、統合前のクラスタ名をそのまま使うことは避けてください。\n* 出力例に記載したJSONのフォーマットに従って出力してください\n\n# サンプルの入出力\n## 入力例（クラスタラベル:説明文）\n- AI技術の環境影響に関する懸念: このクラスタは、AI技術の開発・運用に伴う環境負荷に対する批判的な意見を集約したものです。市民からは、大規模モデルの学習時のエネルギー消費や、データセンターの冷却システムによる環境負荷などに対する強い懸念が表明されています。\n- AI開発の持続可能性への疑問: このクラスタは、AI技術開発全般における持続可能性に対する疑問を示す意見をまとめたものです。エネルギー消費の増大や資源利用の効率性に疑問を持つ声が多く、より環境に配慮した技術開発アプローチを求める意見が特徴です。\n- AI運用の炭素排出量: このクラスタは、AIシステムの運用時における炭素排出量が予想以上に大きい点に対する懸念や不満を反映しています。クラウドサービスの利用拡大やモデル推論時のエネルギー消費、そしてそれに伴う環境への長期的影響が強調されています。\n## 出力例\n{{\n   \"label\": \"AI技術のエネルギー効率と環境持続可能性\",\n   \"description\": \"このクラスタは、AI技術の開発・運用における環境への影響と持続可能性に対する懸念を集約しています。市民は、技術革新の議論の中で、エネルギー効率と炭素排出削減を最優先すべきだとの期待と、現行のAI開発手法に対する環境面での改善要求を強く表明しており、より持続可能で環境に配慮したAI技術の発展を求める声が反映されています。\"\n}}\n","sampling_num":30,"workers":30,"source_code":"$20","model":"gpt-4o-mini"}},{"step":"hierarchical_overview","completed":"2025-04-17T08:16:01.248693","duration":2.652541,"params":{"prompt":"/system \n\nあなたはシンクタンクで働く専門のリサーチアシスタントです。\nチームは特定のテーマに関してパブリック・コンサルテーションを実施し、異なる選択肢のクラスターを分析し始めています。\nこれからクラスターのリストとその簡単な分析が提供されます。\nあなたの仕事は、調査結果の簡潔な要約を返すことです。要約は非常に簡潔に（最大で1段落、最大4文）まとめ、無意味な言葉を避けてください。\n出力は日本語で行ってください。\n","source_code":"$21","model":"gpt-4o-mini"}}],"lock_until":"2025-04-17T08:21:01.249782","current_job":"hierarchical_aggregation","current_job_started":"2025-04-17T08:16:01.249777","current_job_progress":null,"current_jop_tasks":null},"comment_num":60}}],["$","$L22",null,{"result":"$8:0:props:children:2:props:result"}],["$","$L12",null,{"w":"fit-content","mx":"auto","children":["$","$L6",null,{"href":"/","children":["$","$L7",null,{"variant":"outline","size":"md","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-left","children":[["$","path","1wnfg3",{"d":"m15 18-6-6 6-6"}],"$undefined"]}],"一覧へ戻る"]}]}]}],["$","$L23",null,{"my":12,"maxW":"750px","mx":"auto"}],["$","$L24",null,{"meta":"$8:0:props:children:0:props:meta"}]]}],["$","footer",null,{"children":["$","$L25",null,{"direction":{"base":"column","lg":"row"},"justify":"space-between","maxW":"800px","mx":"auto","children":[["$","$L26",null,{"gap":5,"justify":"center","align":"center","children":[["$","$L14",null,{"fontWeight":"bold","fontSize":"lg","children":"テスト環境"}],["$","$L6",null,{"href":"https://example.com/privacy","target":"_blank","rel":"noopener noreferrer","children":["$","$L14",null,{"fontSize":"xs","className":"textLink","children":"プライバシーポリシー"}]}],["$","$L6",null,{"href":"https://example.com/terms","target":"_blank","rel":"noopener noreferrer","children":["$","$L14",null,{"fontSize":"xs","className":"textLink","children":"利用規約"}]}]]}],["$","$L26",null,{"justify":"center","children":["$","$L27",null,{"placement":"bottom","children":[["$","$L28",null,{}],["$","$L29",null,{"children":["$","$L14",null,{"className":"textLink","cursor":"pointer","children":"デジタル民主主義2030プロジェクトについて"}]}],["$","$L2a",null,{"disabled":false,"container":"$undefined","children":["$","$L2b",null,{"padding":"$undefined","children":["$","$L2c",null,{"ref":"$undefined","roundedTop":"md","p":5,"asChild":false,"children":[["$","$L2d",null,{"children":["$","$L2e",null,{"fontSize":"2xl","fontWeight":"bold","textAlign":"center","className":"gradientColor","children":"デジタル民主主義2030プロジェクト"}]}],["$","$L2f",null,{"textAlign":"center","children":[["$","$L12",null,{"mb":8,"maxW":"700px","mx":"auto","children":[["$","$L13",null,{"size":"lg","mb":2,"textAlign":"center","children":"プロジェクトについて"}],["$","$L14",null,{"children":"2030年には、情報技術により民主主義のあり方はアップデートされており、一人ひとりの声が政治・行政に届き、適切に合意形成・政策反映されていくような社会が当たり前になる──そんな未来を目指して立ち上げられたのがデジタル民主主義2030プロジェクトです。 AIやデジタル技術の進化により、これまで不可能だった新しい形の市民参加や政策運営が可能になるはずだという信念に基づいています。"}],["$","$L6",null,{"href":"https://dd2030.org","target":"_blank","rel":"noreferrer noopener","children":["$","$L26",null,{"justify":"center","mt":2,"children":[["$","$L14",null,{"className":"textLink","children":"プロジェクトについての詳細はこちら"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-external-link","children":[["$","path","1q9fwt",{"d":"M15 3h6v6"}],["$","path","gplh6r",{"d":"M10 14 21 3"}],["$","path","a6xqqp",{"d":"M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"}],"$undefined"]}]]}]}]]}],["$","$L12",null,{"mb":8,"maxW":"700px","mx":"auto","children":[["$","$L13",null,{"size":"lg","mb":2,"textAlign":"center","children":"免責"}],["$","$L14",null,{"mb":2,"children":"このレポート内容に関する質問や意見はレポート発行責任者へお問い合わせください。"}],["$","$L14",null,{"children":"大規模言語モデル（LLM）にはバイアスがあり、信頼性の低い結果を生成することが知られています。私たちはこれらの問題を軽減する方法に積極的に取り組んでいますが、現段階ではいかなる保証も提供することはできません。特に重要な決定を下す際は、本アプリの出力結果のみに依存せず、必ず内容を検証してください。"}]]}],["$","$L12",null,{"mb":8,"maxW":"700px","mx":"auto","children":[["$","$L13",null,{"size":"lg","mb":2,"textAlign":"center","children":"謝辞"}],["$","$L14",null,{"children":["このプロジェクトは"," ",["$","a",null,{"className":"textLink","href":"https://ai.objectives.institute/","target":"_blank","rel":"noreferrer","children":"AI Objectives Institute"}]," ","が開発した"," ",["$","a",null,{"className":"textLink","href":"https://github.com/AIObjectives/talk-to-the-city-reports","target":"_blank","rel":"noreferrer","children":"Talk to the City"}]," ","を参考に開発されています。",["$","br",null,{}],"ライセンスに基づいてソースコードを一部活用し、機能追加や改善を実施しています。"]}]]}],["$","$L30",null,{"children":["$","$L7",null,{"variant":"outline","children":"閉じる"}]}]]}]]}]}]}]]}]}]]}]}]]
b:null
f:[["$","title","0",{"children":"さいとう三陸菓匠国分通店のGoogle Mapsレビュー - テスト環境"}],["$","meta","1",{"name":"description","content":"この調査では、岩手の和菓子文化やスイーツ、パンに関する多様な意見が集約されました。特に「かもめの玉子」のような地域特産品や、スイーツの選択肢の豊富さ、イートイン体験の重要性が強調されています。また、顧客体験やサービス品質の向上がリピート訪問に寄与することが示され、店舗の改善が求められています。全体として、食文化の楽しさと顧客満足度の向上が重要なテーマとなっています。"}],["$","meta","2",{"property":"og:title","content":"さいとう三陸菓匠国分通店のGoogle Mapsレビュー - テスト環境"}],["$","meta","3",{"property":"og:description","content":"この調査では、岩手の和菓子文化やスイーツ、パンに関する多様な意見が集約されました。特に「かもめの玉子」のような地域特産品や、スイーツの選択肢の豊富さ、イートイン体験の重要性が強調されています。また、顧客体験やサービス品質の向上がリピート訪問に寄与することが示され、店舗の改善が求められています。全体として、食文化の楽しさと顧客満足度の向上が重要なテーマとなっています。"}],["$","meta","4",{"property":"og:image","content":"http://localhost:3000/98cacd83-fca7-4be1-9186-25ad4a9b584f/opengraph-image.png"}],["$","meta","5",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","6",{"name":"twitter:title","content":"さいとう三陸菓匠国分通店のGoogle Mapsレビュー - テスト環境"}],["$","meta","7",{"name":"twitter:description","content":"この調査では、岩手の和菓子文化やスイーツ、パンに関する多様な意見が集約されました。特に「かもめの玉子」のような地域特産品や、スイーツの選択肢の豊富さ、イートイン体験の重要性が強調されています。また、顧客体験やサービス品質の向上がリピート訪問に寄与することが示され、店舗の改善が求められています。全体として、食文化の楽しさと顧客満足度の向上が重要なテーマとなっています。"}],["$","meta","8",{"name":"twitter:image","content":"http://localhost:3000/98cacd83-fca7-4be1-9186-25ad4a9b584f/opengraph-image.png"}]]
